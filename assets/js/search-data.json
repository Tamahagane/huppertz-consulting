{
  
    
        "post0": {
            "title": "Evaluating a Machine Learning Algorithm with Custom Metrics and Picking Relevant Features",
            "content": "Our data is taken from here. It contains some features of customers of a telecommunications provider and - as the classification variable - the information if a customer churned in the next month after the features were recorded, i.e. if they cancelled their contract. . Our goal today is to discern which factors prevent churn in our customer base and which other factors encourage it. We will try to come up with a classification that gives us acceptable results and then try to find out which features are most responsible for the algorithms decision to classify a datapoint as churner or non-churner. . Furthermore we will define what constitutes an acceptable result by calculating our own success metric that is closely tied to our business case as opposed to relying solely on technical standard metrics like accuracy. . Preprocessing . The data preprocessing part is not very relevant to the evaluation of the algorithms applied to the data, but we will look at some tips and tricks to improve the preprocessing part to keep things interesting. . import warnings warnings.filterwarnings(&#39;ignore&#39;) import pandas as pd import numpy as np pd.set_option(&quot;display.precision&quot;, 2) . Let us read the data into a pandas dataframe and simply take a look at it first. . . Tip: If you try to reproduce this notebook, get your copy of the data into a &#8217;/data/&#8217; subdirectory in the folder where your notebook resides. . df = pd.read_csv(&#39;./data/WA_Fn-UseC_-Telco-Customer-Churn.csv&#39;, decimal=&#39;.&#39;) df.head().T # transposed for readability . 0 1 2 3 4 . customerID 7590-VHVEG | 5575-GNVDE | 3668-QPYBK | 7795-CFOCW | 9237-HQITU | . gender Female | Male | Male | Male | Female | . SeniorCitizen 0 | 0 | 0 | 0 | 0 | . Partner Yes | No | No | No | No | . Dependents No | No | No | No | No | . tenure 1 | 34 | 2 | 45 | 2 | . PhoneService No | Yes | Yes | No | Yes | . MultipleLines No phone service | No | No | No phone service | No | . InternetService DSL | DSL | DSL | DSL | Fiber optic | . OnlineSecurity No | Yes | Yes | Yes | No | . OnlineBackup Yes | No | Yes | No | No | . DeviceProtection No | Yes | No | Yes | No | . TechSupport No | No | No | Yes | No | . StreamingTV No | No | No | No | No | . StreamingMovies No | No | No | No | No | . Contract Month-to-month | One year | Month-to-month | One year | Month-to-month | . PaperlessBilling Yes | No | Yes | No | Yes | . PaymentMethod Electronic check | Mailed check | Mailed check | Bank transfer (automatic) | Electronic check | . MonthlyCharges 30 | 57 | 54 | 42 | 71 | . TotalCharges 29.85 | 1889.5 | 108.15 | 1840.75 | 151.65 | . Churn No | No | Yes | No | Yes | . We can see that the dataset consists largely of categorical variables such the subscription to a specific service of the telco&#39;s portfolio. There are some continuous features such as &#39;tenure&#39; and &#39;TotalCharges&#39;. Finally there is a column &#39;Churn&#39; that tells us if the customer has canceled his or her contract in the last month. Our goal is now to identify those features which are significant in a users decision to churn and to predict if a user will churn in the next month. . A note on realism . While there are plenty of features in the data and we will engineer a few more, there are a few issues that need be addressed: . Someone who has not churned might churn &#39;tomorrow&#39; or in fancy terms: the data is right-censored. That means, our customer might still be marked active although his decision to cancel might have been made already. | There are no external factors known here: the competitor&#39;s prices are unknown, if there are any locally present competitor&#39;s at all is unknown. | It is unknown if the customer had any technical or billing issues and if there are any ongoing or closed customer service issues. | The price level of the individual services is unknown as is their usage intensity by the customer. | It is unknown if any customer rentention measures, such a discounts, have been applied. | There is no information about the quality of service such a bandwith. | The dataset is static. In a real world situation, there would be a timeline associated with certain events like customer service issues and contract extensions. | . So, the dataset has very little in common with data that would realistically be available to a telco provider. It is realistic in the sense that it is not simulated data (as far as I can tell) and there is no clear, constructed relationship between features and the outcome. Therefore, right now, we don&#39;t know if there will be any interpretable result at all. . We want to focus on the performance and tuning possibilities of multiple machine learning algorithms though so these issues with the data are not our primary concern. . Logging . We will later define a workflow that uses the pandas pipe operator to concatenate various functions that will contain preprocessing steps. . But first and foremost, we are going to write a decorator function, which returns meta-information about every single preprocessing step. That in turn serves as a preemptive measure against introducing bugs to our code, when applied to the steps of the data processing pipeline. . It is going to print the number of rows and columns of the preprocessed dataframe, along with the number and sample content of rows containing &#39;nan&#39;s. . def logger(function): def wrap(df, *args, **kwargs): # do the actual preprocessing step result = function(df, *args, **kwargs) # print rows, columns and no. of rows with nans # dunder variable name is built-in name = function.__name__ print( f&#39;{name:20}: shape = {result.shape} nan rows = {(result.shape[0] - result.dropna().shape[0])} &#39; ) # print sample content of nan rows, if there are any nan_rows = result[result.isnull().any(axis=1)] if len(nan_rows) &gt; 0: print(nan_rows.head()) return result return wrap . Preprocessing Pipeline . With the logging decorator done we can turn towards the actual data processing. . In order to access features quicker later on, we list various subsets of columns. We group them by the topic and type of the columns so we can apply similar operations to each, or on the group as a whole. That will simply preprocessing. . # list all the feature category columns cat_cols = [ &#39;gender&#39;, &#39;Partner&#39;, &#39;Dependents&#39;, &#39;PhoneService&#39;, &#39;MultipleLines&#39;, &#39;InternetService&#39;, &#39;OnlineSecurity&#39;, &#39;OnlineBackup&#39;, &#39;DeviceProtection&#39;, &#39;TechSupport&#39;, &#39;StreamingTV&#39;, &#39;StreamingMovies&#39;, &#39;Contract&#39;, &#39;PaperlessBilling&#39;, &#39;PaymentMethod&#39;, &#39;Churn&#39; ] # list all service options service_cols = [ &#39;PhoneService&#39;, &#39;MultipleLines&#39;, &#39;InternetService&#39;, &#39;OnlineSecurity&#39;, &#39;OnlineBackup&#39;, &#39;DeviceProtection&#39;, &#39;TechSupport&#39;, &#39;StreamingTV&#39;, &#39;StreamingMovies&#39; ] # list the numeric columns float_cols = [&#39;tenure&#39;,&#39;MonthlyCharges&#39;, &#39;TotalCharges&#39;] . We are dropping the customer&#39;s ID as it carries no information. Also there is a handful of rows which has an empty &#39;TotalCharges&#39; field, so we are going to remove these as our algos require complete rows. Note that we could also impute these, but our dataset is big enough to spare a few rows. . We also perform a few data sanitization operations such as ensuring that all columns containing numerical values are not treated as strings. Also we encode and scale the data as to not give to much weight to the &#39;tenure&#39;, &#39;Monthly Charges&#39; and &#39;Total Charges&#39; columns, as they can be in the thousands while the services columns result in category values up to &quot;6&quot;. . from sklearn.preprocessing import StandardScaler @logger def copy_df(df): # deep copy of DataFrame as not to alter the original data return df.copy() @logger def drop_id(df): # this does not carry any information df = df.drop([&#39;customerID&#39;], axis=1) return df @logger def remove_whitespace(df): # some entries seem to be empty,but for some reason they # are not &quot;nan&quot; but just some spaces df = df[~(df[&#39;TotalCharges&#39;] == &#39; &#39;)] # strip out all leading and following # whitespaces if there are any for col in df.columns: if df.dtypes[col] == object: df[col] = df[col].str.strip() return df @logger def ensure_formats(df, float_cols=float_cols, cat_cols=cat_cols): for col in float_cols: df[col] = df[col].astype(float) for col in cat_cols: df[col] = df[col].astype(&#39;category&#39;) return df @logger def encode_and_scale(df, float_cols=float_cols, cat_cols=cat_cols): # create one hot encoded version of the categorical variables encoded_columns = pd.get_dummies(df[cat_cols], drop_first=True) # scale the float columns scaler = StandardScaler() scaled_columns = scaler.fit_transform(df[float_cols]) # rebuild the dataframe with the modified columns df = pd.DataFrame(np.concatenate( (np.array(scaled_columns), np.array(encoded_columns)), axis=1), columns=list(float_cols) + list(encoded_columns.columns)) return df . Then we execute a pipeline on the original DataFrame, applying one function after another. Our logging function shows that we merely lost 9 rows along they way which is only slightly more than 1% of the dataset. . df_work = (df .pipe(copy_df) .pipe(drop_id) .pipe(remove_whitespace) .pipe(ensure_formats) .pipe(encode_and_scale) ) . copy_df : shape = (7043, 21) nan rows = 0 drop_id : shape = (7043, 20) nan rows = 0 remove_whitespace : shape = (7032, 20) nan rows = 0 ensure_formats : shape = (7032, 20) nan rows = 0 encode_and_scale : shape = (7032, 30) nan rows = 0 . Our df_work that we will conduct the rest of this experiment on, while leaving the original data completely untouched, looks like this now: . df_work.head(3).T # transposed again for better readability . 0 1 2 . tenure -1.28 | 0.06 | -1.24 | . MonthlyCharges -1.16 | -0.26 | -0.36 | . TotalCharges -0.99 | -0.17 | -0.96 | . gender_Male 0.00 | 1.00 | 1.00 | . Partner_Yes 1.00 | 0.00 | 0.00 | . Dependents_Yes 0.00 | 0.00 | 0.00 | . PhoneService_Yes 0.00 | 1.00 | 1.00 | . MultipleLines_No phone service 1.00 | 0.00 | 0.00 | . MultipleLines_Yes 0.00 | 0.00 | 0.00 | . InternetService_Fiber optic 0.00 | 0.00 | 0.00 | . InternetService_No 0.00 | 0.00 | 0.00 | . OnlineSecurity_No internet service 0.00 | 0.00 | 0.00 | . OnlineSecurity_Yes 0.00 | 1.00 | 1.00 | . OnlineBackup_No internet service 0.00 | 0.00 | 0.00 | . OnlineBackup_Yes 1.00 | 0.00 | 1.00 | . DeviceProtection_No internet service 0.00 | 0.00 | 0.00 | . DeviceProtection_Yes 0.00 | 1.00 | 0.00 | . TechSupport_No internet service 0.00 | 0.00 | 0.00 | . TechSupport_Yes 0.00 | 0.00 | 0.00 | . StreamingTV_No internet service 0.00 | 0.00 | 0.00 | . StreamingTV_Yes 0.00 | 0.00 | 0.00 | . StreamingMovies_No internet service 0.00 | 0.00 | 0.00 | . StreamingMovies_Yes 0.00 | 0.00 | 0.00 | . Contract_One year 0.00 | 1.00 | 0.00 | . Contract_Two year 0.00 | 0.00 | 0.00 | . PaperlessBilling_Yes 1.00 | 0.00 | 1.00 | . PaymentMethod_Credit card (automatic) 0.00 | 0.00 | 0.00 | . PaymentMethod_Electronic check 1.00 | 0.00 | 0.00 | . PaymentMethod_Mailed check 0.00 | 1.00 | 1.00 | . Churn_Yes 0.00 | 0.00 | 1.00 | . As a next step, we are going to create training and hold-out datasets from our data. Although will use cross validation when training the classifiers, it is always better to have another group of data kept separate from the training process as an ultimate test. . The classes in the dataset are fairly unbalanced, therefore a stratification is necessary, i.e. not 33% of rows are chosen at random but it is guaranteed that 33% of rows of customers that churned and 33% of non-churners are chosen. In reality, you would expect an even greater disparity. . You will usually not lose 36% of your customer base in one month (hopefully). . yes_no_ratio=len(df[df[&#39;Churn&#39;]==&#39;Yes&#39;])/len(df[df[&#39;Churn&#39;]==&#39;No&#39;]) print(yes_no_ratio) . 0.36122922303826827 . from sklearn.model_selection import train_test_split y=df_work[&#39;Churn_Yes&#39;] X=df_work.drop([&#39;Churn_Yes&#39;], axis=1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y) . Note that the name of the column containing the info if the customer churned or not has changed from &quot;Churn&quot; to &quot;Churn_Yes&quot; as the result of the one-hot- encoding. We could of course change it back, but I prefer to keep that as a reminder of the encoding for potential debugging. . As a next step, I like to get a qualitative feel for the dataset. This step is entirely optional, but it gives us an idea how well classification algorithms might perform. . Getting a feel for the Topology of the Feature Set . The feature set is high dimensional and we cannot easily visualize, how well we can separate the churners from the non-churners. In order to approximate the data set, we can use a principal component analysis and a linear discriminant analysis. . We start with a PCA which tries to sum up the whole feature sets variance in a defined number of combined variables. It eliminates features that move &quot;in step&quot; with other features, therefore carrying no additional information. Thus, they can be omitted without losing too much information. . import matplotlib.pyplot as plt from sklearn.decomposition import PCA %matplotlib inline pca = PCA(n_components=2) X_r = pca.fit(X_train).transform(X_train) # Percentage of variance explained for each components print(&#39;explained variance ratio (first two components): %s&#39; % str(pca.explained_variance_ratio_)) plt.figure() colors = [&#39;navy&#39;, &#39;orange&#39;] labels = [&#39;non-churners&#39;, &#39;churners&#39;] for color, i, target_name in zip(colors, [0, 1], y): plt.scatter(X_r[y_train == i, 0], X_r[y_train == i, 1], color=color, alpha=.8, lw=1,facecolors=&quot;None&quot;, label=labels[i]) plt.title(&#39;PCA of dataset&#39;) plt.xlabel(&#39;PC 1&#39;) plt.ylabel(&#39;PC 2&#39;) plt.legend() plt.show() . explained variance ratio (first two components): [0.40020818 0.18929489] . We get two blobs which seem to be very distinct from one another, but both contain significant amount of churners and non-churners. There are some areas here, which very clearly belong to the (blue) non-churners but almost non of the areas occupied by (orange) churners are clear cut. There are always non-churners which have similar characteristics as the churners. After all, a churner this month was a non-churner just last month. . It would be interesting to know, what causes the very clear distinction between the two blobs. Looking at the composition of the Principal Components could problably clue us in here, but let us not get distracted. . Let&#39;s try the LDA next which tries to map the feature space onto a &quot;no. of classes-1&quot;-dimensional hyperplane and tries to achieve maximum linear separability. . from sklearn.discriminant_analysis import LinearDiscriminantAnalysis lda = LinearDiscriminantAnalysis(n_components=1) X_r2 = lda.fit(X_train, y_train).transform(X_train) plt.hist((X_r2[y_train == 0, 0],X_r2[y_train == 1, 0]), color=colors, density=True, label=labels) #plt.legend(loc=&#39;best&#39;, shadow=False, scatterpoints=1) plt.title(&#39;LDA of dataset&#39;) plt.xlabel(&#39;LDA Value&#39;) plt.ylabel(&#39;Density&#39;) plt.legend() plt.show() . Similar to a Naive Bayes classification, we can now try to find the optimal value along the mapped feature space to separate orange from blue, i.e. a cutoff value along the x-axis in the graph above. . Obvioulsy, picking any value along the x-axis results in a significant amount of misclassifications in either the churn or non-churn class. . Having a look at the topology of the data set, we can foresee that performance of any classifier might be limited, as churners seem to be very similar to some non-churners, though the reverse does not necessarily hold true. . Performance vs. Business Metrics . While the sklearn models focus on raw performance metrics, now - before we get any classification results which might influence our judgement - is the time to consider the business impact of any model, should it be put into production: . Misclassifying a non-churner as a churner . A false positive classfication means someone is labeled as a potential churner while they have no intention to actually cancel their contract. | This might trigger various processes like customer service getting in touch with the customer and discounts being offered. This incurs costs for personnel and opportunity costs for the discounts. | . | Misclassifying churners a non-churners . A false negative obviously incurs the opportunity cost of losing the customer without any chance to intervene. | There is always a chance that the customer churns despite intervention. A success rate of customer retention intervention must be factored into the calculations. | . | . Let&#39;s assume that a customer pays about 780 Dollars a year for their contract. This means that a false positive results in costs of a 20% discount and a neglegible cost for the personnel and infrastructure to perform the intervention, i.e. 155 Dollars. . A false negative on the other hand means a full 780 Dollars lost, so as long as our success ratio for customer retention is better than 1 in 5 (obviously the inverse of the discount given), it is preferable to have a false positive up to a certain ratio. . Obviously, all these values are assumptions and would need to be challenged by evaluating data from marketing, call center, customer retention experts, etc. . In terms of our exercise today this means we are specifically interested in minimizing the cost of misclassifications. In terms of the KPIs of the model, this is equivalent to maximizing the recall in the churner-class while keeping the precision in the non-churner-class above a certain threshold. . df[&#39;MonthlyCharges&#39;].mean()*12 . df[&#39;MonthlyCharges&#39;].mean()*12*0.2 . In order to measure the business impact directly, we will create a custom scorer using the make_scorer method. This can then be used as a cost function for the training process of the classification algorithms. . from sklearn.metrics import make_scorer def cost_metric(y_test, y_pred, retention_success=0.33, discount=0.2, yearly_base=777): cost_false_negative = retention_success * yearly_base cost_false_positive = discount * yearly_base misclass = np.subtract(y_test, y_pred) fp = np.count_nonzero(misclass &lt; 0) # false positives fn = np.count_nonzero(misclass &gt; 0) # false negatives score = fp * cost_false_positive + fn * cost_false_negative return score # Make scorer and define that higher scores are better # since we want to minimize our cost, greater is worse # therefore the scorer simply multiplies with -1 score = make_scorer(cost_metric, greater_is_better=False) . We could push that idea even further by creating buckets for the customers monthly charges and weighting them, so that mistakes in higher value customers are penalized stronger. . Now that we have a method to judge our algorithms performance and have clean and orderly data, we can move on to actually performing classifications. . Implementing the Actual Classification . Let us conduct our experiment on these classification algorithms: . from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression classifiers = [ SVC, DecisionTreeClassifier, RandomForestClassifier, LogisticRegression ] . We will first just use them as the come out-of-the-box, i.e. without any further finetuning, then we will try a hyperparameter optimization and then we do the whole thing again with an extended feature set, leaving us with 16 different strategies to compare regarding their technical and their business metric performance. . In order to avoid repeating boiler plate code, we first create a wrapper function that performs the training and classification, as well as an accuracy scoring and a scoring according to the business scoring that we created a above. . from sklearn.metrics import accuracy_score def train_and_classify(classifier, *args, **kwargs): # classifier class is first class object # we can use it a function variable clf_churn = classifier(*args, **kwargs) clf_churn.fit(X_train, y_train) acc = accuracy_score(y_test, clf_churn.predict(X_test)) business_metric = score(clf_churn, X_test, y_test) return (clf_churn, acc, business_metric) . # create two dataframes to store the results results_acc = pd.DataFrame() results_cost = pd.DataFrame() . Algorithms &quot;Out of the Box&quot; . Then we perfom the classification and record the accuracy and business metrics in our dataframe. It looks a bit scary, but most of it is just extracting the info from the trained classifier and then formatting the result. . for classifier in classifiers: (clf, acc, business_metric) = train_and_classify(classifier) # extract name of classfier, save accuracy and cost to dataframes # out of the box-classifiers results_acc.loc[str.split(str(clf), &#39;(&#39;)[0], &#39;OOTB&#39;] = acc results_cost.loc[str.split(str(clf), &#39;(&#39;)[0], &#39;OOTB&#39;] = business_metric #also print results here, to motivate us to keep going print( f&quot;{str.split(str(clf),&#39;(&#39;)[0]:30} acc= {acc:10.3} cost={business_metric:10.7}&quot; ) . Algorithms with Hyperparameter Optimization . As a next step, we will provide sklearn with a set of parameters for each classification algorithms and then instruct a grid search across all these parameters. We will then pick the set of parameters which gives us the best result for each algorithm. . Again, in order to avoid copy and paste, we create a wrapper that handles the grid search and training. . from sklearn.model_selection import GridSearchCV def train_optimize_classify(classifier, *args, **kwargs): clf_churn_search = classifier() # Here we can use the custom scorer # to ask the GridSearch to return # the hyperparameters with the # lowest opportunity cost search = GridSearchCV(clf_churn_search, *args, **kwargs, cv=5, verbose=0, n_jobs=8, scoring=score) search.fit(X_train, y_train) acc = accuracy_score(y_test, search.predict(X_test)) business_metric = score(search, X_test, y_test) #print(classification_report(y_test, search.predict(X_test))) #print(&quot;Accuracy Score: &quot;, acc) return (search, acc, business_metric) . As an aside: replacing the scoring in the call above with &#39;recall&#39; should lead to similar results, as recalling as many churners as possible (without being too biased towards it) means minimizing the cost function. . We define all the parameters we want to cycle trough per method. As the methods vary greatly in their concept, we deal with a wide variety of parameters here. . # choosing some of the weights proportional to the cost in order to # see if the additional weighting improves anything param_grid_svc = { &#39;C&#39;: [1, 2, 4, 8], &#39;kernel&#39;: [&#39;rbf&#39;, &#39;sigmoid&#39;], &#39;class_weight&#39;: [None, { 0: 1.35, 1: 2.44 }], &#39;decision_function_shape&#39;: [&#39;ovo&#39;, &#39;ovr&#39;], &#39;random_state&#39;: [42] } param_grid_decision_tree = { &#39;criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;], &#39;min_samples_split&#39;: [2, 10, 20], &#39;class_weight&#39;: [None, { 0: 1.35, 1: 2.44 }], &#39;random_state&#39;: [42] } param_grid_random_forest = { &#39;criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;], &#39;min_samples_split&#39;: [2, 10, 20], &#39;class_weight&#39;: [None, { 0: 1.35, 1: 2.44 }], &#39;random_state&#39;: [42] } param_grid_log_reg = { &#39;C&#39;: [1, 10, 100, 1000], &#39;penalty&#39;: [&#39;l2&#39;], &#39;dual&#39;: [False], &#39;class_weight&#39;: [None, { 0: 1.35, 1: 2.44 }], &#39;random_state&#39;: [42] } param_grids = [ param_grid_svc, param_grid_decision_tree, param_grid_random_forest, param_grid_log_reg ] . Then cycle through all the classifiers and let our wrapper do the work of training, cross-validation and choosing the best set of parameters. . We record the results again in our dataframes for later comparison. . for classifier, param_grid in list(zip(classifiers, param_grids)): (clf, acc, business_metric) = train_optimize_classify(classifier, param_grid=param_grid) results_acc.loc[str.split(str(clf.__dict__[&#39;estimator&#39;]), &#39;(&#39;)[0], &#39;Optimized&#39;] = acc results_cost.loc[str.split(str(clf.__dict__[&#39;estimator&#39;]), &#39;(&#39;)[0], &#39;Optimized&#39;] = business_metric print( f&quot;{str.split(str(clf.__dict__[&#39;estimator&#39;]),&#39;(&#39;)[0]:30} acc= {acc:10.6} cost={business_metric:10.7}&quot; ) . RandomForestClassifier acc= 0.785868 cost= -100769.1 LogisticRegression acc= 0.774666 cost= -98445.9 . Feature Engineering with and without Hyperparameter Optimization . As the last leg of our trip, we will engineer a few more features before we try to classify the dataset again. For this, we define additional transformation to add to our pipeline and then re-run the pipeline on the original dataset. . As additional features we will use: . the number of different services a user has on his contract | the relative cost of the contract among all customers with the same combination of services | if the monthly charges are above or below the average monthly charges on that contract as an indicator if the customer has upgraded the contract during its lifetime | . from functools import partial @logger def count_services_and_generate_codes(df): df[&#39;service_count&#39;] = df[service_cols].apply( lambda x: np.sum([x_el == &#39;Yes&#39; for x_el in x]), axis=1) df[&#39;all_services_code&#39;] = df[service_cols].astype(str).apply( lambda x: &#39;&#39;.join(x), axis=1) return df @logger def join_and_calculate_relative_cost(df): df_look_up_service_charges = df.groupby([&#39;all_services_code&#39;]).agg( {&#39;MonthlyCharges&#39;: [&#39;min&#39;, &#39;max&#39;]}) df_look_up_service_charges.columns = [&#39;min&#39;, &#39;max&#39;] df = df.join(df_look_up_service_charges, on=&#39;all_services_code&#39;) df[&#39;rel_price&#39;] = (df[&#39;MonthlyCharges&#39;] - df[&#39;min&#39;]) / (df[&#39;max&#39;] - df[&#39;min&#39;]) df[&#39;rel_price&#39;] = df[&#39;rel_price&#39;].fillna(1) return df @logger def up_or_downgrade(df): df[&#39;difference_monthly_total&#39;] = df[&#39;tenure&#39;] * df[&#39;MonthlyCharges&#39;] - df[ &#39;TotalCharges&#39;] return df @logger def drop_redundant_and_na(df): df = df.drop([&#39;TotalCharges&#39;], axis=1) #df = df.drop([&#39;TotalCharges&#39;, &#39;all_services_code&#39;, &#39;max&#39;, &#39;min&#39;], axis=1) df = df.dropna() return df # create an extended version of # the encode and scale function # since we now have two more numeric # features encode_and_scale_added_features = partial( encode_and_scale, float_cols=float_cols + [&#39;difference_monthly_total&#39;, &#39;rel_price&#39;]) df_work = (df .pipe(copy_df) .pipe(drop_id) .pipe(remove_whitespace) .pipe(count_services_and_generate_codes) .pipe(join_and_calculate_relative_cost) .pipe(ensure_formats) .pipe(up_or_downgrade) .pipe(encode_and_scale_added_features) .pipe(drop_redundant_and_na) ) . copy_df : shape = (7043, 21) nan rows = 0 drop_id : shape = (7043, 20) nan rows = 0 remove_whitespace : shape = (7032, 20) nan rows = 0 count_services_and_generate_codes: shape = (7032, 22) nan rows = 0 join_and_calculate_relative_cost: shape = (7032, 25) nan rows = 0 ensure_formats : shape = (7032, 25) nan rows = 0 up_or_downgrade : shape = (7032, 26) nan rows = 0 encode_and_scale : shape = (7032, 32) nan rows = 0 drop_redundant_and_na: shape = (7032, 31) nan rows = 0 . We also need to recrate the train and test sets, as they are now based on the extended dataset. . y = df_work[&#39;Churn_Yes&#39;] X = df_work.drop([&#39;Churn_Yes&#39;], axis=1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y) . And the we simply redo the classification on the extended dataset across all the algorithms, record the results and then do it all over again using the new dataset and hyperparameter optimization. . # without hyperparameter optimization for classifier in classifiers: # trains, classifies and evaluates (clf, acc, business_metric) = train_and_classify(classifier) # extract name of classfier, save accuracy and cost to dataframes results_acc.loc[str.split(str(clf), &#39;(&#39;)[0], &#39;OOTB w/ feature engineering&#39;] = acc results_cost.loc[str.split(str(clf), &#39;(&#39;)[0], &#39;OOTB w/ feature engineering&#39;] = business_metric #also print results here, to motivate us to keep going print( f&quot;{str.split(str(clf),&#39;(&#39;)[0]:30} acc= {acc:10.3} cost={business_metric:10.7}&quot; ) . SVC acc= 0.795 cost= -106239.2 DecisionTreeClassifier acc= 0.715 cost= -133177.8 RandomForestClassifier acc= 0.793 cost= -106767.6 LogisticRegression acc= 0.799 cost= -100093.1 . # with hyperparameter optimization for classifier, param_grid in list(zip(classifiers, param_grids)): (clf, acc, business_metric) = train_optimize_classify(classifier, param_grid=param_grid) results_acc.loc[str.split(str(clf.__dict__[&#39;estimator&#39;]), &#39;(&#39;)[0], &#39;Optimized w/ Feature Engineering&#39;] = acc results_cost.loc[str.split(str(clf.__dict__[&#39;estimator&#39;]), &#39;(&#39;)[0], &#39;Optimized w/ Feature Engineering&#39;] = business_metric print( f&quot;{str.split(str(clf.__dict__[&#39;estimator&#39;]),&#39;(&#39;)[0]:30} acc= {acc:10.6} cost={business_metric:10.7}&quot; ) . SVC acc= 0.770358 cost= -101313.0 DecisionTreeClassifier acc= 0.753555 cost= -120100.9 RandomForestClassifier acc= 0.792762 cost= -97979.7 LogisticRegression acc= 0.77682 cost= -98577.99 . This brings us to the results of the whole exercise, which is to deliberate which strategy to apply to maximize the business impact of running an algorithm that is trying to predict the next month&#39;s churners. . So, let us have a look at the accuracy score: . results_acc.style.highlight_max(color=&#39;lightgreen&#39;, axis=None) . OOTB Optimized OOTB w/ feature engineering Optimized w/ Feature Engineering . SVC 0.80 | 0.77 | 0.80 | 0.77 | . DecisionTreeClassifier 0.73 | 0.76 | 0.71 | 0.75 | . RandomForestClassifier 0.79 | 0.79 | 0.79 | 0.79 | . LogisticRegression 0.80 | 0.77 | 0.80 | 0.78 | . It seems that the overall accuracy is best with the vanilla logistic regression. . If we compare that to our business metric - the opportunity cost from misclassfications - we see that an altogether different approach takes the cake: . results_cost.style.highlight_max(color=&#39;lightgreen&#39;, axis=None) . OOTB Optimized OOTB w/ feature engineering Optimized w/ Feature Engineering . SVC -104895.00 | -99759.03 | -106239.21 | -101313.03 | . DecisionTreeClassifier -126627.69 | -117785.43 | -133177.80 | -120100.89 | . RandomForestClassifier -107288.16 | -100769.13 | -106767.57 | -97979.70 | . LogisticRegression -99533.70 | -98445.90 | -100093.14 | -98577.99 | . Note, that we calculate the cost here, but a negative number here does not mean we have negative cost, i.e. profit. The negative sign appears because we told our custom scorer that greater is nor better when calculating cost. Internally, this is handled by multiplying the result of the cost function with &#39;-1&#39;. . We conclude that when comparing various strategies to tackle a classification problem, it is inadvisable to rely on the default scoring without giving it further thought. Rather, one should use any available domain knowledge in order to define a evaluation metric that is in line with the set business goals. . Determining Influential Features . We can also examine the logistic regression classifier for the values of the betas which relate to the odds of being a class &quot;1&quot; member and churn. Larger values represent higher chances to churn. . By that logic high monthly charges and multi-year contracts seem to be the most important factors in a customer&#39;s decision not to churn. . # our clf variable still contains the last classifier, which is the logistic regression feature_influence = pd.DataFrame(zip(X.columns,clf.best_estimator_.coef_[0])) feature_influence.columns = [&#39;Feature&#39;, &#39;beta&#39;] feature_influence.sort_values(by=&#39;beta&#39;).head(3) . Feature beta . 1 MonthlyCharges | -3.76 | . 8 MultipleLines_No phone service | -2.56 | . 25 Contract_Two year | -1.42 | . Conversely, being a streaming customer and using a fiber-optic based internet connection seem to be the biggest factors to encourage a customer to churn. Depending on when the data was collected, this might have been at the height of the cord cutting trend that favored Netflix &amp; co. Source. The phone contracts might have been collateral damage, but without any supplemental data that is pure speculation. . feature_influence.sort_values(by=&#39;beta&#39;).tail(3) . Feature beta . 21 StreamingTV_Yes | 1.53 | . 23 StreamingMovies_Yes | 1.58 | . 10 InternetService_Fiber optic | 4.04 | . We can also use builtin functions from sklearn to determine feature importance: The permutation importance is calculated by comparing the accuracy as is with the accuracy of the classifier working on a modified dataset where one feature column is randomly scrambled. . It is immediately intuitive that a jumbled feature that does nothing to negatively impact our accuracy or any other metric must not be &quot;important&quot; for making a prediction within the model. . from sklearn.inspection import permutation_importance r = permutation_importance(clf, X_test, y_test, n_repeats=30, random_state=0, scoring=score) . for i in r.importances_mean.argsort()[::-1]: if r.importances_mean[i] - 2 * r.importances_std[i] &gt; 0: print(f&quot;{df_work.columns[i]:&lt;40}&quot; f&quot;{r.importances_mean[i]:.3f}&quot; f&quot; +/- {r.importances_std[i]:.3f}&quot;) . InternetService_Fiber optic 77651.308 +/- 3415.076 MonthlyCharges 62379.632 +/- 3371.916 tenure 30305.849 +/- 2725.497 StreamingTV_Yes 18698.764 +/- 2478.761 StreamingMovies_Yes 18538.961 +/- 2909.398 MultipleLines_No phone service 12803.406 +/- 1637.567 Contract_Two year 9291.107 +/- 1824.843 MultipleLines_Yes 9128.973 +/- 2065.377 Contract_One year 5778.808 +/- 1505.091 OnlineBackup_Yes 3210.823 +/- 1065.942 . By this metric we see that the fibre optic option seems to have the largest impact on the prediction accuracy followed by the monthly charges. So, we get consistency in these two features, with one having a great positive, and one having a great negative impact on the customers&#39; probability to churn. . Bonus: Comparing the Result to AutoML . As I am not altogether happy with these results, I wanted to know how our solution fares in comparison with a Google AutoML model. So, I fed the data into Google Cloud and let it train for an hour. Interestingly, AutoML did use the whole hour and charged me 16$, but it improved the accuracy results by 2.1%. . . That is also not that great - it&#39;s a little bit better in the overall accuracy than the model we trained here, but looking at the confusion matrix, it&#39;s practically no good in achieving our business goal. . . At least in terms of feature importance, we achieve similar results. Bear in mind that I one-hot-encoded the variables, so every feature is multiplied by the number of possible categories in that feature. . . Sources . Logging Decorators | Custom Performance Metrics | Dataset | Permutation Importance of Features | .",
            "url": "http://blog.huppertz-consulting.de/classification/feature%20engineering/workflow/evaluation/2020/11/10/classification-business-metrics-relevant-features.html",
            "relUrl": "/classification/feature%20engineering/workflow/evaluation/2020/11/10/classification-business-metrics-relevant-features.html",
            "date": " • Nov 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Support Vector Machines: Building Intuition",
            "content": "When you start having intricate overlaps or irregularly shaped boundaries between the classes SVM generalize well, but need a little bit of tweaking and tuning to achieve good results. In other words: you need to know what you are doing. . Let&#39;s walk through some simple examples from the sklearn documentation first to get a feeling of how SVMs behave. We will go light on the theory as I will explore this another time in a future post. . First we need to import some basics: . import numpy as np import matplotlib.pyplot as plt import matplotlib as mpl from sklearn import svm from sklearn.datasets import make_blobs import warnings warnings.filterwarnings(&#39;ignore&#39;) # matplotlib is a very talkative library . Trivial Examples in 2 and 3 Dimensions . 2 Dimensional Data . The first example is loosely based on the sklearn documentaiton for support vector machines. We start by creating a few 2-dimensional data points $X in mathbb{R}^{40x2}$ and assigning them each one out of two classes $y in {0,1 }^{40}$. . X, y = make_blobs(n_samples=40, centers=2, random_state=6) plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired) # saving the x and y axis max/min values # we will need those later for more plots xlim = plt.gca().get_xlim() ylim = plt.gca().get_ylim() . It is very easy to separate these two classes by intuition with simple straight line, right through the middle of the gap between the two classes. You would then - intuitively - classify any new data south of that line belonging to the red class and any data point north of that line to the blue class. . In order to see if the SVM arrives at a result similiar to our intuition, let us first train the model. . # create and fit the model clf = svm.SVC(kernel=&#39;linear&#39;, C=1) clf.fit(X, y) . SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;, kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) . What follows next is basically only for visualization purposes. We draw an equisdistant grid across the $ mathbb{R}^2$ domain where our features $X$ live and let the SVM decide for each pair of $ x = (x^{(1)},x^{(2)})$ coordinates, which score it assigns to that point. . # create grid of x_1 and x_2 coordinates x_1 = np.linspace(xlim[0], xlim[1], 30) x_2 = np.linspace(ylim[0], ylim[1], 30) X_1, X_2 = np.meshgrid(x_1, x_2) grid = np.vstack([X_1.ravel(), X_2.ravel()]).T . With this grid we can calculate the classification for &quot;all&quot; possible points in the feature space or at least a close enough approximation. . # Calculate the value for every possible x_1-x_2-combination # that is almost the same as using the predict function, but just # shying away from using a threshold to decide which class a # point would belong to Z = clf.decision_function(grid).reshape(X_1.shape) . Now, we can simply plot the data and then overlay the datapoints with the values of the decision function. We can do that as a colored overlay: . # plot the data points plt.scatter(X[:, 0], X[:, 1], c=&#39;k&#39;, s=30) # plot decision function and values cntr1 = plt.contourf(X_1, X_2, Z, levels=20, alpha=0.5, cmap=&#39;viridis&#39;) plt.colorbar(cntr1).set_label(&#39;decision function&#39;) plt.xlabel(&#39;x_1&#39;) plt.ylabel(&#39;x_2&#39;) plt.show() . Or we can simply pick out the values which are most interesting to us, such as the decision boundary, where the decision function becomes $0$ and the support vectors, which are the data points where the decision function becomes $1$ or $-1$ respectively. . # plot the data points plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired) # plot decision boundary and margins plt.contour(X_1, X_2, Z, colors=&#39;k&#39;, levels=[-1, 0, 1], alpha=0.5, linestyles=[&#39;--&#39;, &#39;-&#39;, &#39;--&#39;]) # plot support vectors plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100, linewidth=1, facecolors=&#39;none&#39;, edgecolors=&#39;k&#39;) plt.show() . As we can see, the SVM has generated a solution that is in line with our intuitive guess about how the featuer space should be divided up according to classes. The dashed lines represent the levels of $1$ and $-1$ where the solid line defines the decision boundary at the zero-level of the decision function. . To do right by the SVM algorithm though - and to mentally prepare ourselves for the generalization to non-trivial examples that lies ahead - we have to visualize the data and the decision function in 3D. . from mpl_toolkits.mplot3d import Axes3D fig = plt.figure(figsize=(12, 12)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.plot_surface(X_1, X_2, Z, cmap=&#39;viridis&#39;, alpha=0.5) ax.contour(X_1, X_2, Z, colors=&#39;k&#39;, levels=[-1, 0, 1], alpha=1, linestyles=[&#39;--&#39;, &#39;-&#39;, &#39;--&#39;]) xy_data = np.vstack([X[:, 0].ravel(), X[:, 1].ravel()]).T ax.scatter(X[:, 0], X[:, 1], 0, c=y, s=30, cmap=plt.cm.Paired) ax.set_xlabel(&#39;x_1&#39;) ax.set_ylabel(&#39;x_2&#39;) ax.set_zlabel(&#39;decision function&#39;) ax.view_init(15, -15) . As we can see here, the decision function defines a hyperplane separating our datapoints into areas which map to $Z&lt;0$ and $Z&gt;0$ respectively. Note that the hyper&quot;plane&quot; is actually the solid black line that you see where $Z=0$. . 3-Dimensional Data . Of course there is no need to limit oneself to two features, but as we always need to add at least one dimension to create a hyperplane, it is hard to create sensible visualizations in higher dimensions. But let us at least try for 3 feature dimensions. . # we create 40 separable points # note that X has three dimensions here and y is the class they belong to X, y = make_blobs(n_samples=40, n_features=3, centers=2, random_state=6) fig = plt.figure(figsize=(12, 12)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, s=30, cmap=plt.cm.Paired) # saving the x and y axis max/min values # we will need those later for more plots xlim = plt.gca().get_xlim() ylim = plt.gca().get_ylim() zlim = plt.gca().get_zlim() . We then fit the model again, but now simply with a 3D input. . clf_3d = svm.SVC(kernel=&#39;linear&#39;, C=1) clf_3d.fit(X, y) . SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;, kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) . And then repeat our grid creation procedure again, just add one more dimension. . # create grid of x_1-x_2-x_3 coordinates x_1 = np.linspace(xlim[0], xlim[1], 30) x_2 = np.linspace(ylim[0], ylim[1], 30) x_3 = np.linspace(zlim[0], zlim[1], 30) X_1, X_2, X_3 = np.meshgrid( x_1, x_2, x_3) grid_3d = np.vstack([X_1.ravel(), X_2.ravel(), X_3.ravel()]).T # additionally create a grid only consisting of the data_points # this is essentially just reformatting them # so we can work with the grid and the data # without changing our functions grid_3d_data = np.vstack([X[:,0].ravel(), X[:,1].ravel(), X[:,2].ravel()]).T . # Calculate the value for for every possible x_1-x_2-x_3-combination fourth_dimension_grid = clf_3d.decision_function(grid_3d) fourth_dimension_data = clf_3d.decision_function(grid_3d_data) . Now, we have the little challenge that we have been working with point clouds so far. When drawing in 2D there is an interpolation going on under the hood, so we could easily draw a decision function using &quot;contour&quot; and matplotlib would fill in the gaps in our grid. I know of no such function in 3D, so we need a little hack. . As an approximation of the zero-level of the decision function, I will use the points in space where it is very close to $0$. . decision_boundary_3d = grid_3d[np.abs(fourth_dimension_grid)&lt;0.01] . We then use a triangulated surface plot to fill the gaps in the point cloud. It looks a bit wonky, but it does the job. . fig = plt.figure(figsize=(12, 12)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) # create triangulated surface with the # points of the decision boundary ax.plot_trisurf(decision_boundary_3d[:, 0], decision_boundary_3d[:, 1], decision_boundary_3d[:, 2], color=&#39;teal&#39;, alpha=0.4) # add in the 3D datapoints and color them # according to their decision function values scatter = ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=fourth_dimension_data, s=30, cmap=&#39;viridis&#39;) plt.colorbar(scatter, shrink = .7).set_label(&#39;decision function&#39;) ax.set_xlabel(&#39;x_1&#39;) ax.set_ylabel(&#39;x_2&#39;) ax.set_zlabel(&#39;x_3&#39;) ax.view_init(15, 135) . Of course, this is another very trivial example, the dataset is clearly linearly separable just in three dimensions instead of two. But we have been able to buld intuition about how the separation between classes in more than three dimensions works, i.e. by defining a function whose zero-level separates the classes. . Summary . A few trivial examples have shown the basic principles of SVM: . embed the data into a higher dimensional space | use a function mapping the data into the higher dimensional space | choose the function so that the difference between classes is high, but lower for different datapoints within the same class | split the distance in the values and classify new data according to which side of the split they appear on | . Generalizing to Non-Trivial Examples . The more interesting case is of course the one where we cannot simply arrive at an almost perfect solution by eyeballing and jamming a straight line or plane between the clusters. So what happens when we don&#39;t have a linearly separable dataset? . 2-Dimensional Data . Let us create a nested, 2-dimensional dataset using the make_gaussian_quantiles function. . from sklearn.datasets import make_gaussian_quantiles X, y = make_gaussian_quantiles(mean=None, cov=1.0, n_samples=100, n_features=2, n_classes=2, shuffle=True, random_state=42) # draw the result plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired) # saving the x and y axis max/min values # we will need those later for more plots xlim = plt.gca().get_xlim() ylim = plt.gca().get_ylim() . Then we try to fit our classifier again, without accommodating to the new dataset structure. . clf = svm.SVC(kernel=&#39;linear&#39;) clf.fit(X, y) . SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;, kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) . And visualize the results just like before. . x_1 = np.linspace(xlim[0], xlim[1], 30) x_2 = np.linspace(ylim[0], ylim[1], 30) X_1, X_2 = np.meshgrid(x_1, x_2) grid = np.vstack([X_1.ravel(), X_2.ravel()]).T Z = clf.decision_function(grid).reshape(X_1.shape) plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired) plt.contour(X_1, X_2, Z, colors=&#39;k&#39;, levels=[-1, 0, 1], alpha=0.5, linestyles=[&#39;--&#39;, &#39;-&#39;, &#39;--&#39;]) plt.show() . The SVM is clearly out of its depth with the concentric dataset. Our decision boundary has almost nothing to do with the data and is completely useless. This is where we have to introduce the concept of &#39;kernels&#39;. . In the trivial case above we introduced a linear function that had values larger than $1$ around the one class of dots and smaller than $-1$ around the other class. In the trench between the two sets, our decision function went from $1$ through $0$ to $-1$. . Now we simple need to do the same thing but instead of an linear function, we use another function that is larger than $0$ for data from one class and smaller for the other class and pass thorugh $0$ on its way from one class to another. . And, ideally, it is differentiable to do fancy math stuff with it later. . It turns out that that we can construct such a function by using the sum of a lof of smaller functions and one good option is the gaussian kernel function. You can simply think &#39;normal distribution&#39; here: we create a little normal distribution around each data point and multiply with $1$ or $-1$ accordingly. . def gaussian_kernel(X, Y, sigma=1): similarity = 1 / (sigma * np.sqrt(2 * np.pi)) * np.exp( -np.linalg.norm(np.subtract(X, Y), ord=2) / (2 * (sigma**2))) return np.array(similarity).reshape(1, -1) . This function simply takes to vectors $X$ and $Y$, calculates the euclidean distance between them and then scales the distance on a gaussian curve, i.e. close to the peak if they are very close to each other but then rapidly declining, given the right $ sigma$ . If we choose $(x^{(1)},x^{(2)}) = [0,0]$ as the center, that is what the function looks like: . fig = plt.figure(figsize=(12, 12)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) Z = np.array([gaussian_kernel(point, [0, 0]) for point in grid]).reshape(X_1.shape) ax.plot_surface(X_1, X_2, Z) ax.set_xlabel(&#39;X_0&#39;) ax.set_ylabel(&#39;X_1&#39;) ax.set_zlabel(&#39;kernel function&#39;) ax.view_init(0, 105) . This function fulfills all the criteria above, but in order to use it in the the SVM we still have to comply with a technicality. . I was actually a bit stumped by this, as the sklearn documentation does not go into detail at all about how to properly implement a kernel function. Luckily, there was a stackoverflow comment that came to my rescue and which you will find linked in my sources. It said: . For efficiency reasons, SVM assumes that the kernel is a function accepting two matrices of samples. . So, instead of single vectors, we need a function that takes and returns matrices such as: . from functools import partial def proxy_kernel(X, Y, K): gram_matrix = np.zeros((X.shape[0], Y.shape[0])) for i, x in enumerate(X): for j, y in enumerate(Y): gram_matrix[i, j] = K(x, y) return gram_matrix correct_gaussian_kernel = partial(proxy_kernel, K=gaussian_kernel) . We can then try to fit the SVM again, using the new kernel: . clf = svm.SVC(kernel=correct_gaussian_kernel, C=1) clf.fit(X, y) . SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;, kernel=functools.partial(&lt;function proxy_kernel at 0x0000015D60DA8510&gt;, K=&lt;function gaussian_kernel at 0x0000015D60DF4D90&gt;), max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) . From here on out, it&#39;s smooth sailing: we resuse the grid from before, calculate the decision function on each dot in the grid an visualize the decision boundary . Z = clf.decision_function(grid).reshape(X_1.shape) plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired) plt.contour(X_1, X_2, Z, colors=&#39;k&#39;, levels=[-1, 0, 1], alpha=0.5, linestyles=[&#39;--&#39;, &#39;-&#39;, &#39;--&#39;]) plt.show() . As we see here the decision boundary (solid line) neatly encircles the cluster of blue dots except for a three escapees in the lower right and top left quadrant of the boundary. . When plotting the decision function in 3D and overlaying it with the data, it becomes a sort of well where every data point that crosses the solid boundary falls into the well and gets classified as a blue dot - or in other words: the solid line indicates where the zero-level of the decision function lies and that zero-level defines the hyperplane that separates the data. . fig = plt.figure(figsize=(12, 12)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.plot_surface(X_1, X_2, Z, cmap=&#39;viridis&#39;, alpha=0.2) ax.contour(X_1, X_2, Z, colors=&#39;k&#39;, levels=[-1, 0, 1], alpha=1, linestyles=[&#39;--&#39;, &#39;-&#39;, &#39;--&#39;]) xy_data = np.vstack([X[:, 0].ravel(), X[:, 1].ravel()]).T ax.scatter(X[:, 0], X[:, 1], clf.decision_function(xy_data), c=y, s=30, cmap=plt.cm.Paired) ax.set_xlabel(&#39;x_1&#39;) ax.set_ylabel(&#39;x_2&#39;) ax.set_zlabel(&#39;decision function&#39;) ax.view_init(15, 15) . 3-Dimensional Data . When trying to elevate this experiment to three dimensional input data, we obviously lose the ability to visualize the decision function in its entirety, but as before, we can still visualize the hyperplane of the zero-level set. Let us start with concentric 3D data. i.e. it is a ball of &quot;blue&quot; data in the middle of a &quot;red&quot; data cloud. . # note that X has three dimensions here and y is the class they belong to X, y = make_gaussian_quantiles(mean=None, cov=1.0, n_samples=200, n_features=3, n_classes=2, shuffle=True, random_state=42) fig = plt.figure(figsize=(12, 12)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, s=30, cmap=plt.cm.Paired) # saving the x and y axis max/min values # we will need those later for more plots xlim = plt.gca().get_xlim() ylim = plt.gca().get_ylim() zlim = plt.gca().get_zlim() ax.set_xlabel(&#39;x_1&#39;) ax.set_ylabel(&#39;x_2&#39;) ax.set_zlabel(&#39;x_3&#39;); ax.view_init(15, 215) . And we also classify that with our custom gaussian kernel. So, what&#39;s the gaussian kernel in 3D? It is a sphere where the space is more densely packed in the middle and the density then (depending on $ sigma$) quickly decreased as you move outwards. . clf_3d = svm.SVC(kernel=correct_gaussian_kernel) clf_3d.fit(X, y) . SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;, kernel=functools.partial(&lt;function proxy_kernel at 0x0000015D60DA8510&gt;, K=&lt;function gaussian_kernel at 0x0000015D60DF4D90&gt;), max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) . Again, we create a grid covering the whole data domain and calculating the decision function for all the grid points. . # create grid of x_1-x_2-x_3-coordinates x_1 = np.linspace(xlim[0], xlim[1], 30) x_2 = np.linspace(ylim[0], ylim[1], 30) x_3 = np.linspace(zlim[0], zlim[1], 30) X_1, X_2, X_3 = np.meshgrid(x_1, x_2, x_3) grid_3d = np.vstack([X_1.ravel(), X_2.ravel(), X_3.ravel()]).T # Calculate decision function value fourth_dimension_grid = clf_3d.decision_function(grid_3d) . Properly displaying the 3-dimensional zero-level of a 4-dimensional function was a bit challenging, as there is no function in matplotlib that gracefully handles converting a point cloud to a surface. . So, I sliced the $x_3$-axis up and then, for each fixed $x_3$ value, I calculated the decision function across the whole $x_1$-$x_2$-domain.Finally, I filtered for where the result is zero, thereby identifying a ring that is part of the whole 3D surface. . As matplotlib can apparently nicely interpolate the zero-level in 2 dimensions, we get the contour of a slightly deformed ball that nicely encompasses all the blue data points. . fig = plt.figure(figsize=(12,12)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) for x_3 in np.unique(grid_3d[:,2]): # create grid in x_1-x_2-domain X1,X2 = np.meshgrid(x_1,x_2) # Calculate the value of the decision function for each x_3 slice X3 = clf_3d.decision_function(grid_3d[grid_3d[:,2] == x_3]) # offset the slice to the right position # as we are looking for the zero-level of Z (Z=0) we actually need to # display the level x_3 as Z+x_3 = 0+x_3 = x_3 cset = ax.contour(X1,X2,X3.reshape(X1.shape)+x_3, levels = [x_3], zdir=&#39;z&#39;) # plot the data ax.scatter(X[:, 0], X[:, 1], X[:,2],c = y, s=30, cmap=plt.cm.Paired) ax.set_xlabel(&#39;x_1&#39;) ax.set_ylabel(&#39;x_2&#39;) ax.set_zlabel(&#39;x_3&#39;); ax.view_init(15, 215) . So, we have seen in this chapter how to generalize the method into higher dimensions: Using a similarity kernel, we create a function that tends towards positive values for one class and negative values for the other. . But, this whole ordeal displays one of the biggest drawbacks of the method: you need to get a good idea about the topology that your dataset creates before you can choose an appropriate kernel. Alternatively, you can brute force your way through all the available kernel and parameter options. . Sources . http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf | https://www.youtube.com/watch?v=efR1C6CvhmE | https://www.youtube.com/watch?v=FCUBwP-JTsA | https://www.youtube.com/watch?v=wBVSbVktLIY | https://stackoverflow.com/questions/26962159/how-to-use-a-custom-svm-kernel | .",
            "url": "http://blog.huppertz-consulting.de/classification/application/support%20vector%20machines/2020/10/06/support-vector-machines-building-intuition.html",
            "relUrl": "/classification/application/support%20vector%20machines/2020/10/06/support-vector-machines-building-intuition.html",
            "date": " • Oct 6, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An In-Depth Look at Logistic Regression",
            "content": "Understanding the ins and outs of a logistic regression is non-trivial. Many sources either only touch the theoretical side or the implementation side respectively. In this post, I would like to create a one-stop-shop for the theoretical basis and the practical implementations of the logistic regression. . Our Dataset . We will use the well-known iris dataset as basis for our discussion. It is simple enough to still employ some intuition when trying to understand the logistic regression. When loading the dataset from sklearn, we get 150 observations of measurements of some of the plants&#39; features. Then botanists have classified the irises into three subtypes with fancy latin names, but since I am no botanist, we will just pretend that - based on those measurements - we can determine if the flower blooms red, green or blue. The measurements will end up in the set of observations $X$, with each observation consisting of 4 measurements each and the classification ends up in the vector $y$ with entries being 0 = red, 1 = green, 2 = blue. . from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression import warnings warnings.filterwarnings(&#39;ignore&#39;) import numpy as np import pandas as pd import matplotlib.pyplot as plt . X, y = load_iris(return_X_y=True) . columns = (&#39;Measurement 1&#39;, &#39;Measurement 2&#39;, &#39;Measurement 3&#39;, &#39;Measurement 4&#39;) dataset = pd.DataFrame(X, columns=columns) dataset[&quot;True Classification&quot;] = y dataset . Measurement 1 Measurement 2 Measurement 3 Measurement 4 True Classification . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . 3 4.6 | 3.1 | 1.5 | 0.2 | 0 | . 4 5.0 | 3.6 | 1.4 | 0.2 | 0 | . ... ... | ... | ... | ... | ... | . 145 6.7 | 3.0 | 5.2 | 2.3 | 2 | . 146 6.3 | 2.5 | 5.0 | 1.9 | 2 | . 147 6.5 | 3.0 | 5.2 | 2.0 | 2 | . 148 6.2 | 3.4 | 5.4 | 2.3 | 2 | . 149 5.9 | 3.0 | 5.1 | 1.8 | 2 | . 150 rows × 5 columns . colors = [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;] for X_el, y_el in zip(X, y): plt.plot(X_el, colors[y_el], alpha=0.1) plt.xticks(np.arange(4), columns) plt.show() . We can clearly see that measurement 3 carries a lot of information. It separates neatly between the red flowers and the rest, and it even seems to be a good guide to separate the greens from the blues. Same goes for measurement 4, while 1 and 2 seem to give no (or very little) hint as to which class a certain specimen belongs to. . clf = LogisticRegression(random_state=42).fit(X, y) . After having trained the model, the &quot;predict&quot; function allows us to get a prediction for each input tuple. The classes are numbered not named, but can of course be converted to names using the appropriate mapping. . # 0 = red, 1 = green, 2 = blue clf.predict(X) . array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) . Besides a simple class prediction, we can also get the probability for each class for each input data sample. . clf.predict_proba(X)[0:5] # abbreviated output . array([[8.78030305e-01, 1.21958900e-01, 1.07949250e-05], [7.97058292e-01, 2.02911413e-01, 3.02949242e-05], [8.51997665e-01, 1.47976480e-01, 2.58550858e-05], [8.23406019e-01, 1.76536159e-01, 5.78217704e-05], [8.96034973e-01, 1.03953836e-01, 1.11907339e-05]]) . For the first example above, we assign an 88% chance of it belonging to class &#39;0&#39; , a 12% chance of it belonging to class &#39;1&#39; and (almost) 0% chance of it belonging to class &#39;2&#39;. using the argmax function, we could then map the probabilites to the exact outcome of the predictfunction above. . So, for the first part, we have examined the dataset and we got an idea, how well it will be able to separate the classes, based on the features given. Also, we have generated the output of the sklearn logistic regression that a training on the complete dataset is providing and how to interpret that. . Evaluating the Model . A common metric to evaluate the quality of predictions is the &#39;score&#39;, which - according to the sklearn help - is: . In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. . clf.score(X, y) . 0.96 . Basically that means we count all correctly classified labels as a percentage like so: . correct = [] for y_true, y_pred in zip(y, clf.predict(X)): if y_true == y_pred: correct.append(1) else: correct.append(0) sum(correct) / len(correct) . 0.96 . Given that we used the complete dataset for training, that&#39;s just okay. A more detailed approach at evaluating the quality of our model would be based on the classification report. . from sklearn.metrics import classification_report print(classification_report(y_pred=clf.predict(X), y_true=y)) . precision recall f1-score support 0 1.00 1.00 1.00 50 1 0.98 0.90 0.94 50 2 0.91 0.98 0.94 50 micro avg 0.96 0.96 0.96 150 macro avg 0.96 0.96 0.96 150 weighted avg 0.96 0.96 0.96 150 . While the class &quot;0 = red&quot; has perfect precision and recall, because classes &quot;1 = green&quot; and &quot;2 = blue&quot; somewhat overlap in their features, the algorithm mixes them up. This is shown by the recall 0.9 of class &quot;1&quot;, which means only 90% of all green flowers where classified as green and the precision of class &quot;2&quot; which means only 91% of all flowers classified as blue where actually blue. . y_true_with_jitter = y + np.random.rand(y.shape[0]) * 0.25 y_classified_with_jitter = clf.predict(X) + np.random.rand(y.shape[0]) * 0.25 plt.xticks(np.arange(3), (&#39;true red&#39;, &#39;true green&#39;, &#39;true blue&#39;)) plt.yticks(np.arange(3), (&#39;class. red&#39;, &#39;class. green&#39;, &#39;class. blue&#39;)) plt.scatter(y_true_with_jitter, y_classified_with_jitter, color=[colors[y_el] for y_el in y]) plt.show() . Is it a good model? Well, on one hand, the bare values of our statistics are really quite good, on the other we have a tiny dataset and we trained on the complete dataset as well, as we did not keep a holdout dataset for testing. Since we do not pay too much attention to the actual result of flower&#39;s colors being predicted correctly though and only want to understand how to arrive at the predictions,we will give it a pass. Just make a mental note never to consider that a good result in a real-world application. . A Look at the Inner Workings of the sklearn Logistic Regression . When looking at the variables of the log regression classifier after training, we find three sets of coefficients and three different intercepts. That is because log regression is essentially binary, i.e. does only a yes/no or 1/0 classification. If we have $n &gt; 2$ classes, we need split this problem into $n$ separate &quot;1 vs. rest&quot; classification problems. Each set of coefficients and each intercept belongs to one of these sub-classifications. . clf.__dict__[&quot;coef_&quot;] # just running clf.__dict__ spits out all the info about the trained model . array([[ 0.41021713, 1.46416217, -2.26003266, -1.02103509], [ 0.4275087 , -1.61211605, 0.5758173 , -1.40617325], [-1.70751526, -1.53427768, 2.47096755, 2.55537041]]) . clf.__dict__[&quot;intercept_&quot;] . array([ 0.26421853, 1.09392467, -1.21470917]) . If we now feed a set of features $x^i$ into the trained classifier, we can calculate the probabilities of $x^i$ belonging to a class vs. not belonging to that class via: . $$p(x_i)= frac{1}{1+e^{-( beta_{0} + beta_{1}x^{i}_1 + beta_{2}x^{i}_2 + beta_{3}x^{i}_3 + beta_{4}x^{i}_4)}}$$ . where $ beta_{0}$ is an intercept and $ beta_{1}.. beta_{4}$ are the coefficients for each entry in the feature vector $x^i = (x^{i}_1,x^{i}_2,x^{i}_3,x^{i}_4)$. We will later explore why this term is the correct one. Let us calculate the above for our very first observation in the dataset: . # @ is shorthand for matrix multiplication p_0 = 1 / (1 + np.exp(-(clf.intercept_[0] + X[0] @ clf.coef_[0]))) p_1 = 1 / (1 + np.exp(-(clf.intercept_[1] + X[0] @ clf.coef_[1]))) p_2 = 1 / (1 + np.exp(-(clf.intercept_[2] + X[0] @ clf.coef_[2]))) print(&#39;p_0 =&#39;, p_0) print(&#39;p_1 =&#39;, p_1) print(&#39;p_2 =&#39;, p_2) . p_0 = 0.9838989815463852 p_1 = 0.13666411838386314 p_2 = 1.209652515457732e-05 . With this calculation, we have now determined that our $x^0$ has a $0.98$ chance of belonging to class &quot;0 = red&quot; vs. any of the other classes - either green or blue. As we can see though, these three probabilities do not add up to 100% and why should they? These probabilities belong to three mathematically independent problems: . Does $x^i$ belong to class 0 vs. not to class 0? | Does $x^i$ belong to class 1 vs. not to class 1? | Does $x^i$ belong to class 2 vs. not to class 2? | . What happens, if we linearly scale those probabilities though so that they add up to 1? . p_sum = p_0 + p_1 + p_2 print(&quot;p_0_scaled =&quot;, p_0 / p_sum) print(&quot;p_1_scaled =&quot;, p_1 / p_sum) print(&quot;p_2_scaled =&quot;, p_2 / p_sum) . p_0_scaled = 0.8780303050242847 p_1_scaled = 0.12195890005075813 p_2_scaled = 1.0794924957147882e-05 . We have seen these exact numbers before and can make our choice for a prediction using argmax: . clf.predict_proba(X)[0] . array([8.78030305e-01, 1.21958900e-01, 1.07949250e-05]) . np.argmax(clf.predict_proba(X)[0]) # 0 = red, 1 = green, 2 = blue . 0 . Now we have an understanding how the interpret the data generated by the training process of sklearn and we have looked beyond the clf.predict function to understand how the predictions are picked in the trained model. . The Mathematical Background . We used the formula . $$p(x^i)= frac{1}{1+e^{-( beta_{0} + beta_{1}x^{i}_{1} + beta_{2}x^{i}_{2} + beta_{3}x^{i}_{3} + beta_{4}x^{i}_{4})}}$$ . above, which is technically a choice, not a mathematical coercion (there are, in fact, others that work as well), but why does that make sense? . In order to understand that, we need to understand odds and their relationship with probabilites first. If an event has a 50% chance of ocurring, the odds of it happening are 1:1 (1 time it happens, 1 time it does not). If an event has a 33,3% chance of happening, the odds are 1:2 (1 time it happens, 2 times it does not), 25% represents odds of 1:3, and 20% represents odds of 1:4. Or as a general formula: . $$ Odds = frac{p}{1-p} $$ . e.g. for $p = 0.25$ that evaluates to $ Odds = frac{0.25}{1-0.25} = 0.333... $, i.e. for every 3 times the event under scrutiny does not happen, there will be 1 time where it happens or 1 success : 3 failures. . Substituting the $p$ in the odds formula with the $p(x^i)$ from above, we get: . $$ Odds = e^{ beta_{0} + beta_{1}x^{i}_{1} + beta_{2}x^{i}_{2} + beta_{3}x^{i}_{3} + beta_{4}x^{i}_{4}} $$ . or . $$ log(Odds) = beta_{0} + beta_{1}x^{i}_{1} + beta_{2}x^{i}_{2} + beta_{3}x^{i}_{3} + beta_{4}x^{i}_{4} $$ . Thereby we have created a link to our feature space $X$ and we can map any observation of features to a probability $ p in (0,1)$. All we need then is a somewhat arbitrary cutoff rule, usually $p&gt;.5$. . Furthermore, we get interpretability for free: the coefficient $ beta_i$ describes the change in odds when we increase $x_i$ by one unit. Look again at the spaghetti diagram with the colors above. The greater the value of measurement 3 to smaller the chance we have a red specimen at hand. In fact, the $ beta$ coefficient of measurement 3 in our &quot;red vs. rest&quot; problem is $-2.26$ which means that a unit increase of measurement 3 decreases the odds of the specimen being red by $exp(-2.26) approx .104$, which is very roughly 1 in 9.5. . Let us draw this function $p(x)$ to see what it looks like: . import matplotlib.pyplot as plt def map_to_p(log_odds): odds = np.exp(log_odds) p = odds / (1 + odds) return p lots_of_log_odds_for_drawing = np.linspace(-10, 10, num=1000) mapped_to_p = list(map(map_to_p, lots_of_log_odds_for_drawing)) plt.xlabel(&quot;log(Odds)&quot;) plt.ylabel(&quot;p&quot;) plt.plot(lots_of_log_odds_for_drawing, mapped_to_p); . This function is called the sigmoid function, which - in terms of our logistic regression model - is the so called link function, as it links a predictor, the $ log(Odds)$ linear combination, to a response in $p in (0,1)$. . Fitting the Parameters . Fitting the parameters is a bit tricky, as we cannot employ a least squares regression as in a linear case. We have to use numerical methods like the maximum likelihood estimation (MLE). Intuitively we want our $ beta$ in such a way that the linear combinations with the feature vectors $x$ are as far away from the middle of the sigmoid and as far to one side for &quot;successes&quot; (usually the positive) and as far to the other for &quot;failures&quot;. &quot;Success&quot; means being a member of a certain class $i$ and failure means a higher probability for any other class. . This beta, once we have found it, we will call $ hat beta$. . Let&#39;s have a look at the sigmoid again and use the parameters for the class &quot;0 = red&quot; from the sklearn logistic regression to distribute our $x$s: . colors = [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;] # this is the result of b_0 + b_1 * x_1 + b_2 * x_2 + ... # clf is still our trained classifier log_odds_from_our_dataset = [clf.intercept_[0] + x @ clf.coef_[0] for x in X] plt.xlabel(&quot;log(Odds)&quot;) plt.ylabel(&quot;p&quot;) plt.plot(lots_of_log_odds_for_drawing, mapped_to_p) for x, y_cl in zip(log_odds_from_our_dataset, y): #plt.scatter(x, 0, s=10, c=colors[y_cl]) plt.scatter(x, map_to_p(x), s=10, c=colors[y_cl]) plt.show() . We see now that all of the red dots, which represent the members of class $0$ fall on the right side of 0 (and therefore have $p &gt; .5$), and the other two classes fall on the left side. So, basically, we need to choose $ hat beta$ in such a way the sigmoid function is as close to zero for some $x$ and as close to 1 for some other $x$. This is a very awkward problem to solve. Luckily, we are working with the interval of $p in (0,1)$, which means we know that the maximum is 1. So, we can flip those $x$ that are supposed to result in a value of $p$ close to 0 around by calculating $(1-p)$. Now we have a maximization problem for all $x$ in our domain. . Also, a word on the intercept: while $ beta_1 .. beta_4$ essentially &quot;stretch&quot; our $x$ out in such a way that there is as little overlap between the groups as possible, $ beta_0$ changes the position of the whole set of dots, so that they can be nicely centered around 0. We do actually not need to give special treatment to the intercept, as we can just augment our feature vector $x$ with a static 1 like so $x^i = (1, x^{i}_{1}, x^{i}_{2},x^{i}_{3},x^{i}_{4})$. Using these augmented vectors in the following steps, will simplify things a lot. . Furthermore, as we want to maximize all our individual $p$ and $1-p$ respectively, we can also try to maximize the product of all of them and as we want all the terms of the product to be as close to 1 as possible, that means we want the whole product to be as close to 1 as possible. . In our input data, the class a certain specimen belongs to is denoted by $y_i in {0,1,2 }$ with these numbers representing red, green and blue respectively. Now if we want to translate that into a binary problem, we need a $y_{binary, i} in {1,0 }$, were e.g. for the first classification &quot;red vs. non-red&quot; we denote a success with &quot;1&quot; (flower is red) vs. &quot;0&quot; (flower is not red). I will omit the &quot;binary&quot; for brevity&#39;s sake, but please make a big mental note that the $y_i$s we are working with from now on are not the same ones as above any more. . In mathematical terms, we want to find our estimator $ hat beta$ that maximizes the likelihood function: . $$ l( beta) = prod_{x_i; y_i = 1} p(x_i) times prod_{x_i; y_i = 0} (1-p(x_i)) $$ . or: . $$ hat beta = arg max_{ beta} l( beta) $$ . Which can then be simplified as follows: . $$ l( beta) = prod_{i} p(x_i)^{y_i} (1-p(x_i))^{1-y_i}$$ . We need to introduce the next concept now - in order to tackle this maximization problem, we use the fact that $log(a)$ is increasing monotonically with $a$ so maximizing $log(a)$ is equivalent to maximizing $a$, therefore: . $$ hat beta = arg max_{ beta} l( beta) iff hat beta = arg max_{ beta} log l( beta) $$ . and using this property, we can transform the multiplication in $l( beta)$ to a summation in $ log l( beta)$ or $ll( beta)$ for short. . $$ ll( beta) = sum_{i} y_i log(p(x^i)) + (1-y_i) log(1-p(x^i))$$ . We will now simplify this equation further and we will start with the log probability in the first term (blue highlighting will become clear further down the line): . $$ y_i log(p(x^i)) = y_i log frac{1}{1+e^{- beta x^i}} = color{blue}{-y_i log(1+e^{- beta x^i})} $$ . While that was fairly simple, the second term is a bit more challenging. For now, we will omit the term $(1-y_i)$ and focus on the log inverse probability in the second term: . $$ log(1-p(x^i)) = log(1- frac{1}{1+e^{- beta x^i}}) = log( frac{e^{- beta x^i}}{1+e^{- beta x^i}})$$ . To proceed, we have to make a choice what to do with the term in the brackets: we can either take the $e^{- beta x_i}$ in the numerator and bring it down into the denominator or use the log rule for fractions to separate the fraction into a subtraction of two fractionless logarithms. It turns out, we need to do both in order to get to the simplest possible form of whole combined equation and after distributing the terms from the $(1-y_i)$ we need to treat each with a different strategy. For the $1$ term (note that the minus sign in the denominator has disappeared): . $$ 1 log( frac{e^{- beta x^i}}{1+e^{- beta x^i}}) = log( frac{1}{e^{ beta x^i}(1+e^{- beta x^i})}) = log( frac{1}{e^{ beta x^i}+1}) = color{green}{ - log(e^{ beta x^i}+1)} $$ . and for the $-y_i$ term: . $$ -y_i log( frac{e^{- beta x^i}}{1+e^{- beta x^i}}) = -y_i ( log(e^{- beta x^i}) - log(1+e^{- beta x^i})) $$ . $$ = color{green}{ -y_i(- beta x^i)} color{blue}{ + y_i log(1+e^{- beta x^i})} $$ . When we now reassemble the puzzle pieces, the blue terms cancel each other out and the green terms are left . $$ ll( beta) = sum_{i} color{blue}{-y_i log(1+e^{- beta x^i})} color{green}{ - log(e^{ beta x^i}+1)} color{green}{ -y_i(- beta x^i)} color{blue}{ + y_i log(1+e^{- beta x^i})} $$ . $$ = sum_{i} y_i beta x^i- log(e^{ beta x^i}+1) $$ . To jog our memory: . $y_i in {1,0 }$, representing that an $x^i$ belongs to a certain class with 1 or not with 0 | $x^i in mathbb{R}^5$ with the first element set fixed to $1$, the feature vector of an observation | $ beta in mathbb{R}^5$ the vector of coefficients, with the first entry representing the intercept | . We have now managed to state our optimization problem in comparatively simple terms, as the only thing that is missing now is the $ beta$ that will maximize the last expression above, but all the other variables are clearly defined. We cannot compute the optimal $ beta$ algebraically though and have to rely on numerical methods. . A Naive Example . In order to prepare for the next steps of actually fitting the $ beta$ coefficients, we need translate the theoretical maths into python code. Also translating the three class problem of red, green and blue flowers into multiple binary problems like &quot;flower is red vs. flower is not red&quot; is necessary. First the translation of the log-likelihood function. . def log_likelihood(x, y, beta): ll = 0 for x_el, y_el in zip(x, y): ll += y_el * (beta @ x_el) - np.log(np.exp(beta @ x_el) + 1) return ll . We split the three class problem into 3 binary sub-problems, so we need to modify the class vector in such a way that $y$ only has &quot;1&quot; entries for that single class we are testing for and &quot;0&quot; entries for the other two classes: . y_binary_red = [1 if y_el == True else 0 for y_el in y == 0] y_binary_green = [1 if y_el == True else 0 for y_el in y == 1] y_binary_blue = [1 if y_el == True else 0 for y_el in y == 2] . Furthermore, we need to add a &quot;1&quot; in the beginning of each feature vector $x$, in order to account for the intercept. . # make a vector with just &quot;1&quot;s and glue it to the left side of X X_with_intercept = np.hstack((np.ones((X.shape[0], 1)), X)) . Now we leave the cosy realm of algebraic certainty and need to employ numerical methods to get our estimate $ hat beta$. But before we do that, let us see what we want to accomplish in principle by using a brute force algorithm. We start with random $ beta$ - I cheated here as I already roughly know in which area to find the variables. Given this semi-random beta, we calculate the log-likelihood function, which can assume values between $(- infty, 0)$ as it is a $log$ of a probability $p in (0,1)$. If the random $ beta$ increases our likelihood we keep it, otherwise we throw it out and choose another random $ beta$. In order to visualize the results, we program a little helper function first. . # define a plot function to visualize the result def plot_separation(x, beta, y, color=&#39;red&#39;): color = [&#39;grey&#39;, color] for x_el, y_el in zip(x, y): log_odds = beta @ x_el plt.scatter(log_odds, map_to_p(beta @ x_el), c=color[y_el]) plt.plot(lots_of_log_odds_for_drawing, mapped_to_p) plt.show() . # choose a random, but very small likelihood as basis ll_hat = -1000000 for step in range(10001): # choose &quot;random&quot; beta vector 10.000 times # each entry will be between -3 and 3 beta_random = 6 * np.random.random(5) - 3 ll = log_likelihood(X_with_intercept, y_binary_red, beta_random) # if our log-likelihood has improved, overwrite old beta, save likelihood for futher iterations if ll &gt; ll_hat: beta_hat = beta_random ll_hat = ll # draw the result every 5000 steps if step % 5000 == 0: print(&quot;Step:&quot;, step, &quot;, beta_hat:&quot;, beta_hat, &quot;, ll_hat:&quot;, ll_hat) plot_separation(X_with_intercept, beta_hat, y_binary_red) print() . Step: 0 , beta_hat: [-0.15169337 1.40168034 -2.16626949 -1.47939024 -1.21468793] , ll_hat: -154.31046117175367 . Step: 5000 , beta_hat: [ 0.78059886 0.09592393 2.75367263 -2.91886356 -2.25203117] , ll_hat: -0.4374415812923591 . Step: 10000 , beta_hat: [ 0.78059886 0.09592393 2.75367263 -2.91886356 -2.25203117] , ll_hat: -0.4374415812923591 . . As we can see, this neatly separates the red from the non-red dots onto the &quot;1&quot; and &quot;0&quot; side of the sigmoid curve. To be honest, that seems to work only because the red specimen are somewhat neatly separated from the rest from the get go. If you try the other colors, the results will not be that good. . But we have seen the general principle. By wildly choosing random $ beta$s and keeping the ones that increase likelihood, we push the log odds of our &quot;success&quot; class as far to the right as possible, while we keep the &quot;failures&quot; on the left. Now the only step that is left is transitioning from random guessing into a process that is more sophisticated. . Fitting the Parameters with Gradient Descent . As a next step, we will replace the brute force method with a numerical optimization method like Gradient Descent or Newton-Raphson. Today, we are going to use the Gradient Descent method. In simple terms, we will move $ beta$ in small steps towards the direction that minimizes our error function, which is the true $y$ minus our calculated result for $y$ under our guess for $ beta$. This direction happens to be the negative gradient. The true $y$ is simply our input data for $y$. . def gradient_descent(X, y, steps, learning_rate): beta = np.zeros(X.shape[1]) for _ in range(1, steps): # calculate log odds for all x in X at once log_odds = np.dot(X, beta) # calculate result based on current beta tentative_y = list(map(map_to_p, log_odds)) # calculate difference between current estimate and truth error = np.subtract(y, tentative_y) # see below for explanation gradient = np.dot(error, X) # move beta in opposite direction from error beta += learning_rate * gradient return beta . So what is the gradient of $ll( beta) $ with regards to $ beta$? . $$ nabla_{ beta} ll( beta) = sum_{i} nabla_{ beta} y_i beta x_i- nabla_{ beta} log(e^{ beta x_i}+1) $$ . $$ = sum_{i} y_i x_i- nabla_{ beta} log(e^{ beta x_i}+1) $$ . $$ = sum_{i} y_i x_i - x_i e^{ beta x_i} frac{1}{e^{ beta x_i} +1} $$ . $$ = sum_{i} y_i x_i - x_i frac{1}{1 + e^{- beta x_i}} $$ . $$ = sum_{i} y_i x_i - x_i p(x_i) = sum_{i} (y_i -p(x_i)) x_i $$ . Which is nothing else than the true $y_i$ minus the calculated approximation for $y_i$ which is $p(x_i)$ times the feature vector for each observation, or in matrix form: . $$ nabla_{ beta} ll( beta) = (y_{true} - y_{estimate}( beta))X $$ . To mix things up a bit, let us try using the gradient descent method to identify the blue specimen instead of the red ones: . beta_hat = gradient_descent(X_with_intercept, y_binary_blue, steps=100001, learning_rate=10e-5) . plot_separation(X_with_intercept, beta_hat, y_binary_blue, color=&#39;blue&#39;) . And there we have a separation of the blue dots towards the positive real numbers and the rest towards the negative ones and their respective probabilities going to 1 and 0. As we can see though, the separation does not work as well as in the &quot;red vs. rest&quot; problem. . However, if we try the same with the green specimen, it does not work very well at all. But that was somewhat expected, as we have seen in the very beginning. We should take solace in the fact, that the sklearn implementation does also not fare very well, which can be seen in the confusion matrix, and if we plot the result for the sub-problem &quot;green vs. rest&quot; we can barely differentiate between the green and the grey dots. . # parameters for green from sklearn log-reg beta_0 = np.hstack((clf.__dict__[&quot;intercept_&quot;][0], clf.__dict__[&quot;coef_&quot;][0])) # red beta_1 = np.hstack((clf.__dict__[&quot;intercept_&quot;][1], clf.__dict__[&quot;coef_&quot;][1])) # green beta_2 = np.hstack((clf.__dict__[&quot;intercept_&quot;][2], clf.__dict__[&quot;coef_&quot;][2])) # blue . plot_separation(X_with_intercept, beta_0, y_binary_red, color=&#39;red&#39;) . plot_separation(X_with_intercept, beta_1, y_binary_green, color=&#39;green&#39;) . plot_separation(X_with_intercept, beta_2, y_binary_blue, color=&#39;blue&#39;) . Finally, we can observe that while our solution is not as good as the version implemented in sklearn, it provides results which are quite close already. One difference between the two algorithms is that sklearn penalizes solutions with large coefficients in its optimizer. . Sources: . https://beckernick.github.io/logistic-regression-from-scratch/ | https://www.youtube.com/watch?v=YMJtsYIp4kg | .",
            "url": "http://blog.huppertz-consulting.de/classification/theory/2020/08/25/logistic-regression-in-depth.html",
            "relUrl": "/classification/theory/2020/08/25/logistic-regression-in-depth.html",
            "date": " • Aug 25, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Contents of a Data Science Project Contract",
            "content": "When you are facing your first data science project and the accompanying negotiation of a project contract, it can be a little bit overwhelming and it’s easy to forget to pen down some important issues. In this article I will provide a list of some of these issues that should be defined in a data science project contract - and a few things that should not. . Form and Content of the Result . To manage expectations on the client side, define the deliverables precisely, such as . the goal, e.g. identify factors on the website that influence the average shopping cart volume | decide if any of the 5 marketing campaigns of last year had a measurable impact on the revenue and if so, how much | analyze if the customer retention activities in the call center are generating positive returns | … | . | the format Powerpoint | Word | notebook | Airflow workflow | key-ready application | daily mail reports | … | . | the content one off analysis | model for continuous evaluation | table or visualization | … | . | the timeframe of data to be analyzed and - if applicable - the timeframes for forecasts . | the timing for delivery a.k.a. the deadline | . Wrangling the input data . This is one of the trickiest ones, because part of why you are negotiating a data science project right now might be that your client does not have a very good overview over his own data landscape. But to the extent of your client’s knowledge do define the rough size and shape of the data provided to you and their accessibility: . millions of rows | GB size | remote access or on site only | DB/DWH downtimes | ETL processes blocking you from access (my last project DWH was down for one hour after lunch break for ETL) | … | . Also, include in your contract a passage that allows you to re-negotiate or end the contract prematurely after having an in-depth look at the data. Sometimes it turns out the data is not what is was expected to be in quality and/or size and this might make your project completely infeasible. . You will be paid for your time, not the impact of your results . There is no guarantee that you will find any result of substance and economical impact in an exploratory data science project. If you want to explore, e.g., which factors influence the shopping cart value of a customer, it is entirely possible that the input data set does not contain any answers to your question. Any impactful factors that you do happen to identify might already be at their optimum or cannot be adjusted for reasons outside of the scope of your analysis. Mention in your contract that you do not owe any specific result and that your compensation is for the time and effort of the process, not the efficacy of your results. . Even better: if you have the necessary experience, discuss which possible courses of action your client can pursue before you start your project. E.g. if your analysis shows that older customers spend more money, can your client run TV and Print ads, if it shows that younger customers spend more, can the client’s organization shoulder Instagram marketing or can such expertise be bought. Admitted, that’s a bit cliché, but you get the gist. Without any realistic options to act upon your analysis, the whole premise is moot. . Do not present preliminary results, avoid presenting your results many times over . Do not agree on meetings to present intermediate results without giving great thought to it. Preparing a shiny presentation to placate a sceptical customer after a week or two of work will take focus away from your final result. Preparing the presentation of the result can take between 30 - 50 % of the overall man hours of a project, so if you want to present results after two weeks, you will barely have a week to do the actual work, if at all. Also this can backfire if you decide to pursue a different angle to solve a problem after presenting preliminary results on a different route. . Oftentimes, the client simply wants to be kept in the loop to avoid wasting the money they invest in you. You can offer to communicate regular status updates via Email or chat and you should keep those memos neutral in terms of results to avoid eating your words later, e.g. write “Explored - among other variables - the customers age in relation to the revenue” instead of “Looks like older customer spend more money in your shop”. . Once you find interesting results worthy of presentation, there might be a cascade of “oh, the Head of Marketing needs to see that”, “oh, the COO needs to see that”, “oh, the CEO needs to see that”. This of course leads to you spending more time than expected on the project, because you do not present the results once but 3 or 4 times. Stating in your contract that the results will be presented once and once only will give you leverage, should the client ask you to do more than one. . Access to subject matter experts . While I suggest keeping the C-Level of your client on a “need to know”-basis, I do recommend to keep in close contact with the client’s side subject matter experts as you will have questions that need answers. Everyone needs a vacation once in a while, though. It is most inconvenient if your contacts within the client’s organization are taking that long overdue holiday in the middle of your project. So, make sure you know when they will be present and that they are not constantly beleagured with meetings, but will actually be available to discuss your ideas and findings. . Hardware provided or BYOD . If your client is providing you with hardware and you don’t bring your own device, make sure it’s fast and has sufficient RAM. The minimum nowadays should be 16 GB RAM for batch processing of any significant amount of data. Of course the price of RAM is neglegible compared to the overall budget of the average project and enough RAM to avoid swapping to disk can save you hours if not days over the course of a larger project. . Third Party Tools . If you want to rely on a 3rd party tools - especially when data is uploaded to their servers - make sure you are allowed to do so and it’s within the infosec guidelines of your client. . Minimize PII . If anyhow possible do not accept any personally identifiable information. You don’t want to be involved if at any point there is a data breach and customer information get’s leaked. . Checklist . Add a disclaimer about the exploratory nature of the data science project if applicable | Define a predetermined breaking point after initial look into the data | Not agreed on any preliminary result meetings | Form and content of the final result is defined | Possible analysis outcomes and actions to take upon them have been discussed | How many times the results will be presented is defined | Your compensation for your efforts | Which hardware will be provided (if applicable) | Define which third party tools you want to use and get them signed off by infosec | Define which PII is excluded from the data provided to you | Document size, shape, and scope of data that will be provided to you | Access to subject matter experts is defined and any planned absences have been communicated | .",
            "url": "http://blog.huppertz-consulting.de/project%20management/2020/08/04/data-science-contract-contents.html",
            "relUrl": "/project%20management/2020/08/04/data-science-contract-contents.html",
            "date": " • Aug 4, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Creating a tool for manual testing in Django",
            "content": "Automated testing is still not as prevalent in software development as one would wish. Testing is apparently very expensive, but experience tells us that the absence of testing is oftentimes even more expensive. Unfortunately, budget deciders do not share these experiences with software developers and therefore decide against the implementation of automatic testing in a project. . The project I was working on as as project manager had exactly this setup. Manually testing the software artifacts produced by the devs was one of my jobs as was the documentation of the testing results. . This was obivously not an optimal solution due to various reasons: . as PM I was fairly expensive, junior members would have been more suitable to conduct the user tests | as the software framework was new to the dev team, a lot of functions had to be refactored after their first implementation, which led to features breaking again and again | Communication overhead for all the issues slowed down the development of new features | Just using issues in JIRA was impractical as the constant regression through refactoring would have meant we could never close any issues and user stories and the information fields required for the development workflow were not the same as for the testing workflow | . So in order to remedy this problems I needed an application where I could: . delegate the testing process to junior team members | repeatedly test the same issues again and again | document progression and regression permanently in oredr to provide feedback to the dev team | ensure sufficient test coverage and records about how recently a certain feature was tested | . I decided to create a Django app that would help me realise these goals. . Features . We start with an overview page listing all the issues and their status as well as an aggregate overview over all existing issues faceted according to their priority. . Also in the header there are buttons allowing to choose issues to be tested at random. . . Homepage - high level overview and list of all issues | . When adding a new issue, it is possible to set a priority and an artifact the issue relates to. Artifacts are configurable in the admin backend of Django. . Images that help to describe the issue can be attached and one or multiple sets of credentials like test user account access can be attached. . . Create new issue | . Once an issue has been created it is possible to add comments, images and set a status of the issue (“okay”, “unsure”, “cannot test”, “not working”). The history of status changes will be recorded on the right so it is easy to visually identify how stable an issue is and how carefully it needs to be tested again. . . Issue and testing status | . A simple faceted search is pretty much included OOTB with Django and just need a little bit of configuration. . . Simple search function | . A lot of communication overhead goes into sharing credentials for testing, such a login data for the admin backend or test user credentials. So in order to simplify this process, all credentials that can be shared among the testing team without compromising the security of any live environment can be collected and shared on an overview page as well as linked to the specific issues the are connected with. . . Credentials overview | . Last but not least, all comments attached to issues are chronolgically displayed on the comments overview page to gather the most recent feedback on issues for the project manager and allows for this feedback to be realyed to the dev team. . . Comments section | . So with this tool I could delegate testing tasks to junior team members through the Django account management, record a history of system stability, had a srtem of new input generated by the testers for the dev team and could ensure a constant and evenly distributed coverage across all test cases. .",
            "url": "http://blog.huppertz-consulting.de/tooling/python/django/testing/2020/05/24/manual-testing-manager-app.html",
            "relUrl": "/tooling/python/django/testing/2020/05/24/manual-testing-manager-app.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": " Forecasting Revenue with fbprophet",
            "content": "In this post, we will explore the basic functionality of the fbprophet package and how it can help us to quickly forecast data that is seasonal with an underlying trend. Specifically, we will try to forecast revenue data about two years into the &quot;future&quot; and then we will compare de facto data during these two years. Everything will be based upon revenues from 2012 to October 2017, so we have plenty of data to work with. . Later, we will also explore some of the more advanced features of fbprophet like adding our own regressors (e.g. the weather or marketing spend), logistic growth and differentiating between linear (i.e. additive) and multiplicative seasonal influence. . . Tip: When I tried to install fbprophet there was an error that certain dependencies could not be build. I little research revealed that this has to do with the pystan version I was using. After downgrading pystan to version 2.17 everything worked like a charm. . Let us start by important the basic necessities for our experiment. . import pandas as pd from fbprophet import Prophet import matplotlib.pyplot as plt %matplotlib inline pd.set_option(&quot;display.precision&quot;, 3) . We will choose a testing_cutoff_data before which we will take the data into consideration for training our model and after which we will use the data for evaluating the accuracy of our model. . # only use training data before this year, then compare with data from this year on testing_cutoff_date = &quot;2016&quot; # we can calculate everything based upon daily, weekly or monthly data sampling_frequency = &quot;W&quot; . Additionally, there is a naming convention that fbprohet uses. It requires the time series columns to be labelled ds for the dates and y for the values to be forecast. I pre-formatted the data acordingly, but keep that in mind, when using your own data. . df = pd.read_csv(&quot;daily_revenue.csv&quot;, sep=&quot;,&quot;, decimal=&quot;-&quot;, encoding=&quot;utf-8&quot;) df[&quot;ds&quot;] = pd.to_datetime(df[&quot;ds&quot;]) # split dataset into train and test timeframe df_train = df[df[&quot;ds&quot;] &lt; testing_cutoff_date] df_test = df[df[&quot;ds&quot;] &gt;= testing_cutoff_date] . This gives us dataframes containing the daily revenue, one for the training period and one for the testing period. But for now we will work on weekly data, not daily, as there is a lot of noise and there is really no need to forecast the revenue for a very specific date sometime next year. So we use the resampling function of pandas to aggregate the data on a weekly basis. . df_train_resampled = df_train.set_index(&quot;ds&quot;).resample( &quot;1&quot; + sampling_frequency).sum()[&quot;y&quot;] df_train_resampled = df_train_resampled.reset_index() . df_test_resampled = df_test.set_index(&quot;ds&quot;).resample( &quot;1&quot; + sampling_frequency).sum()[&quot;y&quot;] df_test_resampled = df_test_resampled.reset_index() . Now lets have a look how the whole dataset looks like: . fig, ax = plt.subplots() df_train_resampled.set_index(&quot;ds&quot;).plot(ax=ax) df_test_resampled.set_index(&quot;ds&quot;).plot(ax=ax) ax.legend([&quot;train&quot;, &quot;test&quot;]) plt.show() . There are some apparent challenges our algorithm will have to master: . the trend seems to be approximately linear up until the end of 2015 and but then changes | especially in 2017 there has been an uptick in marketing spending in the company that increased the slope beyond the linear trend in the years before | around christmas and new year&#39;s there is always a sharp decline in revenue | . Training . fbprophet can take holidays into consideration and will fit these as special dates. Later on we can evaluate how these holidays affect the revenue. . m = Prophet() m.add_country_holidays(country_name=&quot;DE&quot;) m.fit(df_train_resampled) if sampling_frequency == &quot;D&quot;: future = m.make_future_dataframe(periods=730, freq=sampling_frequency) elif sampling_frequency == &quot;W&quot;: future = m.make_future_dataframe(periods=104, freq=sampling_frequency) elif sampling_frequency == &quot;M&quot;: future = m.make_future_dataframe(periods=24, freq=sampling_frequency) else: print(&quot;No valid samplig frequency given!&quot;) forecast = m.predict(future) . INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this. INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . When we are using weekly or monthly aggregated data, fbprophet seems to ignore the holidays when we work on aggregated weekly data though, as the following line should produce all forecast rows that have a holiday effect. I haven&#39;t figured out yet how to mark &quot;weeks that contain holidays&quot;. . forecast[forecast[&quot;holidays&quot;] != 0] . ds trend yhat_lower yhat_upper trend_lower trend_upper Christi Himmelfahrt Christi Himmelfahrt_lower Christi Himmelfahrt_upper Erster Mai ... holidays holidays_lower holidays_upper yearly yearly_lower yearly_upper multiplicative_terms multiplicative_terms_lower multiplicative_terms_upper yhat . 0 rows × 46 columns . fig = m.plot(forecast) . fig = m.plot_components(forecast) . Testing . Now that we have a forecast for two years after the last date in the training data set, we can compare this with the actual data from that period of time: . fig, ax = plt.subplots(figsize=(10, 10)) forecast.set_index(&quot;ds&quot;)[&quot;yhat&quot;].plot(ax=ax) df_test_for_plotting = df_test.set_index(&quot;ds&quot;).resample( &quot;1&quot; + sampling_frequency).sum() df_test_for_plotting.plot(ax=ax) ax.legend([&quot;forecast&quot;, &quot;true values&quot;]) . &lt;matplotlib.legend.Legend at 0x1695bf0b160&gt; . Major differences are visible here, especially one the massive revenue decrease during the last days of every year. Nevertheless, out model somewhat follows the shape of the de facto revenue during the testing period. In order to quantify the error in forecasting, we will relate the error to the actual revenue in the given years. . df_for_accuracy = df_test_for_plotting.join(forecast.set_index(&quot;ds&quot;), how=&quot;left&quot;).reset_index() df_for_accuracy_2016 = df_for_accuracy[df_for_accuracy[&quot;ds&quot;] &lt; &quot;2017&quot;] df_for_accuracy_2017 = df_for_accuracy[df_for_accuracy[&quot;ds&quot;] &gt;= &quot;2017&quot;] . df_for_accuracy_2016[&quot;abs. difference&quot;] = df_for_accuracy_2016[ &quot;y&quot;] - df_for_accuracy_2016[&quot;yhat&quot;] df_for_accuracy_2017[&quot;abs. difference&quot;] = df_for_accuracy_2017[ &quot;y&quot;] - df_for_accuracy_2017[&quot;yhat&quot;] . error_2016 = df_for_accuracy_2016[&quot;abs. difference&quot;].sum() / df_for_accuracy_2016[&quot;y&quot;].sum() print(&quot;The relative error in the forecast revenue in 2016 is %.3f.&quot; % error_2016) . The relative error in the forecast revenue in 2016 is 0.002. . error_2017 = df_for_accuracy_2017[&quot;abs. difference&quot;].sum() / df_for_accuracy_2017[&quot;y&quot;].sum() print(&quot;The relative error in the forecast revenue in 2017 is %.3f.&quot; % error_2017) . The relative error in the forecast revenue in 2017 is 0.061. . It turns out, we have a pretty solid estimate for the revenues in the two forecasted years with a relative error of 0.2% and 6% respectively. .",
            "url": "http://blog.huppertz-consulting.de/time%20series/fbprophet/2020/05/22/revenue-forecasting-fbprophet.html",
            "relUrl": "/time%20series/fbprophet/2020/05/22/revenue-forecasting-fbprophet.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Simple Visualizer for Teradata/T-SQL Queries",
            "content": "A Simple Visualizer for T-SQL scripts . A while ago, I was working on a project where the client used an expensive high-performance database cluster in order to do demand forecasting for a German supermarket chain. . What the client did not have: . Automated testing | CD/CI-Pipelines | Coding guidelines | Documentation | . What the client did have however, was a codebase of about 30,000 lines of pure T-SQL code and a team that learned “on the job”, i.e. had never written a functioning application in this SQL dialect. This resulted in a about 10 scripts of varying length and written in different styles by different people, most of whom had left the project in frustration already by the time I joined. . Combine that with requirements which were subject to change on daily (sometimes hourly) basis, arbitrary restrictions from operations (indexes take up too much space) and being about 100% over budget, it was a recipe for disaster. . Debugging could mean scrolling through a single or multiple scripts each consisting of 3,000 lines of code and dozens of SQL statements and manually recalculating steps along the way. This would of course take hours within which priorities of implementation could (and frequently did) change again. . I was in desperate need for some kind of support and wanted to use queryscope to get at least a graphical representation of the queries in question but of course the source code was property of the client and I was not allowed to submit it to an unvetted, unapproved vendor for analysis. . So, I took to recreating parts of queryscope’s features from scratch in python to keep all source on premise, the result of which you can see here: . Running the script will yield results like so: . . Output of the query below | . Caveat: This code was only tested with the very specific way the developers on this project wrote their SQL queries, i.e. using many a CTE to create complex temporary tables and then persisting them into intermediary tables, a pattern which is replicated with mock statements in the following SQL snippet: . WITH cte_category_counts ( category_id, category_name, product_count ) AS ( SELECT c.category_id, c.category_name, COUNT(p.product_id) FROM production.products p INNER JOIN production.categories c ON c.category_id = p.category_id GROUP BY c.category_id, c.category_name ), cte_category_sales(category_id, sales) AS ( SELECT p.category_id, SUM(i.quantity * i.list_price * (1 - i.discount)) FROM sales.order_items i INNER JOIN production.products p ON p.product_id = i.product_id INNER JOIN sales.orders o ON o.order_id = i.order_id WHERE order_status = 4 -- completed GROUP BY p.category_id ) INSERT INTO categories (category_id, category_name, product_count,sales) SELECT c.category_id, c.category_name, c.product_count, s.sales FROM cte_category_counts c INNER JOIN cte_category_sales s ON s.category_id = c.category_id ORDER BY c.category_name; WITH cte_business_division_counts (division_id, counts) AS ( SELECT d.division_id, SUM(c.counts) FROM categories c INNER JOIN organization.divisions d ON c.category_id = d.category_id GROUP BY d.division_id ) cte_business_division_sales (division_id, division_name, sales) AS ( SELECT d.division_id, d.division_name, SUM(c.sales) FROM categories c INNER JOIN organization.divisions d ON c.category_id = d.category_id GROUP BY d.division_id ) INSERT INTO business_divisions (division_id, division_name, product_count,sales) SELECT s.division_id, s.division_name, s.sales, c.counts FROM cte_business_division_counts c INNER JOIN cte_business_division_sales s ON s.division_id = c.division_id ORDER BY c.division_name; . And this here is the source code for the actual visualizer: . import re import os from collections import Counter from graphviz import Digraph import networkx as nx # can be used to generate JSON format of graph # from networkx.readwrite import json_graph basedir = &#39;.&#39; filenames = [&#39;test.sql&#39;] with_deletes = False dot = Digraph(comment=&#39;Structure&#39;, engine=&#39;dot&#39;, strict=True) G = nx.Graph() # graphically highlight tables and their incoming connections highlight_nodes = [] # do not draw these ignore_list = [] # collect all nodes here complete_list = [] for filename in filenames: f = open(os.path.join(basedir, filename)) f_string = f.read() sep = &#39;;&#39; f_list = [x+sep for x in f_string.split(sep)] f_list = [x.replace(&quot; n&quot;, &quot; &quot;) for x in f_list] f_list = [x for x in f_list if x.strip() != &quot;COMMIT;&quot;] complete_list += f_list # define regular expressions for SQL statements re_create = re.compile( r&quot;CREATE s+(?:MULTISET)? s+TABLE s+([a-zA-Z0-9_ .]+) s*,&quot;, re.MULTILINE | re.IGNORECASE) re_insert = re.compile( r&quot;INSERT s+INTO s+([a-zA-Z0-9_ .]+) s* (&quot;, re.MULTILINE | re.IGNORECASE) re_from = re.compile( r&quot;(?&lt;!DELETE) s+FROM s+([a-zA-Z0-9_ .]+)[; s]+&quot;, re.S | re.I) re_cte = re.compile( r&quot;(?:WITH)? s+([A-Za-z0-9_]+) s+AS s? (&quot;, re.MULTILINE | re.IGNORECASE) re_join = re.compile( r&quot;JOIN s+([A-Za-z0-9_ .]+) s&quot;, re.MULTILINE | re.IGNORECASE) re_update = re.compile( r&quot;UPDATE s([ w])+ sSET s[ w , &#39; =_]+&quot;, re.MULTILINE | re.IGNORECASE) re_delete = re.compile( r&quot;DELETE sFROM s([ d w . &#39; =_]+.*;)&quot;, re.S | re.IGNORECASE) node_list = [] delete_nodes = [] create_nodes = [] # go through all statements and check if they match a regex for i, statement in enumerate(complete_list): statement = statement.replace(&quot; n&quot;, &quot; &quot;) to_nodes = [] from_nodes = [] for match in re.findall(re_create, statement): create_nodes.append(match) for match in re.findall(re_delete, statement): delete_nodes.append(match) for match in set(re.findall(re_insert, statement)): to_nodes.append(str.lower(match)) for match in set(re.findall(re_update, statement)): print(match) for match, count in Counter(re.findall(re_cte, statement)).items(): print(&quot;%s : %i&quot; % (match, count)) for match, count in Counter(re.findall(re_from, statement)).items(): if match not in re.findall(re_cte, statement): from_nodes.append(str.lower(match)) # print(5*&#39;-&#39; + &#39;Joins (CTEs removed)&#39; + 5*&#39;-&#39;) for match, count in Counter(re.findall(re_join, statement)).items(): if match not in re.findall(re_cte, f_string): # print(&quot;%s : %i&quot; % (match,count)) from_nodes.append(str.lower(match)) from_nodes = [x for x in from_nodes if x not in ignore_list] to_nodes = [x for x in to_nodes if x not in ignore_list] for to_node in to_nodes: # for every node switch between picking colors from the &quot;left&quot; and &quot;right&quot; end # the spectrum so similar colors do not appear next to each other if i % 2 == 0: edge_color = str(round(i/float(len(complete_list)), 2))+&quot; 1.0 &quot; + str(round(i/float(len(complete_list)), 2)) else: edge_color = str(round((len(complete_list)-i)/float(len(complete_list)), 2)) + &quot; 1.0 &quot; + str(round((len(complete_list)-i)/float(len(complete_list)), 2)) if to_node in highlight_nodes: dot.node(to_node, color=edge_color, style=&#39;filled&#39;, fillcolor=edge_color) else: dot.node(to_node, color=edge_color) for from_node in from_nodes: dot.node(from_node, shape=&#39;box&#39;) if (to_node in highlight_nodes) or (from_node in highlight_nodes): dot.edge(from_node, to_node, color=edge_color, penwidth=&#39;3&#39;) else: dot.edge(from_node, to_node, color=edge_color) # graphically represent deletes within the query if with_deletes: delete_nodes = [str.lower(del_node) for del_node in delete_nodes] delete_label = &#39;&lt;&lt;B&gt;DELETES&lt;/B&gt;&lt;br/&gt;&#39; + &#39;&lt;br/&gt;&#39;.join(delete_nodes) + &#39;&gt;&#39; dot.node(&#39;DELETES&#39;, label=delete_label, shape=&#39;box&#39;) # print(5*&#39;-&#39; + &#39;All participating Tables&#39; + 5*&#39;-&#39;) # for match in set(re.findall(re_from,f_string)+re.findall(re_insert,f_string)+re.findall(re_join,f_string)): # if match not in re.findall(re_cte,f_string): # print (match) dot.render(&#39;output/&#39;+filename.replace(&#39;.&#39;, &#39;_&#39;)+&#39;.gv&#39;, view=True) # print(dot.source) # dot_graph = G.read_dot() # print (json_graph.dumps(dot_graph)) . As you can see, there are also ways to highlight certain nodes, as well as omitting them from the output. There is also an automatic random assignment of a color to any persistent table and all edges going into the respective node are also color-coded accordingly. Nodes that have no downstream dependents within the query are displayed as an ellipse as opposed to a rectangle. . This whole thing is a quick draft that took me about 4-5 hours to write, but it saved me countless hours in debugging. . This script could also be integrated into a CD/CI-pipeline in order to create a constantly updated visual documentation of the project. .",
            "url": "http://blog.huppertz-consulting.de/tooling/python/2020/05/20/sql-visualizer.html",
            "relUrl": "/tooling/python/2020/05/20/sql-visualizer.html",
            "date": " • May 20, 2020"
        }
        
    
  
    
  
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Manuel Huppertz and I have a passion for data, maths and building the right tools for the job. On these pages I will take you on a tour through some of the things I created in the last few years and some of my musings and experiments in the data science field. . I work as a freelance consultant and you can hire me if you need an IT project manager, digital marketing specialist or data scientist. . If you want to contact me, check out my main page. .",
          "url": "http://blog.huppertz-consulting.de/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://blog.huppertz-consulting.de/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}