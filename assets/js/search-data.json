{
  
    
        "post0": {
            "title": "Creating a tool for manual testing in Django",
            "content": "Automated testing is still not as prevalent in software development as one would wish. Testing is apparently very expensive, but experience tells us that the absence of testing is oftentimes even more expensive. Unfortunately, budget deciders do not share these experiences with software developers and therefore decide against the implementation of automatic testing in a project. . The project I was working on as as project manager had exactly this setup. Manually testing the software artifacts produced by the devs was one of my jobs as was the documentation of the testing results. . This was obivously not an optimal solution due to various reasons: . as PM I was fairly expensive, junior members would have been more suitable to conduct the user tests | as the software framework was new to the dev team, a lot of functions had to be refactored after their first implementation, which led to features breaking again and again | Communication overhead for all the issues slowed down the development of new features | Just using issues in JIRA was impractical as the constant regression through refactoring would have meant we could never close any issues and user stories and the information fields required for the development workflow were not the same as for the testing workflow | . So in order to remedy this problems I needed an application where I could: . delegate the testing process to junior team members | repeatedly test the same issues again and again | document progression and regression permanently in oredr to provide feedback to the dev team | ensure sufficient test coverage and records about how recently a certain feature was tested | . I decided to create a Django app that would help me realise these goals. . Features . We start with an overview page listing all the issues and their status as well as an aggregate overview over all existing issues faceted according to their priority. . Also in the header there are buttons allowing to choose issues to be tested at random. . . Homepage - high level overview and list of all issues | . When adding a new issue, it is possible to set a priority and an artifact the issue relates to. Artifacts are configurable in the admin backend of Django. . Images that help to describe the issue can be attached and one or multiple sets of credentials like test user account access can be attached. . . Create new issue | . Once an issue has been created it is possible to add comments, images and set a status of the issue (“okay”, “unsure”, “cannot test”, “not working”). The history of status changes will be recorded on the right so it is easy to visually identify how stable an issue is and how carefully it needs to be tested again. . . Issue and testing status | . A simple faceted search is pretty much included OOTB with Django and just need a little bit of configuration. . . Simple search function | . A lot of communication overhead goes into sharing credentials for testing, such a login data for the admin backend or test user credentials. So in order to simplify this process, all credentials that can be shared among the testing team without compromising the security of any live environment can be collected and shared on an overview page as well as linked to the specific issues the are connected with. . . Credentials overview | . Last but not least, all comments attached to issues are chronolgically displayed on the comments overview page to gather the most recent feedback on issues for the project manager and allows for this feedback to be realyed to the dev team. . . Comments section | . So with this tool I could delegate testing tasks to junior team members through the Django account management, record a history of system stability, had a srtem of new input generated by the testers for the dev team and could ensure a constant and evenly distributed coverage across all test cases. .",
            "url": "http://blog.huppertz-consulting.de/tooling/python/django/testing/2020/05/24/manual-testing-manager-app.html",
            "relUrl": "/tooling/python/django/testing/2020/05/24/manual-testing-manager-app.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": " Forecasting Revenue with fbprophet",
            "content": "In this post, we will explore the basic functionality of the fbprophet package and how it can help us to quickly forecast data that is seasonal with an underlying trend. Specifically, we will try to forecast revenue data about two years into the &quot;future&quot; and then we will compare this with the historical data. Everything will bebased upon data from 2012 to October 2017, so we have plenty of data to work with. . Later, we will also explore some of the more advanced features of fbprophet like adding our own regressors (e.g. the weather or marketing spend), logistic growth and differentiating between linear (i.e. additive) and multiplicative seasonal influence. . . Tip: When I tried to install fbprophet there was an error that certain dependencies could not be build. I little research revealed that this has to do with the pystan version I was using. After downgrading pystan to version 2.17 everything worked like a charm. . Let us start by important the basic necessities for our experiment. . import pandas as pd from fbprophet import Prophet import matplotlib.pyplot as plt %matplotlib inline pd.set_option(&quot;display.precision&quot;, 3) . We will choose a testing_cutoff_data before which we will take the data into consideration for training our model and after which we will use the data for evaluating the accuracy of our model. . # only use training data before this year, then compare with data from this year on testing_cutoff_date = &quot;2016&quot; # we can calculate everything based upon daily, weekly or monthly data sampling_frequency = &quot;W&quot; . Additionally, there is a naming convention that fbprohet uses. It requires the time series columns to be labelled ds for the dates and y for the values to be forecast. I pre-formatted the data acordingly, but keep that in mind, when using your own data. . df = pd.read_csv(&quot;daily_revenue.csv&quot;, sep=&quot;,&quot;, decimal=&quot;-&quot;, encoding=&quot;utf-8&quot;) df[&quot;ds&quot;] = pd.to_datetime(df[&quot;ds&quot;]) # split dataset into train and test timeframe df_train = df[df[&quot;ds&quot;] &lt; testing_cutoff_date] df_test = df[df[&quot;ds&quot;] &gt;= testing_cutoff_date] . This gives us dataframes containing the daily revenue, one for the training period and one for the testing period. But for now we will work on weekly data, not daily, as there is a lot of noise and there is really no need to forecast the revenue for a very specific date sometime next year. So we use the resampling function of pandas to aggregate the data on a weekly basis. . df_train_resampled = df_train.set_index(&quot;ds&quot;).resample( &quot;1&quot; + sampling_frequency).sum()[&quot;y&quot;] df_train_resampled = df_train_resampled.reset_index() . df_test_resampled = df_test.set_index(&quot;ds&quot;).resample( &quot;1&quot; + sampling_frequency).sum()[&quot;y&quot;] df_test_resampled = df_test_resampled.reset_index() . Now lets have a look how the whole dataset looks like: . fig, ax = plt.subplots() df_train_resampled.set_index(&quot;ds&quot;).plot(ax=ax) df_test_resampled.set_index(&quot;ds&quot;).plot(ax=ax) ax.legend([&quot;train&quot;, &quot;test&quot;]) plt.show() . There are some apparent challenges our algorithm will have to master: . the trend seems to be approximately linear up until the end of 2015 and but then changes | especially in 2017 there has been an uptick in marketing spending in the company that generated this revenue curve | around christmas and new year&quot;s there is always a sharp decline in revenue | . Training . fbprophet can take holidays into consideration and will fit these as special dates. Later on we can evaluate how these holidays affect the revenue. . m = Prophet() m.add_country_holidays(country_name=&quot;DE&quot;) m.fit(df_train_resampled) if sampling_frequency == &quot;D&quot;: future = m.make_future_dataframe(periods=730, freq=sampling_frequency) elif sampling_frequency == &quot;W&quot;: future = m.make_future_dataframe(periods=104, freq=sampling_frequency) elif sampling_frequency == &quot;M&quot;: future = m.make_future_dataframe(periods=24, freq=sampling_frequency) else: print(&quot;No valid samplig frequency given!&quot;) forecast = m.predict(future) . INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this. INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . When we are using weekly or monthly aggregated data, fbprophet seems to ignore the holidays though, as the following line should produce all forecast rows that have a holiday effect. . forecast[forecast[&quot;holidays&quot;] != 0] . ds trend yhat_lower yhat_upper trend_lower trend_upper Christi Himmelfahrt Christi Himmelfahrt_lower Christi Himmelfahrt_upper Erster Mai ... holidays holidays_lower holidays_upper yearly yearly_lower yearly_upper multiplicative_terms multiplicative_terms_lower multiplicative_terms_upper yhat . 0 rows × 46 columns . fig = m.plot(forecast) . fig = m.plot_components(forecast) . Testing . Now that we have a forecast for two years after the last date in the training data set, we can compare this with the actual data from that period of time: . fig, ax = plt.subplots(figsize=(10, 10)) forecast.set_index(&quot;ds&quot;)[&quot;yhat&quot;].plot(ax=ax) df_test_for_plotting = df_test.set_index(&quot;ds&quot;).resample( &quot;1&quot; + sampling_frequency).sum() df_test_for_plotting.plot(ax=ax) ax.legend([&quot;forecast&quot;, &quot;true values&quot;]) . &lt;matplotlib.legend.Legend at 0x1f134783d30&gt; . Major differences are visible here, especially one the massive revenue decrease during the last days of every year. Nevertheless, out model somewhat follows the shape of the de facto revenue during the testing period. In order to quantify the error in forecasting, we will relate the error to the actual revenue in the goven years. . df_for_accuracy = df_test_for_plotting.join(forecast.set_index(&quot;ds&quot;), how=&quot;left&quot;).reset_index() df_for_accuracy_2016 = df_for_accuracy[df_for_accuracy[&quot;ds&quot;] &lt; &quot;2017&quot;] df_for_accuracy_2017 = df_for_accuracy[df_for_accuracy[&quot;ds&quot;] &gt;= &quot;2017&quot;] . df_for_accuracy_2016[&quot;abs. difference&quot;] = df_for_accuracy_2016[ &quot;y&quot;] - df_for_accuracy_2016[&quot;yhat&quot;] df_for_accuracy_2017[&quot;abs. difference&quot;] = df_for_accuracy_2017[ &quot;y&quot;] - df_for_accuracy_2017[&quot;yhat&quot;] . error_2016 = df_for_accuracy_2016[&quot;abs. difference&quot;].sum() / df_for_accuracy_2016[&quot;y&quot;].sum() print(&quot;The relative error in the forecast revenue in 2016 is %.3f.&quot; % error_2016) . The relative error in the forecast revenue in 2016 is 0.002. . error_2017 = df_for_accuracy_2017[&quot;abs. difference&quot;].sum() / df_for_accuracy_2017[&quot;y&quot;].sum() print(&quot;The relative error in the forecast revenue in 2017 is %.3f.&quot; % error_2017) . The relative error in the forecast revenue in 2017 is 0.061. . It turns out we have a pretty solid estimate for the revenues in the two forecasted years with a relative error of 0.2% and 6% respectively- .",
            "url": "http://blog.huppertz-consulting.de/time%20series/fbprophet/2020/05/22/revenue-forecasting-fbprophet.html",
            "relUrl": "/time%20series/fbprophet/2020/05/22/revenue-forecasting-fbprophet.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Simple Visualizer for Teradata/T-SQL Queries",
            "content": "A Simple Visualizer for T-SQL scripts . A while ago, I was working on a project where the client used an expensive high-performance database cluster in order to do demand forecasting for a German supermarket chain. . What the client did not have: . Automated testing | CD/CI-Pipelines | Coding guidelines | Documentation | . What the client did have however, was a codebase of about 30,000 lines of pure T-SQL code and a team that learned “on the job”, i.e. had never written a functioning application in this SQL dialect. This resulted in a about 10 scripts of varying length and written in different styles by different people, most of whom had left the project in frustration already by the time I joined. . Combine that with requirements which were subject to change on daily (sometimes hourly) basis, arbitrary restrictions from operations (indexes take up too much space) and being about 100% over budget, it was a recipe for disaster. . Debugging could mean scrolling through a single or multiple scripts each consisting of 3,000 lines of code and dozens of SQL statements and manually recalculating steps along the way. This would of course take hours within which priorities of implementation could (and frequently did) change again. . I was in desperate need for some kind of support and wanted to use queryscope to get at least a graphical representation of the queries in question but of course the source code was property of the client and I was not allowed to submit it to an unvetted, unapproved vendor for analysis. . So, I took to recreating parts of queryscope’s features from scratch in python to keep all source on premise, the result of which you can see here: . Running the script will yield results like so: . . Output of the query below | . Caveat: This code was only tested with the very specific way the developers on this project wrote their SQL queries, i.e. using many a CTE to create complex temporary tables and then persisting them into intermediary tables, a pattern which is replicated with mock statements in the following SQL snippet: . WITH cte_category_counts ( category_id, category_name, product_count ) AS ( SELECT c.category_id, c.category_name, COUNT(p.product_id) FROM production.products p INNER JOIN production.categories c ON c.category_id = p.category_id GROUP BY c.category_id, c.category_name ), cte_category_sales(category_id, sales) AS ( SELECT p.category_id, SUM(i.quantity * i.list_price * (1 - i.discount)) FROM sales.order_items i INNER JOIN production.products p ON p.product_id = i.product_id INNER JOIN sales.orders o ON o.order_id = i.order_id WHERE order_status = 4 -- completed GROUP BY p.category_id ) INSERT INTO categories (category_id, category_name, product_count,sales) SELECT c.category_id, c.category_name, c.product_count, s.sales FROM cte_category_counts c INNER JOIN cte_category_sales s ON s.category_id = c.category_id ORDER BY c.category_name; WITH cte_business_division_counts (division_id, counts) AS ( SELECT d.division_id, SUM(c.counts) FROM categories c INNER JOIN organization.divisions d ON c.category_id = d.category_id GROUP BY d.division_id ) cte_business_division_sales (division_id, division_name, sales) AS ( SELECT d.division_id, d.division_name, SUM(c.sales) FROM categories c INNER JOIN organization.divisions d ON c.category_id = d.category_id GROUP BY d.division_id ) INSERT INTO business_divisions (division_id, division_name, product_count,sales) SELECT s.division_id, s.division_name, s.sales, c.counts FROM cte_business_division_counts c INNER JOIN cte_business_division_sales s ON s.division_id = c.division_id ORDER BY c.division_name; . And this here is the source code for the actual visualizer: . import re import os from collections import Counter from graphviz import Digraph import networkx as nx # can be used to generate JSON format of graph # from networkx.readwrite import json_graph basedir = &#39;.&#39; filenames = [&#39;test.sql&#39;] with_deletes = False dot = Digraph(comment=&#39;Structure&#39;, engine=&#39;dot&#39;, strict=True) G = nx.Graph() # graphically highlight tables and their incoming connections highlight_nodes = [] # do not draw these ignore_list = [] # collect all nodes here complete_list = [] for filename in filenames: f = open(os.path.join(basedir, filename)) f_string = f.read() sep = &#39;;&#39; f_list = [x+sep for x in f_string.split(sep)] f_list = [x.replace(&quot; n&quot;, &quot; &quot;) for x in f_list] f_list = [x for x in f_list if x.strip() != &quot;COMMIT;&quot;] complete_list += f_list # define regular expressions for SQL statements re_create = re.compile( r&quot;CREATE s+(?:MULTISET)? s+TABLE s+([a-zA-Z0-9_ .]+) s*,&quot;, re.MULTILINE | re.IGNORECASE) re_insert = re.compile( r&quot;INSERT s+INTO s+([a-zA-Z0-9_ .]+) s* (&quot;, re.MULTILINE | re.IGNORECASE) re_from = re.compile( r&quot;(?&lt;!DELETE) s+FROM s+([a-zA-Z0-9_ .]+)[; s]+&quot;, re.S | re.I) re_cte = re.compile( r&quot;(?:WITH)? s+([A-Za-z0-9_]+) s+AS s? (&quot;, re.MULTILINE | re.IGNORECASE) re_join = re.compile( r&quot;JOIN s+([A-Za-z0-9_ .]+) s&quot;, re.MULTILINE | re.IGNORECASE) re_update = re.compile( r&quot;UPDATE s([ w])+ sSET s[ w , &#39; =_]+&quot;, re.MULTILINE | re.IGNORECASE) re_delete = re.compile( r&quot;DELETE sFROM s([ d w . &#39; =_]+.*;)&quot;, re.S | re.IGNORECASE) node_list = [] delete_nodes = [] create_nodes = [] # go through all statements and check if they match a regex for i, statement in enumerate(complete_list): statement = statement.replace(&quot; n&quot;, &quot; &quot;) to_nodes = [] from_nodes = [] for match in re.findall(re_create, statement): create_nodes.append(match) for match in re.findall(re_delete, statement): delete_nodes.append(match) for match in set(re.findall(re_insert, statement)): to_nodes.append(str.lower(match)) for match in set(re.findall(re_update, statement)): print(match) for match, count in Counter(re.findall(re_cte, statement)).items(): print(&quot;%s : %i&quot; % (match, count)) for match, count in Counter(re.findall(re_from, statement)).items(): if match not in re.findall(re_cte, statement): from_nodes.append(str.lower(match)) # print(5*&#39;-&#39; + &#39;Joins (CTEs removed)&#39; + 5*&#39;-&#39;) for match, count in Counter(re.findall(re_join, statement)).items(): if match not in re.findall(re_cte, f_string): # print(&quot;%s : %i&quot; % (match,count)) from_nodes.append(str.lower(match)) from_nodes = [x for x in from_nodes if x not in ignore_list] to_nodes = [x for x in to_nodes if x not in ignore_list] for to_node in to_nodes: # for every node switch between picking colors from the &quot;left&quot; and &quot;right&quot; end # the spectrum so similar colors do not appear next to each other if i % 2 == 0: edge_color = str(round(i/float(len(complete_list)), 2))+&quot; 1.0 &quot; + str(round(i/float(len(complete_list)), 2)) else: edge_color = str(round((len(complete_list)-i)/float(len(complete_list)), 2)) + &quot; 1.0 &quot; + str(round((len(complete_list)-i)/float(len(complete_list)), 2)) if to_node in highlight_nodes: dot.node(to_node, color=edge_color, style=&#39;filled&#39;, fillcolor=edge_color) else: dot.node(to_node, color=edge_color) for from_node in from_nodes: dot.node(from_node, shape=&#39;box&#39;) if (to_node in highlight_nodes) or (from_node in highlight_nodes): dot.edge(from_node, to_node, color=edge_color, penwidth=&#39;3&#39;) else: dot.edge(from_node, to_node, color=edge_color) # graphically represent deletes within the query if with_deletes: delete_nodes = [str.lower(del_node) for del_node in delete_nodes] delete_label = &#39;&lt;&lt;B&gt;DELETES&lt;/B&gt;&lt;br/&gt;&#39; + &#39;&lt;br/&gt;&#39;.join(delete_nodes) + &#39;&gt;&#39; dot.node(&#39;DELETES&#39;, label=delete_label, shape=&#39;box&#39;) # print(5*&#39;-&#39; + &#39;All participating Tables&#39; + 5*&#39;-&#39;) # for match in set(re.findall(re_from,f_string)+re.findall(re_insert,f_string)+re.findall(re_join,f_string)): # if match not in re.findall(re_cte,f_string): # print (match) dot.render(&#39;output/&#39;+filename.replace(&#39;.&#39;, &#39;_&#39;)+&#39;.gv&#39;, view=True) # print(dot.source) # dot_graph = G.read_dot() # print (json_graph.dumps(dot_graph)) . As you can see, there are also ways to highlight certain nodes, as well as omitting them from the output. There is also an automatic random assignment of a color to any persistent table and all edges going into the respective node are also color-coded accordingly. Nodes that have no downstream dependents within the query are displayed as an ellipse as opposed to a rectangle. . This whole thing is a quick draft that took me about 4-5 hours to write, but it saved me countless hours in debugging. . This script could also be integrated into a CD/CI-pipeline in order to create a constantly updated visual documentation of the project. .",
            "url": "http://blog.huppertz-consulting.de/tooling/python/2020/05/20/sql-visualizer.html",
            "relUrl": "/tooling/python/2020/05/20/sql-visualizer.html",
            "date": " • May 20, 2020"
        }
        
    
  
    
  
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Manuel Huppertz and I have a passion for data, maths and building the right tools for the job. On these pages I will take you on a tour through some of the things I created in the last few years and some of my musings and experiments in the data science field. . I work as a freelace consultant and you can hire me if you need an IT project manager, digital marketing specialist or data scientist. . If you want to contact me, check out my main page. .",
          "url": "http://blog.huppertz-consulting.de/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://blog.huppertz-consulting.de/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}