{
  
    
        "post0": {
            "title": "Support Vector Machines: Building Intuition",
            "content": "When you start having intricate overlaps or irregularly shaped boundaries between the classes SVM generalize well, but need a little bit of tweaking and tuning to achieve good results. In other words: you need to know what you are doing. . Let&#39;s walk through some simple examples from the sklearn documentation first to get a feeling of how SVMs behave. We will go light on the theory as I will explore this another time in a future post. . First we need to import some basics: . import numpy as np import matplotlib.pyplot as plt import matplotlib as mpl from sklearn import svm from sklearn.datasets import make_blobs import warnings warnings.filterwarnings(&#39;ignore&#39;) # matplotlib is a very talkative library . Trivial Examples in 2 and 3 Dimensions . 2 Dimensional Data . The first example is loosely based on the sklearn documentaiton for support vector machines. We start by creating a few 2-dimensional data points $X in mathbb{R}^{40x2}$ and assigning them each one out of two classes $y in {0,1 }^{40}$. . X, y = make_blobs(n_samples=40, centers=2, random_state=6) plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired) # saving the x and y axis max/min values # we will need those later for more plots xlim = plt.gca().get_xlim() ylim = plt.gca().get_ylim() . It is very easy to separate these two classes by intuition with simple straight line, right through the middle of the gap between the two classes. You would then - intuitively - classify any new data south of that line belonging to the red class and any data point north of that line to the blue class. . In order to see if the SVM arrives at a result similiar to our intuition, let us first train the model. . # create and fit the model clf = svm.SVC(kernel=&#39;linear&#39;, C=1) clf.fit(X, y) . SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;, kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) . What follows next is basically only for visualization purposes. We draw an equisdistant grid across the $ mathbb{R}^2$ domain where our features $X$ live and let the SVM decide for each pair of $ x = (x^{(1)},x^{(2)})$ coordinates, which score it assigns to that point. . # create grid of x_1 and x_2 coordinates x_1 = np.linspace(xlim[0], xlim[1], 30) x_2 = np.linspace(ylim[0], ylim[1], 30) X_1, X_2 = np.meshgrid(x_1, x_2) grid = np.vstack([X_1.ravel(), X_2.ravel()]).T . With this grid we can calculate the classification for &quot;all&quot; possible points in the feature space or at least a close enough approximation. . # Calculate the value for every possible x_1-x_2-combination # that is almost the same as using the predict function, but just # shying away from using a threshold to decide which class a # point would belong to Z = clf.decision_function(grid).reshape(X_1.shape) . Now, we can simply plot the data and then overlay the datapoints with the values of the decision function. We can do that as a colored overlay: . # plot the data points plt.scatter(X[:, 0], X[:, 1], c=&#39;k&#39;, s=30) # plot decision function and values cntr1 = plt.contourf(X_1, X_2, Z, levels=20, alpha=0.5, cmap=&#39;viridis&#39;) plt.colorbar(cntr1).set_label(&#39;decision function&#39;) plt.xlabel(&#39;x_1&#39;) plt.ylabel(&#39;x_2&#39;) plt.show() . Or we can simply pick out the values which are most interesting to us, such as the decision boundary, where the decision function becomes $0$ and the support vectors, which are the data points where the decision function becomes $1$ or $-1$ respectively. . # plot the data points plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired) # plot decision boundary and margins plt.contour(X_1, X_2, Z, colors=&#39;k&#39;, levels=[-1, 0, 1], alpha=0.5, linestyles=[&#39;--&#39;, &#39;-&#39;, &#39;--&#39;]) # plot support vectors plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100, linewidth=1, facecolors=&#39;none&#39;, edgecolors=&#39;k&#39;) plt.show() . As we can see, the SVM has generated a solution that is in line with our intuitive guess about how the featuer space should be divided up according to classes. The dashed lines represent the levels of $1$ and $-1$ where the solid line defines the decision boundary at the zero-level of the decision function. . To do right by the SVM algorithm though - and to mentally prepare ourselves for the generalization to non-trivial examples that lies ahead - we have to visualize the data and the decision function in 3D. . from mpl_toolkits.mplot3d import Axes3D fig = plt.figure(figsize=(12, 12)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.plot_surface(X_1, X_2, Z, cmap=&#39;viridis&#39;, alpha=0.5) ax.contour(X_1, X_2, Z, colors=&#39;k&#39;, levels=[-1, 0, 1], alpha=1, linestyles=[&#39;--&#39;, &#39;-&#39;, &#39;--&#39;]) xy_data = np.vstack([X[:, 0].ravel(), X[:, 1].ravel()]).T ax.scatter(X[:, 0], X[:, 1], 0, c=y, s=30, cmap=plt.cm.Paired) ax.set_xlabel(&#39;x_1&#39;) ax.set_ylabel(&#39;x_2&#39;) ax.set_zlabel(&#39;decision function&#39;) ax.view_init(15, -15) . As we can see here, the decision function defines a hyperplane separating our datapoints into areas which map to $Z&lt;0$ and $Z&gt;0$ respectively. Note that the hyper&quot;plane&quot; is actually the solid black line that you see where $Z=0$. . 3-Dimensional Data . Of course there is no need to limit oneself to two features, but as we always need to add at least one dimension to create a hyperplane, it is hard to create sensible visualizations in higher dimensions. But let us at least try for 3 feature dimensions. . # we create 40 separable points # note that X has three dimensions here and y is the class they belong to X, y = make_blobs(n_samples=40, n_features=3, centers=2, random_state=6) fig = plt.figure(figsize=(12, 12)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, s=30, cmap=plt.cm.Paired) # saving the x and y axis max/min values # we will need those later for more plots xlim = plt.gca().get_xlim() ylim = plt.gca().get_ylim() zlim = plt.gca().get_zlim() . We then fit the model again, but now simply with a 3D input. . clf_3d = svm.SVC(kernel=&#39;linear&#39;, C=1) clf_3d.fit(X, y) . SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;, kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) . And then repeat our grid creation procedure again, just add one more dimension. . # create grid of x_1-x_2-x_3 coordinates x_1 = np.linspace(xlim[0], xlim[1], 30) x_2 = np.linspace(ylim[0], ylim[1], 30) x_3 = np.linspace(zlim[0], zlim[1], 30) X_1, X_2, X_3 = np.meshgrid( x_1, x_2, x_3) grid_3d = np.vstack([X_1.ravel(), X_2.ravel(), X_3.ravel()]).T # additionally create a grid only consisting of the data_points # this is essentially just reformatting them # so we can work with the grid and the data # without changing our functions grid_3d_data = np.vstack([X[:,0].ravel(), X[:,1].ravel(), X[:,2].ravel()]).T . # Calculate the value for for every possible x_1-x_2-x_3-combination fourth_dimension_grid = clf_3d.decision_function(grid_3d) fourth_dimension_data = clf_3d.decision_function(grid_3d_data) . Now, we have the little challenge that we have been working with point clouds so far. When drawing in 2D there is an interpolation going on under the hood, so we could easily draw a decision function using &quot;contour&quot; and matplotlib would fill in the gaps in our grid. I know of no such function in 3D, so we need a little hack. . As an approximation of the zero-level of the decision function, I will use the points in space where it is very close to $0$. . decision_boundary_3d = grid_3d[np.abs(fourth_dimension_grid)&lt;0.01] . We then use a triangulated surface plot to fill the gaps in the point cloud. It looks a bit wonky, but it does the job. . fig = plt.figure(figsize=(12, 12)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) # create triangulated surface with the # points of the decision boundary ax.plot_trisurf(decision_boundary_3d[:, 0], decision_boundary_3d[:, 1], decision_boundary_3d[:, 2], color=&#39;teal&#39;, alpha=0.4) # add in the 3D datapoints and color them # according to their decision function values scatter = ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=fourth_dimension_data, s=30, cmap=&#39;viridis&#39;) plt.colorbar(scatter, shrink = .7).set_label(&#39;decision function&#39;) ax.set_xlabel(&#39;x_1&#39;) ax.set_ylabel(&#39;x_2&#39;) ax.set_zlabel(&#39;x_3&#39;) ax.view_init(15, 135) . Of course, this is another very trivial example, the dataset is clearly linearly separable just in three dimensions instead of two. But we have been able to buld intuition about how the separation between classes in more than three dimensions works, i.e. by defining a function whose zero-level separates the classes. . Summary . A few trivial examples have shown the basic principles of SVM: . embed the data into a higher dimensional space | use a function mapping the data into the higher dimensional space | choose the function so that the difference between classes is high, but lower for different datapoints within the same class | split the distance in the values and classify new data according to which side of the split they appear on | . Generalizing to Non-Trivial Examples . The more interesting case is of course the one where we cannot simply arrive at an almost perfect solution by eyeballing and jamming a straight line or plane between the clusters. So what happens when we don&#39;t have a linearly separable dataset? . 2-Dimensional Data . Let us create a nested, 2-dimensional dataset using the make_gaussian_quantiles function. . from sklearn.datasets import make_gaussian_quantiles X, y = make_gaussian_quantiles(mean=None, cov=1.0, n_samples=100, n_features=2, n_classes=2, shuffle=True, random_state=42) # draw the result plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired) # saving the x and y axis max/min values # we will need those later for more plots xlim = plt.gca().get_xlim() ylim = plt.gca().get_ylim() . Then we try to fit our classifier again, without accommodating to the new dataset structure. . clf = svm.SVC(kernel=&#39;linear&#39;) clf.fit(X, y) . SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;, kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) . And visualize the results just like before. . x_1 = np.linspace(xlim[0], xlim[1], 30) x_2 = np.linspace(ylim[0], ylim[1], 30) X_1, X_2 = np.meshgrid(x_1, x_2) grid = np.vstack([X_1.ravel(), X_2.ravel()]).T Z = clf.decision_function(grid).reshape(X_1.shape) plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired) plt.contour(X_1, X_2, Z, colors=&#39;k&#39;, levels=[-1, 0, 1], alpha=0.5, linestyles=[&#39;--&#39;, &#39;-&#39;, &#39;--&#39;]) plt.show() . The SVM is clearly out of its depth with the concentric dataset. Our decision boundary has almost nothing to do with the data and is completely useless. This is where we have to introduce the concept of &#39;kernels&#39;. . In the trivial case above we introduced a linear function that had values larger than $1$ around the one class of dots and smaller than $-1$ around the other class. In the trench between the two sets, our decision function went from $1$ through $0$ to $-1$. . Now we simple need to do the same thing but instead of an linear function, we use another function that is larger than $0$ for data from one class and smaller for the other class and pass thorugh $0$ on its way from one class to another. . And, ideally, it is differentiable to do fancy math stuff with it later. . It turns out that that we can construct such a function by using the sum of a lof of smaller functions and one good option is the gaussian kernel function. You can simply think &#39;normal distribution&#39; here: we create a little normal distribution around each data point and multiply with $1$ or $-1$ accordingly. . def gaussian_kernel(X, Y, sigma=1): similarity = 1 / (sigma * np.sqrt(2 * np.pi)) * np.exp( -np.linalg.norm(np.subtract(X, Y), ord=2) / (2 * (sigma**2))) return np.array(similarity).reshape(1, -1) . This function simply takes to vectors $X$ and $Y$, calculates the euclidean distance between them and then scales the distance on a gaussian curve, i.e. close to the peak if they are very close to each other but then rapidly declining, given the right $ sigma$ . If we choose $(x^{(1)},x^{(2)}) = [0,0]$ as the center, that is what the function looks like: . fig = plt.figure(figsize=(12, 12)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) Z = np.array([gaussian_kernel(point, [0, 0]) for point in grid]).reshape(X_1.shape) ax.plot_surface(X_1, X_2, Z) ax.set_xlabel(&#39;X_0&#39;) ax.set_ylabel(&#39;X_1&#39;) ax.set_zlabel(&#39;kernel function&#39;) ax.view_init(0, 105) . This function fulfills all the criteria above, but in order to use it in the the SVM we still have to comply with a technicality. . I was actually a bit stumped by this, as the sklearn documentation does not go into detail at all about how to properly implement a kernel function. Luckily, there was a stackoverflow comment that came to my rescue and which you will find linked in my sources. It said: . For efficiency reasons, SVM assumes that the kernel is a function accepting two matrices of samples. . So, instead of single vectors, we need a function that takes and returns matrices such as: . from functools import partial def proxy_kernel(X, Y, K): gram_matrix = np.zeros((X.shape[0], Y.shape[0])) for i, x in enumerate(X): for j, y in enumerate(Y): gram_matrix[i, j] = K(x, y) return gram_matrix correct_gaussian_kernel = partial(proxy_kernel, K=gaussian_kernel) . We can then try to fit the SVM again, using the new kernel: . clf = svm.SVC(kernel=correct_gaussian_kernel, C=1) clf.fit(X, y) . SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;, kernel=functools.partial(&lt;function proxy_kernel at 0x0000015D60DA8510&gt;, K=&lt;function gaussian_kernel at 0x0000015D60DF4D90&gt;), max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) . From here on out, it&#39;s smooth sailing: we resuse the grid from before, calculate the decision function on each dot in the grid an visualize the decision boundary . Z = clf.decision_function(grid).reshape(X_1.shape) plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired) plt.contour(X_1, X_2, Z, colors=&#39;k&#39;, levels=[-1, 0, 1], alpha=0.5, linestyles=[&#39;--&#39;, &#39;-&#39;, &#39;--&#39;]) plt.show() . As we see here the decision boundary (solid line) neatly encircles the cluster of blue dots except for a three escapees in the lower right and top left quadrant of the boundary. . When plotting the decision function in 3D and overlaying it with the data, it becomes a sort of well where every data point that crosses the solid boundary falls into the well and gets classified as a blue dot - or in other words: the solid line indicates where the zero-level of the decision function lies and that zero-level defines the hyperplane that separates the data. . fig = plt.figure(figsize=(12, 12)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.plot_surface(X_1, X_2, Z, cmap=&#39;viridis&#39;, alpha=0.2) ax.contour(X_1, X_2, Z, colors=&#39;k&#39;, levels=[-1, 0, 1], alpha=1, linestyles=[&#39;--&#39;, &#39;-&#39;, &#39;--&#39;]) xy_data = np.vstack([X[:, 0].ravel(), X[:, 1].ravel()]).T ax.scatter(X[:, 0], X[:, 1], clf.decision_function(xy_data), c=y, s=30, cmap=plt.cm.Paired) ax.set_xlabel(&#39;x_1&#39;) ax.set_ylabel(&#39;x_2&#39;) ax.set_zlabel(&#39;decision function&#39;) ax.view_init(15, 15) . 3-Dimensional Data . When trying to elevate this experiment to three dimensional input data, we obviously lose the ability to visualize the decision function in its entirety, but as before, we can still visualize the hyperplane of the zero-level set. Let us start with concentric 3D data. i.e. it is a ball of &quot;blue&quot; data in the middle of a &quot;red&quot; data cloud. . # note that X has three dimensions here and y is the class they belong to X, y = make_gaussian_quantiles(mean=None, cov=1.0, n_samples=200, n_features=3, n_classes=2, shuffle=True, random_state=42) fig = plt.figure(figsize=(12, 12)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, s=30, cmap=plt.cm.Paired) # saving the x and y axis max/min values # we will need those later for more plots xlim = plt.gca().get_xlim() ylim = plt.gca().get_ylim() zlim = plt.gca().get_zlim() ax.set_xlabel(&#39;x_1&#39;) ax.set_ylabel(&#39;x_2&#39;) ax.set_zlabel(&#39;x_3&#39;); ax.view_init(15, 215) . And we also classify that with our custom gaussian kernel. So, what&#39;s the gaussian kernel in 3D? It is a sphere where the space is more densely packed in the middle and the density then (depending on $ sigma$) quickly decreased as you move outwards. . clf_3d = svm.SVC(kernel=correct_gaussian_kernel) clf_3d.fit(X, y) . SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;, kernel=functools.partial(&lt;function proxy_kernel at 0x0000015D60DA8510&gt;, K=&lt;function gaussian_kernel at 0x0000015D60DF4D90&gt;), max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) . Again, we create a grid covering the whole data domain and calculating the decision function for all the grid points. . # create grid of x_1-x_2-x_3-coordinates x_1 = np.linspace(xlim[0], xlim[1], 30) x_2 = np.linspace(ylim[0], ylim[1], 30) x_3 = np.linspace(zlim[0], zlim[1], 30) X_1, X_2, X_3 = np.meshgrid(x_1, x_2, x_3) grid_3d = np.vstack([X_1.ravel(), X_2.ravel(), X_3.ravel()]).T # Calculate decision function value fourth_dimension_grid = clf_3d.decision_function(grid_3d) . Properly displaying the 3-dimensional zero-level of a 4-dimensional function was a bit challenging, as there is no function in matplotlib that gracefully handles converting a point cloud to a surface. . So, I sliced the $x_3$-axis up and then, for each fixed $x_3$ value, I calculated the decision function across the whole $x_1$-$x_2$-domain.Finally, I filtered for where the result is zero, thereby identifying a ring that is part of the whole 3D surface. . As matplotlib can apparently nicely interpolate the zero-level in 2 dimensions, we get the contour of a slightly deformed ball that nicely encompasses all the blue data points. . fig = plt.figure(figsize=(12,12)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) for x_3 in np.unique(grid_3d[:,2]): # create grid in x_1-x_2-domain X1,X2 = np.meshgrid(x_1,x_2) # Calculate the value of the decision function for each x_3 slice X3 = clf_3d.decision_function(grid_3d[grid_3d[:,2] == x_3]) # offset the slice to the right position # as we are looking for the zero-level of Z (Z=0) we actually need to # display the level x_3 as Z+x_3 = 0+x_3 = x_3 cset = ax.contour(X1,X2,X3.reshape(X1.shape)+x_3, levels = [x_3], zdir=&#39;z&#39;) # plot the data ax.scatter(X[:, 0], X[:, 1], X[:,2],c = y, s=30, cmap=plt.cm.Paired) ax.set_xlabel(&#39;x_1&#39;) ax.set_ylabel(&#39;x_2&#39;) ax.set_zlabel(&#39;x_3&#39;); ax.view_init(15, 215) . So, we have seen in this chapter how to generalize the method into higher dimensions: Using a similarity kernel, we create a function that tends towards positive values for one class and negative values for the other. . But, this whole ordeal displays one of the biggest drawbacks of the method: you need to get a good idea about the topology that your dataset creates before you can choose an appropriate kernel. Alternatively, you can brute force your way through all the available kernel and parameter options. . Sources . http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf | https://www.youtube.com/watch?v=efR1C6CvhmE | https://www.youtube.com/watch?v=FCUBwP-JTsA | https://www.youtube.com/watch?v=wBVSbVktLIY | https://stackoverflow.com/questions/26962159/how-to-use-a-custom-svm-kernel | .",
            "url": "http://blog.huppertz-consulting.de/classification/application/support%20vector%20machines/2020/10/06/support-vector-machines-building-intuition.html",
            "relUrl": "/classification/application/support%20vector%20machines/2020/10/06/support-vector-machines-building-intuition.html",
            "date": " • Oct 6, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "An In-Depth Look at Logistic Regression",
            "content": "Understanding the ins and outs of a logistic regression is non-trivial. Many sources either only touch the theoretical side or the implementation side respectively. In this post, I would like to create a one-stop-shop for the theoretical basis and the practical implementations of the logistic regression. . Our Dataset . We will use the well-known iris dataset as basis for our discussion. It is easy enough to still employ some intuition when trying to understand the logistic regression. When loading the dataset from sklearn, we get 150 observations of measurements of some of the plants features. Then botanists have classified the irises into three subtypes with fancy latin names, but since I am no botanist, we will just pretend that - based on those measurements - we can determine if the flower blooms red, green or blue. The measurements will end up in the set of observations $X$, with each observation consisting of 4 measurements each and the classification ends up in the vector $y$ with entries being 0 = red, 1 = green, 2 = blue. . from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression import warnings warnings.filterwarnings(&#39;ignore&#39;) import numpy as np import pandas as pd import matplotlib.pyplot as plt . X, y = load_iris(return_X_y=True) . columns = (&#39;Measurement 1&#39;, &#39;Measurement 2&#39;, &#39;Measurement 3&#39;, &#39;Measurement 4&#39;) dataset = pd.DataFrame(X, columns=columns) dataset[&quot;True Classification&quot;] = y dataset . Measurement 1 Measurement 2 Measurement 3 Measurement 4 True Classification . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . 3 4.6 | 3.1 | 1.5 | 0.2 | 0 | . 4 5.0 | 3.6 | 1.4 | 0.2 | 0 | . ... ... | ... | ... | ... | ... | . 145 6.7 | 3.0 | 5.2 | 2.3 | 2 | . 146 6.3 | 2.5 | 5.0 | 1.9 | 2 | . 147 6.5 | 3.0 | 5.2 | 2.0 | 2 | . 148 6.2 | 3.4 | 5.4 | 2.3 | 2 | . 149 5.9 | 3.0 | 5.1 | 1.8 | 2 | . 150 rows × 5 columns . colors = [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;] for X_el, y_el in zip(X, y): plt.plot(X_el, colors[y_el], alpha=0.1) plt.xticks(np.arange(4), columns) plt.show() . We can clearly see, that measurement 3 carries a lot of information. It separates neatly between the red flowers and the rest, and it even seems to be a good guide to separate the greens from the blues. Same goes for measurement 4, while 1 and 2 seem to give no hint as to which class a certain specimen belongs to. . clf = LogisticRegression(random_state=42).fit(X, y) . After having trained the model, the predict function allows us to get a predict for each input tuple. The classes are numbered not named, but can of course be converted to names using the appropriate mapping. . clf.predict(X) . array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) . Besides a simple class prediction, we can also get the probability for each class for each input data sample. . clf.predict_proba(X)[0:5] # abbreviated output . array([[8.78030305e-01, 1.21958900e-01, 1.07949250e-05], [7.97058292e-01, 2.02911413e-01, 3.02949242e-05], [8.51997665e-01, 1.47976480e-01, 2.58550858e-05], [8.23406019e-01, 1.76536159e-01, 5.78217704e-05], [8.96034973e-01, 1.03953836e-01, 1.11907339e-05]]) . For the first example above, we assign an 88% chance of it belonging to class &#39;0&#39; , a 12% chance of it belonging to class &#39;1&#39; and (almost) 0% chance of it belonging to class &#39;2&#39;. using the argmax function, we could then map the probabilites to the exact outcome of the predictfunction above. . So, for the first part, we have examined the dataset and we got an idea, how well it will be able to separate the classes, based on the features given. Also, we have generated the output of the sklearn logistic regression that a training on the complete dataset is providing and how to interpret that. . Evaluating the Model . A common metric to evaluate the quality of predictions is the &#39;score&#39;, which - according to the sklearn help - is: . In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. . clf.score(X, y) . 0.96 . Basically that means we count all correctly classified labels as a percentage like so: . correct = [] for y_true, y_pred in zip(y, clf.predict(X)): if y_true == y_pred: correct.append(1) else: correct.append(0) sum(correct) / len(correct) . 0.96 . Given that we used the complete dataset for training, that&#39;s just okay. A more detailed approach at evaluating the quality of our model would be based on the classification report. . from sklearn.metrics import classification_report print(classification_report(y_pred=clf.predict(X), y_true=y)) . precision recall f1-score support 0 1.00 1.00 1.00 50 1 0.98 0.90 0.94 50 2 0.91 0.98 0.94 50 micro avg 0.96 0.96 0.96 150 macro avg 0.96 0.96 0.96 150 weighted avg 0.96 0.96 0.96 150 . While the class 0 = red has perfect precision and recall, because classes &quot;1 = green&quot; and &quot;2 = blue&quot; somewhat overlap in their features, the algorithm mixes them up. This is shown by the recall 0.9 of class &quot;1&quot;, which means only 90% of all green flowers where classified as green and the precision of class &quot;2&quot; which means only 91% of all flowers classified as blue where actually blue. . y_true_with_jitter = y + np.random.rand(y.shape[0]) * 0.25 y_classified_with_jitter = clf.predict(X) + np.random.rand(y.shape[0]) * 0.25 plt.xticks(np.arange(3), (&#39;true red&#39;, &#39;true green&#39;, &#39;true blue&#39;)) plt.yticks(np.arange(3), (&#39;class. red&#39;, &#39;class. green&#39;, &#39;class. blue&#39;)) plt.scatter(y_true_with_jitter, y_classified_with_jitter, color=[colors[y_el] for y_el in y]) plt.show() . Is it a good model? Well, on one hand, the bare values of our statistics are really quite good, on the other we have a tiny dataset and we trained on the complete dataset as well, as we did not keep a holdout dataset for testing. Since we do not pay too much attention to the actual result of flower&#39;s colors being predicted correctly though and only want to understand how to arrive at the predictions,we will give it a pass. Just make a mental note never to do that in a real-world application. . A Look at the Inner Workings of the sklearn Logistic Regression . When looking at the variables of the log regression classifier after training, we find three sets of coefficients and three different intercept. That is because log regression is essentially binary, i.e. does only a yes/no or 1/0 classification. If we have $n &gt; 2$ classes in a classification problem, we need split this problem into $n$ separate &quot;1 vs. all&quot; or &quot;1 vs. rest&quot; classification problems. Each set of coefficients and each intercept belongs to a each of these sub-classifications. . clf.__dict__[&quot;coef_&quot;] # just running clf.__dict__ spits out all the info about the trained model . array([[ 0.41021713, 1.46416217, -2.26003266, -1.02103509], [ 0.4275087 , -1.61211605, 0.5758173 , -1.40617325], [-1.70751526, -1.53427768, 2.47096755, 2.55537041]]) . clf.__dict__[&quot;intercept_&quot;] . array([ 0.26421853, 1.09392467, -1.21470917]) . If we now feed a set of features $x^i$ into the trained classifier, we can calculate the probabilities of $x^i$ belonging a class vs. not belonging to that class via: . $$p(x_i)= frac{1}{1+e^{-( beta_{0} + beta_{1}x^{i}_1 + beta_{2}x^{i}_2 + beta_{3}x^{i}_3 + beta_{4}x^{i}_4)}}$$ . where $ beta_{0}$ is an intercept and $ beta_{1}.. beta_{4}$ are the coefficients for each entry in the feature vector $x^i = (x^{i}_1,x^{i}_2,x^{i}_3,x^{i}_4)$. We will later explore, why this term is the correct one. Let us calculate the above for our very first observation in the dataset: . # @ is shorthand for matrix multiplication p_0 = 1 / (1 + np.exp(-(clf.intercept_[0] + X[0] @ clf.coef_[0]))) p_1 = 1 / (1 + np.exp(-(clf.intercept_[1] + X[0] @ clf.coef_[1]))) p_2 = 1 / (1 + np.exp(-(clf.intercept_[2] + X[0] @ clf.coef_[2]))) print(&#39;p_0 =&#39;, p_0) print(&#39;p_1 =&#39;, p_1) print(&#39;p_2 =&#39;, p_2) . p_0 = 0.9838989815463852 p_1 = 0.13666411838386314 p_2 = 1.209652515457732e-05 . With this calculation, we have now determined that our $x^0$ has a $0.98$ chance of belonging to class &quot;0 = red&quot; vs. any of the other classes - either green or blue. As we can see though, these three probabilities do not add up to 1 and why should they? These probabilities belong to three mathematically independent problems: . Does $x^i$ belong to class 0 vs. not to class 0? | Does $x^i$ belong to class 1 vs. not to class 1? | Does $x^i$ belong to class 2 vs. not to class 2? | . What happens, if we linearly scale those probabilities though so that they add up to 1? . p_sum = p_0 + p_1 + p_2 print(&quot;p_0_scaled =&quot;, p_0 / p_sum) print(&quot;p_1_scaled =&quot;, p_1 / p_sum) print(&quot;p_2_scaled =&quot;, p_2 / p_sum) . p_0_scaled = 0.8780303050242847 p_1_scaled = 0.12195890005075813 p_2_scaled = 1.0794924957147882e-05 . We have seen these exact numbers before and can make our choice for a prediction using argmax: . clf.predict_proba(X)[0] . array([8.78030305e-01, 1.21958900e-01, 1.07949250e-05]) . np.argmax(clf.predict_proba(X)[0]) # 0 = red, 1 = green, 2 = blue . 0 . Now we have an understanding how the interpret the data generated by the training process of sklearn and we have looked beyond the clf.predict function to understand how the predictions are then picked in the trained model. . The Mathematical Background . We used the formula . $$p(x^i)= frac{1}{1+e^{-( beta_{0} + beta_{1}x^{i}_{1} + beta_{2}x^{i}_{2} + beta_{3}x^{i}_{3} + beta_{4}x^{i}_{4})}}$$ . above, which is technically a choice, not a mathematical coercion (there are, in fact, others that work as well), but why does that make sense? . In order to understand that, we need to understand odds and their relationship with probabilites first. If an event has a 50% chance of ocurring, the odds of it happening are 1:1 (1 time it happens, 1 time it does not). If an event has a 33,3% chance of happening, the odds are 1:2 (1 time it happens, 2 times it does not), 25% represents odds of 1:3, and 20% represents odds of 1:4. Or as a general formula: . $$ Odds = frac{p}{1-p} $$ . e.g. for $p = 0.25$ that evaluates to $ Odds = frac{0.25}{1-0.25} = 0.333... $, i.e. for every 3 times the event under scrutiny does not happen, there will be 1 time where it happens or 1 success : 3 failures. . Substituting the $p$ in the odds formula with the $p(x^i)$ from above, we get: . $$ Odds = e^{ beta_{0} + beta_{1}x^{i}_{1} + beta_{2}x^{i}_{2} + beta_{3}x^{i}_{3} + beta_{4}x^{i}_{4}} $$ . or . $$ log(Odds) = beta_{0} + beta_{1}x^{i}_{1} + beta_{2}x^{i}_{2} + beta_{3}x^{i}_{3} + beta_{4}x^{i}_{4} $$ . Thereby we have created a link between out feature space $x$ and we can map any observation to a probability $ p in (0,1)$. All we need then is a somewhat arbitrary cutoff rule, usually $&gt;.5$. . Furthermore, we get interpretability for free: the coefficient $ beta_i$ describes the change in odds, when we increase $x_i$ by one unit. Look again at the spaghetti diagram with the colors above. The greater the value of measurement 3 to smaller the chance we have a red specimen at hand. In fact, the $ beta$ coefficient of measurement 3 in our &quot;red vs. rest&quot; problem is $-2.26$ which means that a unit increase of measurement 3 decreases the odds of the specimen being red by $exp(-2.26) approx .104$, which is very roughly 1 in 9.5. . Let us draw this function $p(x)$ to see what it looks like: . import matplotlib.pyplot as plt def map_to_p(log_odds): odds = np.exp(log_odds) p = odds / (1 + odds) return p lots_of_log_odds_for_drawing = np.linspace(-10, 10, num=1000) mapped_to_p = list(map(map_to_p, lots_of_log_odds_for_drawing)) plt.xlabel(&quot;log(Odds)&quot;) plt.ylabel(&quot;p&quot;) plt.plot(lots_of_log_odds_for_drawing, mapped_to_p); . This function is called the sigmoid function, which - in terms of our logistic regression model - is the so called link function, as it links a predictor, the $ log(Odds)$ linear combination, to a response in $p in (0,1)$. . Fitting the Parameters . Fitting the parameters is a bit tricky, as we cannot employ a least squares regression as in a linear case. We have to use numerical methods like the maximum likelihood estimation (MLE). Intuitively we want our $ beta$ in such a way, that the linear combinations with $x$ are as far away from the middle of the sigmoid and as to one side for &quot;successes&quot; (usually the positive) and as far to the other for &quot;failures&quot;. Success means being a member of a certain class $i$ and failure means a higher probability for any other class. . This beta, once we have found it, we will call $ hat beta$. . Let&#39;s have alook at the sigmoid again and use the parameters for the class $i=0$ from the sklearn logistic regression to distribute our $x$s: . colors = [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;] # this is the result of b_0 + b_1 * x_1 + b_2 * x_2 + ... log_odds_from_our_dataset = [clf.intercept_[0] + x @ clf.coef_[0] for x in X] plt.xlabel(&quot;log(Odds)&quot;) plt.ylabel(&quot;p&quot;) plt.plot(lots_of_log_odds_for_drawing, mapped_to_p) for x, y_cl in zip(log_odds_from_our_dataset, y): #plt.scatter(x, 0, s=10, c=colors[y_cl]) plt.scatter(x, map_to_p(x), s=10, c=colors[y_cl]) plt.show() . We see now that all of the red dots, which represent the members of class $i=0$ fall on the right side of 0 (and therefore have $p &gt; .5$), and the other two classes fall on the left side. So, basically, we need to choose $ hat beta$ in such a way the sigmoid function is as close to zero for some $x$ and as close to 1 for some other $x$. This is a very awkward problem to solve. Luckily, we are working with the interval of $p in (0,1)$, which means we know that the maximum is 1. So, we can flip those $x$ that are supposed to result in a value of $p$ close to 0 around by calculating $(1-p)$. Now we have a maximization problem for all $x$ in our domain. . Also, a word on the intercept: while $ beta_1 .. beta_4$ essentially &quot;stretch&quot; our $x$ out in such a way that there is as little overlap between the groups as possible, $ beta_0$ changes the position of the whole set of dots, so that they can be nicely centered around 0. We do actually not need to give special treatment to the intercept, as we can just augment our feature vector $x$ with a static 1 like so $x_i = (1, x^{i}_{1}, x^{i}_{2},x^{i}_{3},x^{i}_{4})$. Using these augmented vectors in the following steps, will simplify things a lot. . Furthermore, as we want to maximize all our individual $p$ and $1-p$ respectively, we can also try to maximize the product of all of them and as we want all the terms of the product to be as close to 1 as possible, that means we want the whole product to be as close to 1 as possible. . In our input data, the class a certain specimen belongs to is denoted by $y_i in {0,1,2 }$ with these numbers representing red, green and blue respectively. Now if we want to translate that into a binary problem, we need a $y_{binary, i} in {1,0 }$, were e.g. for the first classification &quot;red vs. non-red&quot; we denote a success with &quot;1&quot; (flower is red) vs. &quot;0&quot; (flower is not red). I will omit the &quot;binary&quot; for brevity&#39;s sake, but please make a big mental note that the $y_i$s we are working with from now on are not the same ones as above any more. . In mathematical terms, we want to find our estimator $ hat beta$ that maximizes the likelihood function: . $$ l( beta) = prod_{x_i; y_i = 1} p(x_i) times prod_{x_i; y_i = 0} (1-p(x_i)) $$ . or: . $$ hat beta = arg max_{ beta} l( beta) $$ . Which can then be simplified as follows: . $$ l( beta) = prod_{i} p(x_i)^{y_i} (1-p(x_i))^{1-y_i}$$ . We need to introduce the next concept now - in order to tackle this maximization problem, we use the fact that $log(a)$ is increasing monotonously with $a$ so maximizing $log(a)$ is equivalent to maximizing $a$, therefore: . $$ hat beta = arg max_{ beta} l( beta) iff hat beta = arg max_{ beta} log l( beta) $$ . and using this property, we can transform the multiplication in $l( beta)$ to a summation in $ log l( beta)$ or $ll( beta)$ for short. . $$ ll( beta) = sum_{i} y_i log(p(x_i)) + (1-y_i) log(1-p(x_i))$$ . We will now simplify this equation further and we will start with the log probability in the first term (blue highlighting will become clear further down the line): . $$ y_i log(p(x_i)) = y_i log frac{1}{1+e^{- beta x_i}} = color{blue}{-y_i log(1+e^{- beta x_i})} $$ . While that was fairly simple, the second term is a bit more challenging. For now, we will omit the term $(1-y_i)$ and focus on the log inverse probability in the second term: . $$ log(1-p(x_i)) = log(1- frac{1}{1+e^{- beta x_i}}) = log( frac{e^{- beta x_i}}{1+e^{- beta x_i}})$$ . To proceed, we have to make a choice what to do with the term in the brackets: we can either take the $e^{- beta x_i}$ in the numerator and bring it down into the denominator or use the log rule for fractions to separate the fraction into a subtraction of two fractionless logarithms. It turns out, we need to do both in order to get to the simplest possible form of whole combined equation and after distributing the terms from the $(1-y_i)$ we need to treat each with a different strategy. For the $1$ term (note that the minus sign in the denominator has disappeared): . $$ 1 log( frac{e^{- beta x_i}}{1+e^{- beta x_i}}) = log( frac{1}{e^{ beta x_i}(1+e^{- beta x_i})}) = log( frac{1}{e^{ beta x_i}+1}) = color{green}{ - log(e^{ beta x_i}+1)} $$ . and for the $-y_i$ term: . $$ -y_i log( frac{e^{- beta x_i}}{1+e^{- beta x_i}}) = -y_i ( log(e^{- beta x_i}) - log(1+e^{- beta x_i})) $$ . $$ = color{green}{ -y_i(- beta x_i)} color{blue}{ + y_i log(1+e^{- beta x_i})} $$ . When we now reassemble the puzzle pieces, the blue terms cancel each other out and the green terms are left . $$ ll( beta) = sum_{i} color{blue}{-y_i log(1+e^{- beta x_i})} color{green}{ - log(e^{ beta x_i}+1)} color{green}{ -y_i(- beta x_i)} color{blue}{ + y_i log(1+e^{- beta x_i})} $$ . $$ = sum_{i} y_i beta x_i- log(e^{ beta x_i}+1) $$ . To jog our memory: . $y_i in {1,0 }$, representing that an $x_i$ belongs to a certain class with 1 or not with 0 | $x_i in mathbb{R}^5$ with the first element set fixed to $1$, the feature vector of an observation | $ beta in mathbb{R}^5$ the vector of coefficients, with the first entry representing the intercept | . We have now managed to state our optimization problem in comparatively simple terms, as the only thing that is missing now is the $ beta$ that will maximize the last expression above, but all the other variables are clearly defined. We cannot compute the optimal $ beta$ algebraically though and have to rely on numerical methods. . A Naive Example . In order to prepare for the next steps of actually fitting the $ beta$ coefficients, we need translate the theoretical maths into python code. Also translating the three class problem of red, green and blue flowers into multiple binary problems like &quot;flower is red vs. flower is not red&quot; is necessary. First the translation of the log-likelihood function. . def log_likelihood(x, y, beta): ll = 0 for x_el, y_el in zip(x, y): ll += y_el * (beta @ x_el) - np.log(np.exp(beta @ x_el) + 1) return ll . We split the three class problem into 3 binary sub-problems, so we need to modify the class vector in such a way that $y$ only has &quot;1&quot; entries for that single class we are testing for and &quot;0&quot; entries for the other two classes: . y_binary_red = [1 if y_el == True else 0 for y_el in y == 0] y_binary_green = [1 if y_el == True else 0 for y_el in y == 1] y_binary_blue = [1 if y_el == True else 0 for y_el in y == 2] . Furthermore, we need to add a &quot;1&quot; in the beginning of each feature vector $x$, in order to account for the intercept. . # make a vector with just &quot;1&quot;s and glue it to the left side of X X_with_intercept = np.hstack((np.ones((X.shape[0], 1)), X)) . Now we leave the cosy realm of algebraic certainty and need to employ numerical methods to get our estimate $ hat beta$. But before we do that, let us see what we want to accomplish in principle by using a brute force algorithm. We start with random $ beta$ - I cheated here as I already roughly know in which area to find the variables. Given this semi-random beta, we calculate the log-likelihood function, which can assume values between $(- infty, 0)$ as it is a $log$ of a probability $p in (0,1)$. If the random $ beta$ increases our likelihood we keep it, otherwise we throw it out and choose another random $ beta$. In order to visualize the results, we program a little helper function first. . # define a plot function to visualize the result def plot_separation(x, beta, y, color=&#39;red&#39;): color = [&#39;grey&#39;, color] for x_el, y_el in zip(x, y): log_odds = beta @ x_el plt.scatter(log_odds, map_to_p(beta @ x_el), c=color[y_el]) plt.plot(lots_of_log_odds_for_drawing, mapped_to_p) plt.show() . # choose a random, but very small likelihood as basis ll_hat = -1000000 for step in range(10001): # choose &quot;random&quot; beta vector 10.000 times # each entry will be between -3 and 3 beta_random = 6 * np.random.random(5) - 3 ll = log_likelihood(X_with_intercept, y_binary_red, beta_random) # if our log-likelihood has improved, overwrite old beta, save likelihood for futher iterations if ll &gt; ll_hat: beta_hat = beta_random ll_hat = ll # draw the result every 5000 steps if step % 5000 == 0: print(&quot;Step:&quot;, step, &quot;, beta_hat:&quot;, beta_hat, &quot;, ll_hat:&quot;, ll_hat) plot_separation(X_with_intercept, beta_hat, y_binary_red) print() . Step: 0 , beta_hat: [-0.15169337 1.40168034 -2.16626949 -1.47939024 -1.21468793] , ll_hat: -154.31046117175367 . Step: 5000 , beta_hat: [ 0.78059886 0.09592393 2.75367263 -2.91886356 -2.25203117] , ll_hat: -0.4374415812923591 . Step: 10000 , beta_hat: [ 0.78059886 0.09592393 2.75367263 -2.91886356 -2.25203117] , ll_hat: -0.4374415812923591 . . As we can see, this neatly separates the red from the non-red dots onto the &quot;1&quot; and &quot;0&quot; side of the sigmoid curve. To be honest, that seems to work only because the red specimen are somewhat neatly separated from the rest from the get go. If you try the other colors, the results will not be that good. . But we have seen the general principle. By wildly choosing random $ beta$s and keeping the ones that increase likelihood, we push the log odds of our &quot;success&quot; class as far to the right as possible, while we keep the &quot;failures&quot; on the left. Now the only step that is left is transitioning from random guessing into a process that is more sophisticated. . Fitting the Parameters with Gradient Descent . As a next step, we will replace the brute force method with a numerical optimization method like Gradient Descent or Newton-Raphson. Today, we are going to use the Gradient Descent method. In simple terms, we will move $ beta$ in small steps towards the direction that minimizes our error function, which is the true $y$ minus our calculated result for $y$ under our guess for $ beta$. This direction happens to be the negative gradient. The true $y$ is simply our inpput dataa for $y$. . def gradient_descent(X, y, steps, learning_rate): beta = np.zeros(X.shape[1]) for _ in range(1, steps): # calculate log odds for all x in X at once log_odds = np.dot(X, beta) # calculate result based on current beta tentative_y = list(map(map_to_p, log_odds)) # calculate difference between current estimate and truth error = np.subtract(y, tentative_y) # see below for explanation gradient = np.dot(error, X) # move beta in opposite direction from error beta += learning_rate * gradient return beta . So what is the gradient of $ll( beta) $ with regards to $ beta$? . $$ nabla_{ beta} ll( beta) = sum_{i} nabla_{ beta} y_i beta x_i- nabla_{ beta} log(e^{ beta x_i}+1) $$ . $$ = sum_{i} y_i x_i- nabla_{ beta} log(e^{ beta x_i}+1) $$ . $$ = sum_{i} y_i x_i - x_i e^{ beta x_i} frac{1}{e^{ beta x_i} +1} $$ . $$ = sum_{i} y_i x_i - x_i frac{1}{1 + e^{- beta x_i}} $$ . $$ = sum_{i} y_i x_i - x_i p(x_i) = sum_{i} (y_i -p(x_i)) x_i $$ . Which is nothing else than the true $y_i$ minus the calculated approximation for $y_i$ which is $p(x_i)$ times the feature vector for each observation, or in matrix form: . $$ nabla_{ beta} ll( beta) = (y_{true} - y_{estimate}( beta))X $$ . To mix things up a bit, let us try using the gradient descent method to identify the blue specimen instead of the red ones: . beta_hat = gradient_descent(X_with_intercept, y_binary_blue, steps=100001, learning_rate=10e-5) . plot_separation(X_with_intercept, beta_hat, y_binary_blue, color=&#39;blue&#39;) . And there we have a separation of the blue dots towards the positive real numbers and the rest towards the negative ones and their respective probabilities going to 1 and 0. As we can see though, the separation does not work as well as in the &quot;red vs. rest&quot; problem. . However, if we try the same with the green specimen, it does not work very well at all. But that was somewhat expected, as we have seen in the very beginning. We should take solace in the fact, that the sklearn implementation does also not fare very well, which can be seen in the confusion matrix, and if we plot the result for the sub-problem &quot;green vs. rest&quot; we can barely differentiate between the green and the grey dots. . # parameters for green from sklearn log-reg beta_0 = np.hstack((clf.__dict__[&quot;intercept_&quot;][0], clf.__dict__[&quot;coef_&quot;][0])) # red beta_1 = np.hstack((clf.__dict__[&quot;intercept_&quot;][1], clf.__dict__[&quot;coef_&quot;][1])) # green beta_2 = np.hstack((clf.__dict__[&quot;intercept_&quot;][2], clf.__dict__[&quot;coef_&quot;][2])) # blue . plot_separation(X_with_intercept, beta_0, y_binary_red, color=&#39;red&#39;) . plot_separation(X_with_intercept, beta_1, y_binary_green, color=&#39;green&#39;) . plot_separation(X_with_intercept, beta_2, y_binary_blue, color=&#39;blue&#39;) . Finally, we can observe that while our solution is not as good as the version implemented in sklearn, it provides results which are quite close already. One difference between the two algorithms is that sklearn penalizes solutions with large coefficients in its optimizer. . Sources: . https://beckernick.github.io/logistic-regression-from-scratch/ | https://www.youtube.com/watch?v=YMJtsYIp4kg | .",
            "url": "http://blog.huppertz-consulting.de/classification/theory/2020/08/25/logistic-regression-in-depth.html",
            "relUrl": "/classification/theory/2020/08/25/logistic-regression-in-depth.html",
            "date": " • Aug 25, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Contents of a Data Science Project Contract",
            "content": "When you are facing your first data science project and the accompanying negotiation of a project contract, it can be a little bit overwhelming and it’s easy to forget to pen down some important issues. In this article I will provide a list of some of these issues that should be defined in a data science project contract - and a few things that should not. . Form and Content of the Result . To manage expectations on the client side, define the deliverables precisely, such as . the goal, e.g. identify factors on the website that influence the average shopping cart volume | decide if any of the 5 marketing campaigns of last year had a measurable impact on the revenue and if so, how much | analyze if the customer retention activities in the call center are generating positive returns | … | . | the format Powerpoint | Word | notebook | Airflow workflow | key-ready application | daily mail reports | … | . | the content one off analysis | model for continuous evaluation | table or visualization | … | . | the timeframe of data to be analyzed and - if applicable - the timeframes for forecasts . | the timing for delivery a.k.a. the deadline | . Wrangling the input data . This is one of the trickiest ones, because part of why you are negotiating a data science project right now might be that your client does not have a very good overview over his own data landscape. But to the extent of your client’s knowledge do define the rough size and shape of the data provided to you and their accessibility: . millions of rows | GB size | remote access or on site only | DB/DWH downtimes | ETL processes blocking you from access (my last project DWH was down for one hour after lunch break for ETL) | … | . Also, include in your contract a passage that allows you to re-negotiate or end the contract prematurely after having an in-depth look at the data. Sometimes it turns out the data is not what is was expected to be in quality and/or size and this might make your project completely infeasible. . You will be paid for your time, not the impact of your results . There is no guarantee that you will find any result of substance and economical impact in an exploratory data science project. If you want to explore, e.g., which factors influence the shopping cart value of a customer, it is entirely possible that the input data set does not contain any answers to your question. Any impactful factors that you do happen to identify might already be at their optimum or cannot be adjusted for reasons outside of the scope of your analysis. Mention in your contract that you do not owe any specific result and that your compensation is for the time and effort of the process, not the efficacy of your results. . Even better: if you have the necessary experience, discuss which possible courses of action your client can pursue before you start your project. E.g. if your analysis shows that older customers spend more money, can your client run TV and Print ads, if it shows that younger customers spend more, can the client’s organization shoulder Instagram marketing or can such expertise be bought. Admitted, that’s a bit cliché, but you get the gist. Without any realistic options to act upon your analysis, the whole premise is moot. . Do not present preliminary results, avoid presenting your results many times over . Do not agree on meetings to present intermediate results without giving great thought to it. Preparing a shiny presentation to placate a sceptical customer after a week or two of work will take focus away from your final result. Preparing the presentation of the result can take between 30 - 50 % of the overall man hours of a project, so if you want to present results after two weeks, you will barely have a week to do the actual work, if at all. Also this can backfire if you decide to pursue a different angle to solve a problem after presenting preliminary results on a different route. . Oftentimes, the client simply wants to be kept in the loop to avoid wasting the money they invest in you. You can offer to communicate regular status updates via Email or chat and you should keep those memos neutral in terms of results to avoid eating your words later, e.g. write “Explored - among other variables - the customers age in relation to the revenue” instead of “Looks like older customer spend more money in your shop”. . Once you find interesting results worthy of presentation, there might be a cascade of “oh, the Head of Marketing needs to see that”, “oh, the COO needs to see that”, “oh, the CEO needs to see that”. This of course leads to you spending more time than expected on the project, because you do not present the results once but 3 or 4 times. Stating in your contract that the results will be presented once and once only will give you leverage, should the client ask you to do more than one. . Access to subject matter experts . While I suggest keeping the C-Level of your client on a “need to know”-basis, I do recommend to keep in close contact with the client’s side subject matter experts as you will have questions that need answers. Everyone needs a vacation once in a while, though. It is most inconvenient if your contacts within the client’s organization are taking that long overdue holiday in the middle of your project. So, make sure you know when they will be present and that they are not constantly beleagured with meetings, but will actually be available to discuss your ideas and findings. . Hardware provided or BYOD . If your client is providing you with hardware and you don’t bring your own device, make sure it’s fast and has sufficient RAM. The minimum nowadays should be 16 GB RAM for batch processing of any significant amount of data. Of course the price of RAM is neglegible compared to the overall budget of the average project and enough RAM to avoid swapping to disk can save you hours if not days over the course of a larger project. . Third Party Tools . If you want to rely on a 3rd party tools - especially when data is uploaded to their servers - make sure you are allowed to do so and it’s within the infosec guidelines of your client. . Minimize PII . If anyhow possible do not accept any personally identifiable information. You don’t want to be involved if at any point there is a data breach and customer information get’s leaked. . Checklist . Add a disclaimer about the exploratory nature of the data science project if applicable | Define a predetermined breaking point after initial look into the data | Not agreed on any preliminary result meetings | Form and content of the final result is defined | Possible analysis outcomes and actions to take upon them have been discussed | How many times the results will be presented is defined | Your compensation for your efforts | Which hardware will be provided (if applicable) | Define which third party tools you want to use and get them signed off by infosec | Define which PII is excluded from the data provided to you | Document size, shape, and scope of data that will be provided to you | Access to subject matter experts is defined and any planned absences have been communicated | .",
            "url": "http://blog.huppertz-consulting.de/project%20management/2020/08/04/data-science-contract-contents.html",
            "relUrl": "/project%20management/2020/08/04/data-science-contract-contents.html",
            "date": " • Aug 4, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Creating a tool for manual testing in Django",
            "content": "Automated testing is still not as prevalent in software development as one would wish. Testing is apparently very expensive, but experience tells us that the absence of testing is oftentimes even more expensive. Unfortunately, budget deciders do not share these experiences with software developers and therefore decide against the implementation of automatic testing in a project. . The project I was working on as as project manager had exactly this setup. Manually testing the software artifacts produced by the devs was one of my jobs as was the documentation of the testing results. . This was obivously not an optimal solution due to various reasons: . as PM I was fairly expensive, junior members would have been more suitable to conduct the user tests | as the software framework was new to the dev team, a lot of functions had to be refactored after their first implementation, which led to features breaking again and again | Communication overhead for all the issues slowed down the development of new features | Just using issues in JIRA was impractical as the constant regression through refactoring would have meant we could never close any issues and user stories and the information fields required for the development workflow were not the same as for the testing workflow | . So in order to remedy this problems I needed an application where I could: . delegate the testing process to junior team members | repeatedly test the same issues again and again | document progression and regression permanently in oredr to provide feedback to the dev team | ensure sufficient test coverage and records about how recently a certain feature was tested | . I decided to create a Django app that would help me realise these goals. . Features . We start with an overview page listing all the issues and their status as well as an aggregate overview over all existing issues faceted according to their priority. . Also in the header there are buttons allowing to choose issues to be tested at random. . . Homepage - high level overview and list of all issues | . When adding a new issue, it is possible to set a priority and an artifact the issue relates to. Artifacts are configurable in the admin backend of Django. . Images that help to describe the issue can be attached and one or multiple sets of credentials like test user account access can be attached. . . Create new issue | . Once an issue has been created it is possible to add comments, images and set a status of the issue (“okay”, “unsure”, “cannot test”, “not working”). The history of status changes will be recorded on the right so it is easy to visually identify how stable an issue is and how carefully it needs to be tested again. . . Issue and testing status | . A simple faceted search is pretty much included OOTB with Django and just need a little bit of configuration. . . Simple search function | . A lot of communication overhead goes into sharing credentials for testing, such a login data for the admin backend or test user credentials. So in order to simplify this process, all credentials that can be shared among the testing team without compromising the security of any live environment can be collected and shared on an overview page as well as linked to the specific issues the are connected with. . . Credentials overview | . Last but not least, all comments attached to issues are chronolgically displayed on the comments overview page to gather the most recent feedback on issues for the project manager and allows for this feedback to be realyed to the dev team. . . Comments section | . So with this tool I could delegate testing tasks to junior team members through the Django account management, record a history of system stability, had a srtem of new input generated by the testers for the dev team and could ensure a constant and evenly distributed coverage across all test cases. .",
            "url": "http://blog.huppertz-consulting.de/tooling/python/django/testing/2020/05/24/manual-testing-manager-app.html",
            "relUrl": "/tooling/python/django/testing/2020/05/24/manual-testing-manager-app.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": " Forecasting Revenue with fbprophet",
            "content": "In this post, we will explore the basic functionality of the fbprophet package and how it can help us to quickly forecast data that is seasonal with an underlying trend. Specifically, we will try to forecast revenue data about two years into the &quot;future&quot; and then we will compare de facto data during these two years. Everything will be based upon revenues from 2012 to October 2017, so we have plenty of data to work with. . Later, we will also explore some of the more advanced features of fbprophet like adding our own regressors (e.g. the weather or marketing spend), logistic growth and differentiating between linear (i.e. additive) and multiplicative seasonal influence. . . Tip: When I tried to install fbprophet there was an error that certain dependencies could not be build. I little research revealed that this has to do with the pystan version I was using. After downgrading pystan to version 2.17 everything worked like a charm. . Let us start by important the basic necessities for our experiment. . import pandas as pd from fbprophet import Prophet import matplotlib.pyplot as plt %matplotlib inline pd.set_option(&quot;display.precision&quot;, 3) . We will choose a testing_cutoff_data before which we will take the data into consideration for training our model and after which we will use the data for evaluating the accuracy of our model. . # only use training data before this year, then compare with data from this year on testing_cutoff_date = &quot;2016&quot; # we can calculate everything based upon daily, weekly or monthly data sampling_frequency = &quot;W&quot; . Additionally, there is a naming convention that fbprohet uses. It requires the time series columns to be labelled ds for the dates and y for the values to be forecast. I pre-formatted the data acordingly, but keep that in mind, when using your own data. . df = pd.read_csv(&quot;daily_revenue.csv&quot;, sep=&quot;,&quot;, decimal=&quot;-&quot;, encoding=&quot;utf-8&quot;) df[&quot;ds&quot;] = pd.to_datetime(df[&quot;ds&quot;]) # split dataset into train and test timeframe df_train = df[df[&quot;ds&quot;] &lt; testing_cutoff_date] df_test = df[df[&quot;ds&quot;] &gt;= testing_cutoff_date] . This gives us dataframes containing the daily revenue, one for the training period and one for the testing period. But for now we will work on weekly data, not daily, as there is a lot of noise and there is really no need to forecast the revenue for a very specific date sometime next year. So we use the resampling function of pandas to aggregate the data on a weekly basis. . df_train_resampled = df_train.set_index(&quot;ds&quot;).resample( &quot;1&quot; + sampling_frequency).sum()[&quot;y&quot;] df_train_resampled = df_train_resampled.reset_index() . df_test_resampled = df_test.set_index(&quot;ds&quot;).resample( &quot;1&quot; + sampling_frequency).sum()[&quot;y&quot;] df_test_resampled = df_test_resampled.reset_index() . Now lets have a look how the whole dataset looks like: . fig, ax = plt.subplots() df_train_resampled.set_index(&quot;ds&quot;).plot(ax=ax) df_test_resampled.set_index(&quot;ds&quot;).plot(ax=ax) ax.legend([&quot;train&quot;, &quot;test&quot;]) plt.show() . There are some apparent challenges our algorithm will have to master: . the trend seems to be approximately linear up until the end of 2015 and but then changes | especially in 2017 there has been an uptick in marketing spending in the company that increased the slope beyond the linear trend in the years before | around christmas and new year&#39;s there is always a sharp decline in revenue | . Training . fbprophet can take holidays into consideration and will fit these as special dates. Later on we can evaluate how these holidays affect the revenue. . m = Prophet() m.add_country_holidays(country_name=&quot;DE&quot;) m.fit(df_train_resampled) if sampling_frequency == &quot;D&quot;: future = m.make_future_dataframe(periods=730, freq=sampling_frequency) elif sampling_frequency == &quot;W&quot;: future = m.make_future_dataframe(periods=104, freq=sampling_frequency) elif sampling_frequency == &quot;M&quot;: future = m.make_future_dataframe(periods=24, freq=sampling_frequency) else: print(&quot;No valid samplig frequency given!&quot;) forecast = m.predict(future) . INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this. INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . When we are using weekly or monthly aggregated data, fbprophet seems to ignore the holidays when we work on aggregated weekly data though, as the following line should produce all forecast rows that have a holiday effect. I haven&#39;t figured out yet how to mark &quot;weeks that contain holidays&quot;. . forecast[forecast[&quot;holidays&quot;] != 0] . ds trend yhat_lower yhat_upper trend_lower trend_upper Christi Himmelfahrt Christi Himmelfahrt_lower Christi Himmelfahrt_upper Erster Mai ... holidays holidays_lower holidays_upper yearly yearly_lower yearly_upper multiplicative_terms multiplicative_terms_lower multiplicative_terms_upper yhat . 0 rows × 46 columns . fig = m.plot(forecast) . fig = m.plot_components(forecast) . Testing . Now that we have a forecast for two years after the last date in the training data set, we can compare this with the actual data from that period of time: . fig, ax = plt.subplots(figsize=(10, 10)) forecast.set_index(&quot;ds&quot;)[&quot;yhat&quot;].plot(ax=ax) df_test_for_plotting = df_test.set_index(&quot;ds&quot;).resample( &quot;1&quot; + sampling_frequency).sum() df_test_for_plotting.plot(ax=ax) ax.legend([&quot;forecast&quot;, &quot;true values&quot;]) . &lt;matplotlib.legend.Legend at 0x1695bf0b160&gt; . Major differences are visible here, especially one the massive revenue decrease during the last days of every year. Nevertheless, out model somewhat follows the shape of the de facto revenue during the testing period. In order to quantify the error in forecasting, we will relate the error to the actual revenue in the given years. . df_for_accuracy = df_test_for_plotting.join(forecast.set_index(&quot;ds&quot;), how=&quot;left&quot;).reset_index() df_for_accuracy_2016 = df_for_accuracy[df_for_accuracy[&quot;ds&quot;] &lt; &quot;2017&quot;] df_for_accuracy_2017 = df_for_accuracy[df_for_accuracy[&quot;ds&quot;] &gt;= &quot;2017&quot;] . df_for_accuracy_2016[&quot;abs. difference&quot;] = df_for_accuracy_2016[ &quot;y&quot;] - df_for_accuracy_2016[&quot;yhat&quot;] df_for_accuracy_2017[&quot;abs. difference&quot;] = df_for_accuracy_2017[ &quot;y&quot;] - df_for_accuracy_2017[&quot;yhat&quot;] . error_2016 = df_for_accuracy_2016[&quot;abs. difference&quot;].sum() / df_for_accuracy_2016[&quot;y&quot;].sum() print(&quot;The relative error in the forecast revenue in 2016 is %.3f.&quot; % error_2016) . The relative error in the forecast revenue in 2016 is 0.002. . error_2017 = df_for_accuracy_2017[&quot;abs. difference&quot;].sum() / df_for_accuracy_2017[&quot;y&quot;].sum() print(&quot;The relative error in the forecast revenue in 2017 is %.3f.&quot; % error_2017) . The relative error in the forecast revenue in 2017 is 0.061. . It turns out, we have a pretty solid estimate for the revenues in the two forecasted years with a relative error of 0.2% and 6% respectively. .",
            "url": "http://blog.huppertz-consulting.de/time%20series/fbprophet/2020/05/22/revenue-forecasting-fbprophet.html",
            "relUrl": "/time%20series/fbprophet/2020/05/22/revenue-forecasting-fbprophet.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Simple Visualizer for Teradata/T-SQL Queries",
            "content": "A Simple Visualizer for T-SQL scripts . A while ago, I was working on a project where the client used an expensive high-performance database cluster in order to do demand forecasting for a German supermarket chain. . What the client did not have: . Automated testing | CD/CI-Pipelines | Coding guidelines | Documentation | . What the client did have however, was a codebase of about 30,000 lines of pure T-SQL code and a team that learned “on the job”, i.e. had never written a functioning application in this SQL dialect. This resulted in a about 10 scripts of varying length and written in different styles by different people, most of whom had left the project in frustration already by the time I joined. . Combine that with requirements which were subject to change on daily (sometimes hourly) basis, arbitrary restrictions from operations (indexes take up too much space) and being about 100% over budget, it was a recipe for disaster. . Debugging could mean scrolling through a single or multiple scripts each consisting of 3,000 lines of code and dozens of SQL statements and manually recalculating steps along the way. This would of course take hours within which priorities of implementation could (and frequently did) change again. . I was in desperate need for some kind of support and wanted to use queryscope to get at least a graphical representation of the queries in question but of course the source code was property of the client and I was not allowed to submit it to an unvetted, unapproved vendor for analysis. . So, I took to recreating parts of queryscope’s features from scratch in python to keep all source on premise, the result of which you can see here: . Running the script will yield results like so: . . Output of the query below | . Caveat: This code was only tested with the very specific way the developers on this project wrote their SQL queries, i.e. using many a CTE to create complex temporary tables and then persisting them into intermediary tables, a pattern which is replicated with mock statements in the following SQL snippet: . WITH cte_category_counts ( category_id, category_name, product_count ) AS ( SELECT c.category_id, c.category_name, COUNT(p.product_id) FROM production.products p INNER JOIN production.categories c ON c.category_id = p.category_id GROUP BY c.category_id, c.category_name ), cte_category_sales(category_id, sales) AS ( SELECT p.category_id, SUM(i.quantity * i.list_price * (1 - i.discount)) FROM sales.order_items i INNER JOIN production.products p ON p.product_id = i.product_id INNER JOIN sales.orders o ON o.order_id = i.order_id WHERE order_status = 4 -- completed GROUP BY p.category_id ) INSERT INTO categories (category_id, category_name, product_count,sales) SELECT c.category_id, c.category_name, c.product_count, s.sales FROM cte_category_counts c INNER JOIN cte_category_sales s ON s.category_id = c.category_id ORDER BY c.category_name; WITH cte_business_division_counts (division_id, counts) AS ( SELECT d.division_id, SUM(c.counts) FROM categories c INNER JOIN organization.divisions d ON c.category_id = d.category_id GROUP BY d.division_id ) cte_business_division_sales (division_id, division_name, sales) AS ( SELECT d.division_id, d.division_name, SUM(c.sales) FROM categories c INNER JOIN organization.divisions d ON c.category_id = d.category_id GROUP BY d.division_id ) INSERT INTO business_divisions (division_id, division_name, product_count,sales) SELECT s.division_id, s.division_name, s.sales, c.counts FROM cte_business_division_counts c INNER JOIN cte_business_division_sales s ON s.division_id = c.division_id ORDER BY c.division_name; . And this here is the source code for the actual visualizer: . import re import os from collections import Counter from graphviz import Digraph import networkx as nx # can be used to generate JSON format of graph # from networkx.readwrite import json_graph basedir = &#39;.&#39; filenames = [&#39;test.sql&#39;] with_deletes = False dot = Digraph(comment=&#39;Structure&#39;, engine=&#39;dot&#39;, strict=True) G = nx.Graph() # graphically highlight tables and their incoming connections highlight_nodes = [] # do not draw these ignore_list = [] # collect all nodes here complete_list = [] for filename in filenames: f = open(os.path.join(basedir, filename)) f_string = f.read() sep = &#39;;&#39; f_list = [x+sep for x in f_string.split(sep)] f_list = [x.replace(&quot; n&quot;, &quot; &quot;) for x in f_list] f_list = [x for x in f_list if x.strip() != &quot;COMMIT;&quot;] complete_list += f_list # define regular expressions for SQL statements re_create = re.compile( r&quot;CREATE s+(?:MULTISET)? s+TABLE s+([a-zA-Z0-9_ .]+) s*,&quot;, re.MULTILINE | re.IGNORECASE) re_insert = re.compile( r&quot;INSERT s+INTO s+([a-zA-Z0-9_ .]+) s* (&quot;, re.MULTILINE | re.IGNORECASE) re_from = re.compile( r&quot;(?&lt;!DELETE) s+FROM s+([a-zA-Z0-9_ .]+)[; s]+&quot;, re.S | re.I) re_cte = re.compile( r&quot;(?:WITH)? s+([A-Za-z0-9_]+) s+AS s? (&quot;, re.MULTILINE | re.IGNORECASE) re_join = re.compile( r&quot;JOIN s+([A-Za-z0-9_ .]+) s&quot;, re.MULTILINE | re.IGNORECASE) re_update = re.compile( r&quot;UPDATE s([ w])+ sSET s[ w , &#39; =_]+&quot;, re.MULTILINE | re.IGNORECASE) re_delete = re.compile( r&quot;DELETE sFROM s([ d w . &#39; =_]+.*;)&quot;, re.S | re.IGNORECASE) node_list = [] delete_nodes = [] create_nodes = [] # go through all statements and check if they match a regex for i, statement in enumerate(complete_list): statement = statement.replace(&quot; n&quot;, &quot; &quot;) to_nodes = [] from_nodes = [] for match in re.findall(re_create, statement): create_nodes.append(match) for match in re.findall(re_delete, statement): delete_nodes.append(match) for match in set(re.findall(re_insert, statement)): to_nodes.append(str.lower(match)) for match in set(re.findall(re_update, statement)): print(match) for match, count in Counter(re.findall(re_cte, statement)).items(): print(&quot;%s : %i&quot; % (match, count)) for match, count in Counter(re.findall(re_from, statement)).items(): if match not in re.findall(re_cte, statement): from_nodes.append(str.lower(match)) # print(5*&#39;-&#39; + &#39;Joins (CTEs removed)&#39; + 5*&#39;-&#39;) for match, count in Counter(re.findall(re_join, statement)).items(): if match not in re.findall(re_cte, f_string): # print(&quot;%s : %i&quot; % (match,count)) from_nodes.append(str.lower(match)) from_nodes = [x for x in from_nodes if x not in ignore_list] to_nodes = [x for x in to_nodes if x not in ignore_list] for to_node in to_nodes: # for every node switch between picking colors from the &quot;left&quot; and &quot;right&quot; end # the spectrum so similar colors do not appear next to each other if i % 2 == 0: edge_color = str(round(i/float(len(complete_list)), 2))+&quot; 1.0 &quot; + str(round(i/float(len(complete_list)), 2)) else: edge_color = str(round((len(complete_list)-i)/float(len(complete_list)), 2)) + &quot; 1.0 &quot; + str(round((len(complete_list)-i)/float(len(complete_list)), 2)) if to_node in highlight_nodes: dot.node(to_node, color=edge_color, style=&#39;filled&#39;, fillcolor=edge_color) else: dot.node(to_node, color=edge_color) for from_node in from_nodes: dot.node(from_node, shape=&#39;box&#39;) if (to_node in highlight_nodes) or (from_node in highlight_nodes): dot.edge(from_node, to_node, color=edge_color, penwidth=&#39;3&#39;) else: dot.edge(from_node, to_node, color=edge_color) # graphically represent deletes within the query if with_deletes: delete_nodes = [str.lower(del_node) for del_node in delete_nodes] delete_label = &#39;&lt;&lt;B&gt;DELETES&lt;/B&gt;&lt;br/&gt;&#39; + &#39;&lt;br/&gt;&#39;.join(delete_nodes) + &#39;&gt;&#39; dot.node(&#39;DELETES&#39;, label=delete_label, shape=&#39;box&#39;) # print(5*&#39;-&#39; + &#39;All participating Tables&#39; + 5*&#39;-&#39;) # for match in set(re.findall(re_from,f_string)+re.findall(re_insert,f_string)+re.findall(re_join,f_string)): # if match not in re.findall(re_cte,f_string): # print (match) dot.render(&#39;output/&#39;+filename.replace(&#39;.&#39;, &#39;_&#39;)+&#39;.gv&#39;, view=True) # print(dot.source) # dot_graph = G.read_dot() # print (json_graph.dumps(dot_graph)) . As you can see, there are also ways to highlight certain nodes, as well as omitting them from the output. There is also an automatic random assignment of a color to any persistent table and all edges going into the respective node are also color-coded accordingly. Nodes that have no downstream dependents within the query are displayed as an ellipse as opposed to a rectangle. . This whole thing is a quick draft that took me about 4-5 hours to write, but it saved me countless hours in debugging. . This script could also be integrated into a CD/CI-pipeline in order to create a constantly updated visual documentation of the project. .",
            "url": "http://blog.huppertz-consulting.de/tooling/python/2020/05/20/sql-visualizer.html",
            "relUrl": "/tooling/python/2020/05/20/sql-visualizer.html",
            "date": " • May 20, 2020"
        }
        
    
  
    
  
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Manuel Huppertz and I have a passion for data, maths and building the right tools for the job. On these pages I will take you on a tour through some of the things I created in the last few years and some of my musings and experiments in the data science field. . I work as a freelance consultant and you can hire me if you need an IT project manager, digital marketing specialist or data scientist. . If you want to contact me, check out my main page. .",
          "url": "http://blog.huppertz-consulting.de/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://blog.huppertz-consulting.de/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}