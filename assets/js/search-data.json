{
  
    
        "post0": {
            "title": " Forecasting Revenue with fbprophet",
            "content": "Forecasting Revenue with fbprophet . Let us start by important the basic necessities for our experiment. . . Tip: When I tried to install fbprophet there was an error that certain dependencies could not be build. I little research revealed that this has to do with the pystan version I was using. After downgrading pystan to version 2.17 everything worked like a charm. . import pandas as pd from fbprophet import Prophet import matplotlib.pyplot as plt %matplotlib inline pd.set_option(&quot;display.precision&quot;, 3) . The dataset includes data from 2012 to October 2017. We will choose a &#39;&#39;&#39;testing_cutoff_data&#39;&#39;&#39; before which we will take the data into consideration for training our model and after which we will use the data for evaluating the accuracy of our model. . # only use training data before this year, then compare with data from this year on testing_cutoff_date = &quot;2016&quot; # we can calculate everything based upon daily, weekly or monthly data sampling_frequency = &quot;W&quot; . Additionally, there is a naming convention that fbprohet uses. It requires the time series columns to be labelled ds for the dates and y for the values to be forecast. I pre-formatted the data acordingly, but keep that in mind, when using your own data. . df = pd.read_csv(&quot;daily_revenue.csv&quot;, sep=&quot;,&quot;, decimal=&quot;-&quot;, encoding=&quot;utf-8&quot;) df[&#39;ds&#39;] = pd.to_datetime(df[&#39;ds&#39;]) # split dataset into train and test timeframe df_train = df[df[&#39;ds&#39;] &lt; testing_cutoff_date] df_test = df[df[&#39;ds&#39;] &gt;= testing_cutoff_date] . This gives us dataframes containing the daily revenue, one for the training period and one for the testing period. But for now we will work on weekly data, not daily, as there is a lot of noise and there is really no need to forecast the revenue for a very specific date sometime next year. So we use the resampling function of pandas to aggregate the data on a weekly basis. . df_train_resampled = df_train.set_index(&#39;ds&#39;).resample( &#39;1&#39; + sampling_frequency).sum()[&#39;y&#39;] df_train_resampled = df_train_resampled.reset_index() . df_test_resampled = df_test.set_index(&#39;ds&#39;).resample( &#39;1&#39; + sampling_frequency).sum()[&#39;y&#39;] df_test_resampled = df_test_resampled.reset_index() . Now lets have a look how the whole dataset looks like: . fig, ax = plt.subplots() df_train_resampled.set_index(&#39;ds&#39;).plot(ax=ax) df_test_resampled.set_index(&#39;ds&#39;).plot(ax=ax) ax.legend([&quot;train&quot;, &quot;test&quot;]) . &lt;matplotlib.legend.Legend at 0x25aca5092e8&gt; . There are some apparent challenges our algorithm will have to master: . the trend seems to be approximately linear up until the end of 2015 and but then changes | especially in 2017 there has been an uptick in marketing spending in the company that generated this revenue curve | around christmas and new year&#39;s there is always a sharp decline in revenue | . Training . fbprophet can take holidays into consideration and will fit these as special dates. Later on we can evaluate how these holidays affect the revenue. . m = Prophet() m.add_country_holidays(country_name=&#39;DE&#39;) m.fit(df_train_resampled) if sampling_frequency == &#39;D&#39;: future = m.make_future_dataframe(periods=730, freq=sampling_frequency) elif sampling_frequency == &#39;W&#39;: future = m.make_future_dataframe(periods=104, freq=sampling_frequency) elif sampling_frequency == &#39;M&#39;: future = m.make_future_dataframe(periods=24, freq=sampling_frequency) else: print(&quot;No valid samplig frequency given!&quot;) forecast = m.predict(future) . INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this. INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. c: python python36-64 lib site-packages pystan misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. . When we are using weekly or monthly aggregated data, fbprophet seems to ignore the holidays though, as the following line should produce all forecast rows that have a holiday effect. . forecast[forecast[&#39;holidays&#39;] != 0] . ds trend yhat_lower yhat_upper trend_lower trend_upper Christi Himmelfahrt Christi Himmelfahrt_lower Christi Himmelfahrt_upper Erster Mai ... holidays holidays_lower holidays_upper yearly yearly_lower yearly_upper multiplicative_terms multiplicative_terms_lower multiplicative_terms_upper yhat . 0 rows × 46 columns . fig = m.plot(forecast) . fig = m.plot_components(forecast) . Testing . Now that we have a forecast for two years after the last date in the training data set, we can compare this with the actual data from that period of time: . fig, ax = plt.subplots(figsize=(10, 10)) forecast.set_index(&quot;ds&quot;)[&quot;yhat&quot;].plot(ax=ax) df_test_for_plotting = df_test.set_index(&quot;ds&quot;).resample( &quot;1&quot; + sampling_frequency).sum() df_test_for_plotting.plot(ax=ax) ax.legend([&#39;forecast&#39;, &quot;true values&quot;]) . &lt;matplotlib.legend.Legend at 0x25ac5e2dcf8&gt; . Major differences are visible here, especially one the massive revenue decrease during the last days of every year. Nevertheless, out model somewhat follows the shape of the de facto revenue during the testing period. In order to quantify the error in forecasting, we will relate the error to the actual revenue in the goven years. . df_for_accuracy = df_test_for_plotting.join(forecast.set_index(&#39;ds&#39;), how=&#39;left&#39;).reset_index() df_for_accuracy_2016 = df_for_accuracy[df_for_accuracy[&#39;ds&#39;] &lt; &#39;2017&#39;] df_for_accuracy_2017 = df_for_accuracy[df_for_accuracy[&#39;ds&#39;] &gt;= &#39;2017&#39;] . df_for_accuracy_2016[&#39;abs. difference&#39;] = df_for_accuracy_2016[ &#39;y&#39;] - df_for_accuracy_2016[&#39;yhat&#39;] df_for_accuracy_2017[&#39;abs. difference&#39;] = df_for_accuracy_2017[ &#39;y&#39;] - df_for_accuracy_2017[&#39;yhat&#39;] . c: python python36-64 lib site-packages ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy c: python python36-64 lib site-packages ipykernel_launcher.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . error_2016 = df_for_accuracy_2016[&#39;abs. difference&#39;].sum() / df_for_accuracy_2016[&#39;y&#39;].sum() print(&quot;The relative error in the forecast revenue in 2016 is %.3f.&quot; % error_2016) . The relative error in the forecast revenue in 2016 is 0.002. . error_2017 = df_for_accuracy_2017[&#39;abs. difference&#39;].sum() / df_for_accuracy_2017[&#39;y&#39;].sum() print(&quot;The relative error in the forecast revenue in 2017 is %.3f.&quot; % error_2017) . The relative error in the forecast revenue in 2017 is 0.061. . It turns out we have a pretty solid estimate for the revenues in the two forecasted years with a relative error of 0.2% and 6% respectively- .",
            "url": "https://tamahagane.github.io/huppertz-consulting/time%20series/fbprophet/2020/05/22/revenue-forecasting-fbprophet.html",
            "relUrl": "/time%20series/fbprophet/2020/05/22/revenue-forecasting-fbprophet.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Simple Visualizer for Teradata/T-SQL Queries",
            "content": "A Simple Visualizer for T-SQL scripts . A while ago, I was working on a project where the client used an expensive high-performance database cluster in order to do demand forecasting for a German supermarket chain. . What the client did not have: . Automated testing | CD/CI-Pipelines | Coding guidelines | Documentation | . What the client did have however, was a codebase of about 30,000 lines of pure T-SQL code and a team that learned “on the job”, i.e. had never written a functioning application in this SQL dialect. This resulted in a about 10 scripts of varying length and written in different styles by different people, most of whom had left the project in frustration already by the time I joined. . Combine that with requirements which were subject to change on daily (sometimes hourly) basis, arbitrary restrictions from operations (indexes take up too much space) and being about 100% over budget, it was a recipe for disaster. . Debugging could mean scrolling through a single or multiple scripts each consisting of 3,000 lines of code and dozens of SQL statements and manually recalculating steps along the way. This would of course take hours within which priorities of implementation could (and frequently did) change again. . I was in desperate need for some kind of support and wanted to use queryscope to get at least a graphical representation of the queries in question but of course the source code was property of the client and I was not allowed to submit it to an unvetted, unapproved vendor for analysis. . So, I took to recreating parts of queryscope’s features from scratch in python to keep all source on premise, the result of which you can see here: . Running the script will yield results like so: . . Output of the query below | . Caveat: This code was only tested with the very specific way the developers on this project wrote their SQL queries, i.e. using many a CTE to create complex temporary tables and then persisting them into intermediary tables, a pattern which is replicated with mock statements in the following SQL snippet: . WITH cte_category_counts ( category_id, category_name, product_count ) AS ( SELECT c.category_id, c.category_name, COUNT(p.product_id) FROM production.products p INNER JOIN production.categories c ON c.category_id = p.category_id GROUP BY c.category_id, c.category_name ), cte_category_sales(category_id, sales) AS ( SELECT p.category_id, SUM(i.quantity * i.list_price * (1 - i.discount)) FROM sales.order_items i INNER JOIN production.products p ON p.product_id = i.product_id INNER JOIN sales.orders o ON o.order_id = i.order_id WHERE order_status = 4 -- completed GROUP BY p.category_id ) INSERT INTO categories (category_id, category_name, product_count,sales) SELECT c.category_id, c.category_name, c.product_count, s.sales FROM cte_category_counts c INNER JOIN cte_category_sales s ON s.category_id = c.category_id ORDER BY c.category_name; WITH cte_business_division_counts (division_id, counts) AS ( SELECT d.division_id, SUM(c.counts) FROM categories c INNER JOIN organization.divisions d ON c.category_id = d.category_id GROUP BY d.division_id ) cte_business_division_sales (division_id, division_name, sales) AS ( SELECT d.division_id, d.division_name, SUM(c.sales) FROM categories c INNER JOIN organization.divisions d ON c.category_id = d.category_id GROUP BY d.division_id ) INSERT INTO business_divisions (division_id, division_name, product_count,sales) SELECT s.division_id, s.division_name, s.sales, c.counts FROM cte_business_division_counts c INNER JOIN cte_business_division_sales s ON s.division_id = c.division_id ORDER BY c.division_name; . And this here is the source code for the actual visualizer: . import re import os from collections import Counter from graphviz import Digraph import networkx as nx # can be used to generate JSON format of graph # from networkx.readwrite import json_graph basedir = &#39;.&#39; filenames = [&#39;test.sql&#39;] with_deletes = False dot = Digraph(comment=&#39;Structure&#39;, engine=&#39;dot&#39;, strict=True) G = nx.Graph() # graphically highlight tables and their incoming connections highlight_nodes = [] # do not draw these ignore_list = [] # collect all nodes here complete_list = [] for filename in filenames: f = open(os.path.join(basedir, filename)) f_string = f.read() sep = &#39;;&#39; f_list = [x+sep for x in f_string.split(sep)] f_list = [x.replace(&quot; n&quot;, &quot; &quot;) for x in f_list] f_list = [x for x in f_list if x.strip() != &quot;COMMIT;&quot;] complete_list += f_list # define regular expressions for SQL statements re_create = re.compile( r&quot;CREATE s+(?:MULTISET)? s+TABLE s+([a-zA-Z0-9_ .]+) s*,&quot;, re.MULTILINE | re.IGNORECASE) re_insert = re.compile( r&quot;INSERT s+INTO s+([a-zA-Z0-9_ .]+) s* (&quot;, re.MULTILINE | re.IGNORECASE) re_from = re.compile( r&quot;(?&lt;!DELETE) s+FROM s+([a-zA-Z0-9_ .]+)[; s]+&quot;, re.S | re.I) re_cte = re.compile( r&quot;(?:WITH)? s+([A-Za-z0-9_]+) s+AS s? (&quot;, re.MULTILINE | re.IGNORECASE) re_join = re.compile( r&quot;JOIN s+([A-Za-z0-9_ .]+) s&quot;, re.MULTILINE | re.IGNORECASE) re_update = re.compile( r&quot;UPDATE s([ w])+ sSET s[ w , &#39; =_]+&quot;, re.MULTILINE | re.IGNORECASE) re_delete = re.compile( r&quot;DELETE sFROM s([ d w . &#39; =_]+.*;)&quot;, re.S | re.IGNORECASE) node_list = [] delete_nodes = [] create_nodes = [] # go through all statements and check if they match a regex for i, statement in enumerate(complete_list): statement = statement.replace(&quot; n&quot;, &quot; &quot;) to_nodes = [] from_nodes = [] for match in re.findall(re_create, statement): create_nodes.append(match) for match in re.findall(re_delete, statement): delete_nodes.append(match) for match in set(re.findall(re_insert, statement)): to_nodes.append(str.lower(match)) for match in set(re.findall(re_update, statement)): print(match) for match, count in Counter(re.findall(re_cte, statement)).items(): print(&quot;%s : %i&quot; % (match, count)) for match, count in Counter(re.findall(re_from, statement)).items(): if match not in re.findall(re_cte, statement): from_nodes.append(str.lower(match)) # print(5*&#39;-&#39; + &#39;Joins (CTEs removed)&#39; + 5*&#39;-&#39;) for match, count in Counter(re.findall(re_join, statement)).items(): if match not in re.findall(re_cte, f_string): # print(&quot;%s : %i&quot; % (match,count)) from_nodes.append(str.lower(match)) from_nodes = [x for x in from_nodes if x not in ignore_list] to_nodes = [x for x in to_nodes if x not in ignore_list] for to_node in to_nodes: # for every node switch between picking colors from the &quot;left&quot; and &quot;right&quot; end # the spectrum so similar colors do not appear next to each other if i % 2 == 0: edge_color = str(round(i/float(len(complete_list)), 2))+&quot; 1.0 &quot; + str(round(i/float(len(complete_list)), 2)) else: edge_color = str(round((len(complete_list)-i)/float(len(complete_list)), 2)) + &quot; 1.0 &quot; + str(round((len(complete_list)-i)/float(len(complete_list)), 2)) if to_node in highlight_nodes: dot.node(to_node, color=edge_color, style=&#39;filled&#39;, fillcolor=edge_color) else: dot.node(to_node, color=edge_color) for from_node in from_nodes: dot.node(from_node, shape=&#39;box&#39;) if (to_node in highlight_nodes) or (from_node in highlight_nodes): dot.edge(from_node, to_node, color=edge_color, penwidth=&#39;3&#39;) else: dot.edge(from_node, to_node, color=edge_color) # graphically represent deletes within the query if with_deletes: delete_nodes = [str.lower(del_node) for del_node in delete_nodes] delete_label = &#39;&lt;&lt;B&gt;DELETES&lt;/B&gt;&lt;br/&gt;&#39; + &#39;&lt;br/&gt;&#39;.join(delete_nodes) + &#39;&gt;&#39; dot.node(&#39;DELETES&#39;, label=delete_label, shape=&#39;box&#39;) # print(5*&#39;-&#39; + &#39;All participating Tables&#39; + 5*&#39;-&#39;) # for match in set(re.findall(re_from,f_string)+re.findall(re_insert,f_string)+re.findall(re_join,f_string)): # if match not in re.findall(re_cte,f_string): # print (match) dot.render(&#39;output/&#39;+filename.replace(&#39;.&#39;, &#39;_&#39;)+&#39;.gv&#39;, view=True) # print(dot.source) # dot_graph = G.read_dot() # print (json_graph.dumps(dot_graph)) . As you can see, there are also ways to highlight certain nodes, as well as omitting them from the output. There is also an automatic random assignment of a color to any persistent table and all edges going into the respective node are also color-coded accordingly. Nodes that have no downstream dependents within the query are displayed as an ellipse as opposed to a rectangle. . This whole thing is a quick draft that took me about 4-5 hours to write, but it saved me countless hours in debugging. . This script could also be integrated into a CD/CI-pipeline in order to create a constantly updated visual documentation of the project. .",
            "url": "https://tamahagane.github.io/huppertz-consulting/tooling/python/2020/05/20/sql-visualizer.html",
            "relUrl": "/tooling/python/2020/05/20/sql-visualizer.html",
            "date": " • May 20, 2020"
        }
        
    
  
    
  
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://tamahagane.github.io/huppertz-consulting/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://tamahagane.github.io/huppertz-consulting/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://tamahagane.github.io/huppertz-consulting/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tamahagane.github.io/huppertz-consulting/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}